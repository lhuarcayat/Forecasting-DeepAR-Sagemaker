{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db60a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:198: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/23/25 13:44:29] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py#1352\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1352</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/23/25 13:44:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=152078;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=310607;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py#1352\u001b\\\u001b[2m1352\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Usuario\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pandas import DateOffset\n",
    "import boto3\n",
    "import sagemaker\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930756f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9410da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data_csv/27_materiales_mod.csv',sep=',',index_col=0,parse_dates=True,decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b61c2eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los materiales son [20000337001 20000400003 20000815002 20000815003 20000837001 20000837002\n",
      " 20001016001 20001374001 20001497002 20001770001 20003147001 20003257001\n",
      " 20003257002 20003257004 20003259001 20006083001 20007769001 20008046001\n",
      " 20008540001 25101938001 25101940001 25109220001 25109223001 25109225001\n",
      " 25109232001 25109241001 25110068001]\n"
     ]
    }
   ],
   "source": [
    "materiales = df['material'].unique()\n",
    "print(f'Los materiales son {materiales}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cf0c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = []\n",
    "materiales = df['material'].unique()\n",
    "for mat in materiales:\n",
    "    serie = df[df['material'] == mat].sort_index()\n",
    "    timeseries.append(serie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd303f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de valores negativos: 0\n"
     ]
    }
   ],
   "source": [
    "val_negative = df[df['cantidad'] < 0]\n",
    "df_val_negative = val_negative[['cantidad', 'material']]\n",
    "#print(f'Valores negativos:\\n{df_val_negative}')\n",
    "print(f'Cantidad de valores negativos: {len(df_val_negative)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e9e589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_frequencies(timeseries, infer_when_missing=True):\n",
    "    \"\"\"\n",
    "    Revisa la frecuencia de cada DataFrame en la lista `timeseries`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timeseries : list[pd.DataFrame] | list[pd.Series]\n",
    "        Lista de objetos con índice de fechas.\n",
    "    infer_when_missing : bool, default True\n",
    "        Si la frecuencia no está definida, intenta inferirla con `pd.infer_freq`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapeo {posición_en_lista: frecuencia_detectada (str | None)}\n",
    "    \"\"\"\n",
    "    freqs = {}\n",
    "    for i, ts in enumerate(timeseries):\n",
    "        if not isinstance(ts.index, pd.DatetimeIndex):\n",
    "            raise TypeError(f\"Elemento {i} no tiene DatetimeIndex\")\n",
    "\n",
    "        # frecuencia explícita (pandas la guarda en .freq)\n",
    "        freq = ts.index.freq\n",
    "\n",
    "        # si no hay frecuencia y se pide inferir\n",
    "        if freq is None and infer_when_missing:\n",
    "            freq = pd.infer_freq(ts.index)\n",
    "\n",
    "        freqs[i] = str(freq) if freq is not None else None\n",
    "        print(f\"timeseries[{i}] → {freq}\")\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51fa986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeseries[0] → None\n",
      "timeseries[1] → None\n",
      "timeseries[2] → None\n",
      "timeseries[3] → None\n",
      "timeseries[4] → None\n",
      "timeseries[5] → None\n",
      "timeseries[6] → None\n",
      "timeseries[7] → None\n",
      "timeseries[8] → None\n",
      "timeseries[9] → None\n",
      "timeseries[10] → None\n",
      "timeseries[11] → None\n",
      "timeseries[12] → None\n",
      "timeseries[13] → None\n",
      "timeseries[14] → None\n",
      "timeseries[15] → None\n",
      "timeseries[16] → None\n",
      "timeseries[17] → None\n",
      "timeseries[18] → None\n",
      "timeseries[19] → None\n",
      "timeseries[20] → None\n",
      "timeseries[21] → None\n",
      "timeseries[22] → None\n",
      "timeseries[23] → None\n",
      "timeseries[24] → None\n",
      "timeseries[25] → None\n",
      "timeseries[26] → None\n"
     ]
    }
   ],
   "source": [
    "freqs = check_frequencies(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d79d4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completar_series_temporales_list(timeseries_list, freq='D'):\n",
    "    \"\"\"\n",
    "    Completa las series temporales agregando fechas faltantes con cantidad=NaN.\n",
    "    Mantiene los valores de las otras columnas para las fechas agregadas.\n",
    "    \n",
    "    Args:\n",
    "        timeseries_list: Lista de DataFrames con índice 'fecha' y columna 'cantidad'\n",
    "        freq: Frecuencia para completar ('D' para diario, 'W' para semanal, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Lista con DataFrames completados\n",
    "    \"\"\"\n",
    "    resultado = []\n",
    "    \n",
    "    for i, df in enumerate(tqdm(timeseries_list, desc=\"Completando series temporales\")):\n",
    "        # Verificar que es un DataFrame válido\n",
    "        if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "            resultado.append(df)\n",
    "            continue\n",
    "        \n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Verificar que el índice es 'fecha' o convertirlo\n",
    "        if df_copy.index.name != 'fecha':\n",
    "            if 'fecha' in df_copy.columns:\n",
    "                df_copy.set_index('fecha', inplace=True)\n",
    "            else:\n",
    "                print(f\"DataFrame {i}: No se encontró la columna o índice 'fecha'\")\n",
    "                resultado.append(df)\n",
    "                continue\n",
    "        \n",
    "        # Verificar que existe la columna 'cantidad'\n",
    "        if 'cantidad' not in df_copy.columns:\n",
    "            print(f\"DataFrame {i}: No se encontró la columna 'cantidad'\")\n",
    "            resultado.append(df)\n",
    "            continue\n",
    "        \n",
    "        # Asegurarse de que el índice es datetime\n",
    "        if not isinstance(df_copy.index, pd.DatetimeIndex):\n",
    "            df_copy.index = pd.to_datetime(df_copy.index)\n",
    "        \n",
    "        # Si el DataFrame está vacío después de las verificaciones, continuar\n",
    "        if df_copy.empty:\n",
    "            resultado.append(df_copy)\n",
    "            continue\n",
    "        \n",
    "        # Obtener el rango completo de fechas\n",
    "        fecha_min = df_copy.index.min()\n",
    "        fecha_max = df_copy.index.max()\n",
    "        \n",
    "        # Crear el índice completo de fechas\n",
    "        fechas_completas = pd.date_range(start=fecha_min, end=fecha_max, freq=freq)\n",
    "        \n",
    "        # Reindexar el DataFrame para incluir todas las fechas\n",
    "        df_completo = df_copy.reindex(fechas_completas)\n",
    "        \n",
    "        # Asignar nombre al índice\n",
    "        df_completo.index.name = 'fecha'\n",
    "        \n",
    "        # Para las fechas nuevas (que no existían), completar las columnas que no son 'cantidad'\n",
    "        # con los valores más comunes o un valor de referencia\n",
    "        for columna in df_completo.columns:\n",
    "            if columna != 'cantidad':\n",
    "                # Buscar el valor más común (moda) para esta columna\n",
    "                valores_no_nulos = df_copy[columna].dropna()\n",
    "                \n",
    "                if not valores_no_nulos.empty:\n",
    "                    if valores_no_nulos.dtype in ['object', 'string']:\n",
    "                        # Para columnas categóricas/texto, usar la moda\n",
    "                        valor_relleno = valores_no_nulos.mode().iloc[0] if len(valores_no_nulos.mode()) > 0 else valores_no_nulos.iloc[0]\n",
    "                    else:\n",
    "                        # Para columnas numéricas, usar la moda o la mediana si no hay moda clara\n",
    "                        moda = valores_no_nulos.mode()\n",
    "                        if len(moda) > 0:\n",
    "                            valor_relleno = moda.iloc[0]\n",
    "                        else:\n",
    "                            valor_relleno = valores_no_nulos.median()\n",
    "                    \n",
    "                    # Rellenar solo los valores NaN (fechas nuevas)\n",
    "                    df_completo[columna] = df_completo[columna].fillna(valor_relleno)\n",
    "        \n",
    "        # La columna 'cantidad' ya debe tener NaN para las fechas nuevas (esto es lo que queremos)\n",
    "        \n",
    "        print(f\"DataFrame {i}: {len(df_copy)} → {len(df_completo)} registros ({len(df_completo) - len(df_copy)} fechas agregadas)\")\n",
    "        \n",
    "        resultado.append(df_completo)\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "642d945e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Completando series temporales:  85%|████████▌ | 23/27 [00:00<00:00, 229.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0: 525 → 1370 registros (845 fechas agregadas)\n",
      "DataFrame 1: 326 → 1353 registros (1027 fechas agregadas)\n",
      "DataFrame 2: 551 → 1374 registros (823 fechas agregadas)\n",
      "DataFrame 3: 544 → 1370 registros (826 fechas agregadas)\n",
      "DataFrame 4: 399 → 1372 registros (973 fechas agregadas)\n",
      "DataFrame 5: 524 → 1360 registros (836 fechas agregadas)\n",
      "DataFrame 6: 528 → 1373 registros (845 fechas agregadas)\n",
      "DataFrame 7: 217 → 1339 registros (1122 fechas agregadas)\n",
      "DataFrame 8: 633 → 1323 registros (690 fechas agregadas)\n",
      "DataFrame 9: 637 → 1366 registros (729 fechas agregadas)\n",
      "DataFrame 10: 916 → 1373 registros (457 fechas agregadas)\n",
      "DataFrame 11: 623 → 1366 registros (743 fechas agregadas)\n",
      "DataFrame 12: 753 → 1366 registros (613 fechas agregadas)\n",
      "DataFrame 13: 476 → 1374 registros (898 fechas agregadas)\n",
      "DataFrame 14: 1023 → 1374 registros (351 fechas agregadas)\n",
      "DataFrame 15: 741 → 1368 registros (627 fechas agregadas)\n",
      "DataFrame 16: 611 → 1349 registros (738 fechas agregadas)\n",
      "DataFrame 17: 466 → 1316 registros (850 fechas agregadas)\n",
      "DataFrame 18: 686 → 1092 registros (406 fechas agregadas)\n",
      "DataFrame 19: 741 → 1166 registros (425 fechas agregadas)\n",
      "DataFrame 20: 654 → 904 registros (250 fechas agregadas)\n",
      "DataFrame 21: 622 → 1049 registros (427 fechas agregadas)\n",
      "DataFrame 22: 597 → 1345 registros (748 fechas agregadas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Completando series temporales: 100%|██████████| 27/27 [00:00<00:00, 228.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 23: 689 → 1359 registros (670 fechas agregadas)\n",
      "DataFrame 24: 546 → 1358 registros (812 fechas agregadas)\n",
      "DataFrame 25: 633 → 1347 registros (714 fechas agregadas)\n",
      "DataFrame 26: 681 → 1232 registros (551 fechas agregadas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "timeseries = completar_series_temporales_list(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12d1a207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeseries[0] → <Day>\n",
      "timeseries[1] → <Day>\n",
      "timeseries[2] → <Day>\n",
      "timeseries[3] → <Day>\n",
      "timeseries[4] → <Day>\n",
      "timeseries[5] → <Day>\n",
      "timeseries[6] → <Day>\n",
      "timeseries[7] → <Day>\n",
      "timeseries[8] → <Day>\n",
      "timeseries[9] → <Day>\n",
      "timeseries[10] → <Day>\n",
      "timeseries[11] → <Day>\n",
      "timeseries[12] → <Day>\n",
      "timeseries[13] → <Day>\n",
      "timeseries[14] → <Day>\n",
      "timeseries[15] → <Day>\n",
      "timeseries[16] → <Day>\n",
      "timeseries[17] → <Day>\n",
      "timeseries[18] → <Day>\n",
      "timeseries[19] → <Day>\n",
      "timeseries[20] → <Day>\n",
      "timeseries[21] → <Day>\n",
      "timeseries[22] → <Day>\n",
      "timeseries[23] → <Day>\n",
      "timeseries[24] → <Day>\n",
      "timeseries[25] → <Day>\n",
      "timeseries[26] → <Day>\n"
     ]
    }
   ],
   "source": [
    "freqs = check_frequencies(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58d45574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumar_cantidades_fechas(lista_dataframes, fechas=['2023-11-01', '2023-11-02']):\n",
    "    \"\"\"\n",
    "    Suma la columna 'cantidad' para las fechas especificadas en cada dataframe\n",
    "    \n",
    "    Args:\n",
    "        lista_dataframes: Lista de dataframes con índice de fecha\n",
    "        fechas: Lista de fechas a considerar (por defecto '2023-11-01' y '2023-11-02')\n",
    "    \n",
    "    Returns:\n",
    "        Un diccionario con las sumas por cada dataframe\n",
    "    \"\"\"\n",
    "    resultados = {}\n",
    "    \n",
    "    for i, df in enumerate(lista_dataframes):\n",
    "        suma = 0\n",
    "        for fecha in fechas:\n",
    "            if fecha in df.index:\n",
    "                suma += df.loc[fecha, 'cantidad']\n",
    "        resultados[f'dataframe_{i}'] = suma\n",
    "    \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23b58d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = sumar_cantidades_fechas(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d282fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15b81b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers_timeseries_list(timeseries_list, z_score_threshold=3, excluded_dates=None):\n",
    "    \"\"\"\n",
    "    Identifica y marca como NaN los valores atípicos en la columna 'cantidad'\n",
    "    basándose en el z-score calculado por día de la semana (lunes a domingo).\n",
    "    Excluye fechas específicas del análisis.\n",
    "    \n",
    "    Args:\n",
    "        timeseries_list: Lista de DataFrames con índice 'fecha' y columna 'cantidad'\n",
    "        z_score_threshold: Umbral de z-score para considerar un valor como outlier\n",
    "        excluded_dates: Lista de fechas a excluir del análisis de outliers\n",
    "        \n",
    "    Returns:\n",
    "        Lista con los DataFrames procesados\n",
    "    \"\"\"\n",
    "    # Convertir fechas excluidas a datetime si se proporcionan\n",
    "    if excluded_dates is None:\n",
    "        excluded_dates = ['2023-11-01', '2023-11-02']  # Fechas por defecto a excluir\n",
    "    \n",
    "    excluded_dates = [pd.to_datetime(date) for date in excluded_dates]\n",
    "    \n",
    "    # Lista para almacenar resultados\n",
    "    cleaned_series = []\n",
    "    \n",
    "    # Procesar cada DataFrame\n",
    "    for i, df in enumerate(tqdm(timeseries_list, desc=\"Limpiando outliers por día de semana\")):\n",
    "        # Verificar que es un DataFrame válido\n",
    "        if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "            cleaned_series.append(df)\n",
    "            continue\n",
    "        \n",
    "        # Hacer una copia para no modificar el original\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Resetear índice si 'fecha' está como índice\n",
    "        if df_copy.index.name == 'fecha':\n",
    "            df_copy = df_copy.reset_index()\n",
    "        elif 'fecha' not in df_copy.columns and hasattr(df_copy.index, 'name'):\n",
    "            # Si el índice parece ser fecha pero no tiene nombre\n",
    "            df_copy = df_copy.reset_index()\n",
    "            if 'index' in df_copy.columns:\n",
    "                df_copy.rename(columns={'index': 'fecha'}, inplace=True)\n",
    "        \n",
    "        # Verificar que existe la columna 'cantidad'\n",
    "        if 'cantidad' not in df_copy.columns:\n",
    "            print(f\"DataFrame {i}: No se encontró la columna 'cantidad'\")\n",
    "            cleaned_series.append(df)\n",
    "            continue\n",
    "        \n",
    "        # Asegurarse de que la fecha es datetime\n",
    "        if df_copy['fecha'].dtype != 'datetime64[ns]':\n",
    "            df_copy['fecha'] = pd.to_datetime(df_copy['fecha'])\n",
    "        \n",
    "        # Crear columna de día de la semana (0=lunes, 6=domingo)\n",
    "        df_copy['weekday'] = df_copy['fecha'].dt.dayofweek\n",
    "        \n",
    "        # Crear máscara para excluir fechas específicas\n",
    "        exclude_mask = df_copy['fecha'].isin(excluded_dates)\n",
    "        \n",
    "        # Datos para análisis (excluyendo fechas específicas y valores NaN existentes)\n",
    "        analysis_df = df_copy[~exclude_mask].copy()\n",
    "        \n",
    "        # Calcular z-scores para cada día de la semana\n",
    "        outlier_indices = []\n",
    "        \n",
    "        for weekday in range(7):  # 0=lunes, 6=domingo\n",
    "            # Filtrar datos para este día de la semana\n",
    "            day_mask = analysis_df['weekday'] == weekday\n",
    "            day_data = analysis_df.loc[day_mask, 'cantidad']\n",
    "            \n",
    "            # Si no hay suficientes datos (al menos 5) o todos son NaN, continuar\n",
    "            if len(day_data) < 5 or day_data.isna().all():\n",
    "                continue\n",
    "            \n",
    "            # Calcular media y desviación estándar (ignorando NaN)\n",
    "            day_mean = day_data.mean()\n",
    "            day_std = day_data.std()\n",
    "            \n",
    "            # Evitar división por cero\n",
    "            if day_std == 0 or pd.isna(day_std):\n",
    "                continue\n",
    "            \n",
    "            # Calcular z-scores: (valor - media) / desviación estándar\n",
    "            z_scores = (day_data - day_mean) / day_std\n",
    "            \n",
    "            # Identificar outliers (z-score absoluto mayor que umbral)\n",
    "            day_outliers = day_mask & z_scores.abs().gt(z_score_threshold)\n",
    "            \n",
    "            # Guardar índices de outliers\n",
    "            if day_outliers.any():\n",
    "                outlier_indices.extend(analysis_df[day_outliers].index.tolist())\n",
    "        \n",
    "        # Reemplazar outliers con NaN\n",
    "        if outlier_indices:\n",
    "            df_copy.loc[outlier_indices, 'cantidad'] = np.nan\n",
    "            print(f\"DataFrame {i}: Se identificaron {len(outlier_indices)} outliers de {len(df_copy)} registros\")\n",
    "        \n",
    "        # Eliminar columna temporal weekday\n",
    "        df_copy.drop('weekday', axis=1, inplace=True)\n",
    "        \n",
    "        # Restaurar el índice original si era necesario\n",
    "        if df.index.name == 'fecha':\n",
    "            df_copy.set_index('fecha', inplace=True)\n",
    "        \n",
    "        # Guardar DataFrame procesado\n",
    "        cleaned_series.append(df_copy)\n",
    "    \n",
    "    return cleaned_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "135c6468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Limpiando outliers por día de semana:   0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0: Se identificaron 9 outliers de 1370 registros\n",
      "DataFrame 1: Se identificaron 7 outliers de 1353 registros\n",
      "DataFrame 2: Se identificaron 12 outliers de 1374 registros\n",
      "DataFrame 3: Se identificaron 12 outliers de 1370 registros\n",
      "DataFrame 4: Se identificaron 9 outliers de 1372 registros\n",
      "DataFrame 5: Se identificaron 12 outliers de 1360 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Limpiando outliers por día de semana:  33%|███▎      | 9/27 [00:00<00:00, 79.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 6: Se identificaron 13 outliers de 1373 registros\n",
      "DataFrame 7: Se identificaron 4 outliers de 1339 registros\n",
      "DataFrame 8: Se identificaron 8 outliers de 1323 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Limpiando outliers por día de semana:  67%|██████▋   | 18/27 [00:00<00:00, 84.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 9: Se identificaron 13 outliers de 1366 registros\n",
      "DataFrame 10: Se identificaron 21 outliers de 1373 registros\n",
      "DataFrame 11: Se identificaron 13 outliers de 1366 registros\n",
      "DataFrame 12: Se identificaron 20 outliers de 1366 registros\n",
      "DataFrame 13: Se identificaron 15 outliers de 1374 registros\n",
      "DataFrame 14: Se identificaron 20 outliers de 1374 registros\n",
      "DataFrame 15: Se identificaron 17 outliers de 1368 registros\n",
      "DataFrame 16: Se identificaron 13 outliers de 1349 registros\n",
      "DataFrame 17: Se identificaron 12 outliers de 1316 registros\n",
      "DataFrame 18: Se identificaron 12 outliers de 1092 registros\n",
      "DataFrame 19: Se identificaron 16 outliers de 1166 registros\n",
      "DataFrame 20: Se identificaron 14 outliers de 904 registros\n",
      "DataFrame 21: Se identificaron 14 outliers de 1049 registros\n",
      "DataFrame 22: Se identificaron 12 outliers de 1345 registros\n",
      "DataFrame 23: Se identificaron 14 outliers de 1359 registros\n",
      "DataFrame 24: Se identificaron 17 outliers de 1358 registros\n",
      "DataFrame 25: Se identificaron 13 outliers de 1347 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Limpiando outliers por día de semana: 100%|██████████| 27/27 [00:00<00:00, 81.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 26: Se identificaron 15 outliers de 1232 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "timeseries = clean_outliers_timeseries_list(timeseries, z_score_threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68ac8e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Completando series temporales:   0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0: 1370 → 1370 registros (0 fechas agregadas)\n",
      "DataFrame 1: 1353 → 1353 registros (0 fechas agregadas)\n",
      "DataFrame 2: 1374 → 1374 registros (0 fechas agregadas)\n",
      "DataFrame 3: 1370 → 1370 registros (0 fechas agregadas)\n",
      "DataFrame 4: 1372 → 1372 registros (0 fechas agregadas)\n",
      "DataFrame 5: 1360 → 1360 registros (0 fechas agregadas)\n",
      "DataFrame 6: 1373 → 1373 registros (0 fechas agregadas)\n",
      "DataFrame 7: 1339 → 1339 registros (0 fechas agregadas)\n",
      "DataFrame 8: 1323 → 1323 registros (0 fechas agregadas)\n",
      "DataFrame 9: 1366 → 1366 registros (0 fechas agregadas)\n",
      "DataFrame 10: 1373 → 1373 registros (0 fechas agregadas)\n",
      "DataFrame 11: 1366 → 1366 registros (0 fechas agregadas)\n",
      "DataFrame 12: 1366 → 1366 registros (0 fechas agregadas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Completando series temporales: 100%|██████████| 27/27 [00:00<00:00, 233.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 13: 1374 → 1374 registros (0 fechas agregadas)\n",
      "DataFrame 14: 1374 → 1374 registros (0 fechas agregadas)\n",
      "DataFrame 15: 1368 → 1368 registros (0 fechas agregadas)\n",
      "DataFrame 16: 1349 → 1349 registros (0 fechas agregadas)\n",
      "DataFrame 17: 1316 → 1316 registros (0 fechas agregadas)\n",
      "DataFrame 18: 1092 → 1092 registros (0 fechas agregadas)\n",
      "DataFrame 19: 1166 → 1166 registros (0 fechas agregadas)\n",
      "DataFrame 20: 904 → 904 registros (0 fechas agregadas)\n",
      "DataFrame 21: 1049 → 1049 registros (0 fechas agregadas)\n",
      "DataFrame 22: 1345 → 1345 registros (0 fechas agregadas)\n",
      "DataFrame 23: 1359 → 1359 registros (0 fechas agregadas)\n",
      "DataFrame 24: 1358 → 1358 registros (0 fechas agregadas)\n",
      "DataFrame 25: 1347 → 1347 registros (0 fechas agregadas)\n",
      "DataFrame 26: 1232 → 1232 registros (0 fechas agregadas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "timeseries = completar_series_temporales_list(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54cafbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeseries[0] → <Day>\n",
      "timeseries[1] → <Day>\n",
      "timeseries[2] → <Day>\n",
      "timeseries[3] → <Day>\n",
      "timeseries[4] → <Day>\n",
      "timeseries[5] → <Day>\n",
      "timeseries[6] → <Day>\n",
      "timeseries[7] → <Day>\n",
      "timeseries[8] → <Day>\n",
      "timeseries[9] → <Day>\n",
      "timeseries[10] → <Day>\n",
      "timeseries[11] → <Day>\n",
      "timeseries[12] → <Day>\n",
      "timeseries[13] → <Day>\n",
      "timeseries[14] → <Day>\n",
      "timeseries[15] → <Day>\n",
      "timeseries[16] → <Day>\n",
      "timeseries[17] → <Day>\n",
      "timeseries[18] → <Day>\n",
      "timeseries[19] → <Day>\n",
      "timeseries[20] → <Day>\n",
      "timeseries[21] → <Day>\n",
      "timeseries[22] → <Day>\n",
      "timeseries[23] → <Day>\n",
      "timeseries[24] → <Day>\n",
      "timeseries[25] → <Day>\n",
      "timeseries[26] → <Day>\n"
     ]
    }
   ],
   "source": [
    "freqs = check_frequencies(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "724af656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_anomalous_dates_list(timeseries_list, anomalous_dates=['2023-11-01', '2023-11-02']):\n",
    "    \"\"\"\n",
    "    Guarda y reemplaza valores anómalos en fechas específicas utilizando una estrategia híbrida.\n",
    "    Adaptado para trabajar con lista de DataFrames con índice 'fecha'.\n",
    "    \n",
    "    Args:\n",
    "        timeseries_list: Lista de DataFrames con índice 'fecha' y columna 'cantidad'\n",
    "        anomalous_dates: Lista de fechas consideradas anómalas\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: (lista de series procesadas, diccionario con valores originales guardados)\n",
    "    \"\"\"\n",
    "    # Convertir fechas anómalas a datetime\n",
    "    anomalous_dates = [pd.to_datetime(date) for date in anomalous_dates]\n",
    "    \n",
    "    # Diccionario para almacenar valores originales (usando índice de la lista como clave)\n",
    "    original_values = {}\n",
    "    \n",
    "    # Lista para almacenar series procesadas\n",
    "    processed_series = []\n",
    "    \n",
    "    # Procesar cada DataFrame\n",
    "    for i, df in enumerate(tqdm(timeseries_list, desc=\"Procesando fechas anómalas\")):\n",
    "        # Verificar que es un DataFrame válido\n",
    "        if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "            processed_series.append(df)\n",
    "            continue\n",
    "        \n",
    "        # Hacer una copia para no modificar el original\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Verificar que el índice es 'fecha' o convertirlo\n",
    "        if df_copy.index.name != 'fecha':\n",
    "            if 'fecha' in df_copy.columns:\n",
    "                df_copy.set_index('fecha', inplace=True)\n",
    "            else:\n",
    "                print(f\"DataFrame {i}: No se encontró la columna o índice 'fecha'\")\n",
    "                processed_series.append(df)\n",
    "                continue\n",
    "        \n",
    "        # Verificar que existe la columna 'cantidad'\n",
    "        if 'cantidad' not in df_copy.columns:\n",
    "            print(f\"DataFrame {i}: No se encontró la columna 'cantidad'\")\n",
    "            processed_series.append(df)\n",
    "            continue\n",
    "        \n",
    "        # Asegurarse de que el índice es datetime\n",
    "        if not isinstance(df_copy.index, pd.DatetimeIndex):\n",
    "            df_copy.index = pd.to_datetime(df_copy.index)\n",
    "        \n",
    "        # Inicializar diccionario para este DataFrame\n",
    "        original_values[i] = {}\n",
    "        \n",
    "        # Para cada fecha anómala\n",
    "        for anomalous_date in anomalous_dates:\n",
    "            # Verificar si esta fecha existe en el DataFrame\n",
    "            if anomalous_date not in df_copy.index:\n",
    "                continue\n",
    "            \n",
    "            # Obtener el valor original\n",
    "            if not pd.isna(df_copy.loc[anomalous_date, 'cantidad']):\n",
    "                original_value = df_copy.loc[anomalous_date, 'cantidad']\n",
    "                \n",
    "                # Guardar el valor original\n",
    "                original_values[i][anomalous_date.strftime('%Y-%m-%d')] = original_value\n",
    "            else:\n",
    "                # Si no hay dato para esta fecha o es NaN, continuar con la siguiente fecha\n",
    "                continue\n",
    "            \n",
    "            # 1. Obtener el valor del mismo día del año anterior (V₁)\n",
    "            previous_year_date = anomalous_date - pd.DateOffset(years=1)\n",
    "            \n",
    "            if previous_year_date in df_copy.index and not pd.isna(df_copy.loc[previous_year_date, 'cantidad']):\n",
    "                v1 = df_copy.loc[previous_year_date, 'cantidad']\n",
    "            else:\n",
    "                # Si no hay valor del año anterior, v1 será None\n",
    "                v1 = None\n",
    "            \n",
    "            # 2. Calcular la mediana del día de la semana correspondiente (V₂)\n",
    "            weekday = anomalous_date.dayofweek\n",
    "            \n",
    "            # Crear máscara para el día de la semana, excluyendo fechas anómalas\n",
    "            weekday_mask = (df_copy.index.dayofweek == weekday) & (~df_copy.index.isin(anomalous_dates))\n",
    "            weekday_data = df_copy.loc[weekday_mask, 'cantidad'].dropna()\n",
    "            \n",
    "            if len(weekday_data) >= 3:  # Al menos 3 valores para una mediana confiable\n",
    "                v2 = weekday_data.median()\n",
    "            else:\n",
    "                # Si no hay suficientes datos para este día, usar todos los datos no anómalos\n",
    "                exclude_anomalous_mask = ~df_copy.index.isin(anomalous_dates)\n",
    "                all_data = df_copy.loc[exclude_anomalous_mask, 'cantidad'].dropna()\n",
    "                \n",
    "                if len(all_data) >= 3:\n",
    "                    v2 = all_data.median()\n",
    "                else:\n",
    "                    # Si no hay suficientes datos, mantener el valor original\n",
    "                    continue\n",
    "            \n",
    "            # 3. Aplicar reglas de decisión\n",
    "            if v1 is not None:\n",
    "                # Calcular la diferencia relativa\n",
    "                relative_diff = abs(v1 - v2) / v2 if v2 != 0 else float('inf')\n",
    "                \n",
    "                # Verificar si v1 es atípico (>2.5 veces la mediana o <0.4 veces)\n",
    "                is_v1_outlier = (v1 > 2.5 * v2) or (v1 < 0.4 * v2)\n",
    "                \n",
    "                if not is_v1_outlier and relative_diff < 0.5:\n",
    "                    # Si la diferencia es menor al 50% y v1 no es outlier, usar v1\n",
    "                    replacement_value = v1\n",
    "                    method = \"valor_año_anterior\"\n",
    "                elif is_v1_outlier:\n",
    "                    # Si v1 es claramente atípico, usar v2\n",
    "                    replacement_value = v2\n",
    "                    method = \"mediana_dia_semana\"\n",
    "                else:\n",
    "                    # Si la diferencia es mayor pero v1 no es atípico, calcular promedio ponderado\n",
    "                    replacement_value = 0.6 * v1 + 0.4 * v2\n",
    "                    method = \"promedio_ponderado\"\n",
    "            else:\n",
    "                # Si no hay valor del año anterior, usar v2\n",
    "                replacement_value = v2\n",
    "                method = \"mediana_dia_semana\"\n",
    "            \n",
    "            # Reemplazar el valor anómalo\n",
    "            df_copy.loc[anomalous_date, 'cantidad'] = replacement_value\n",
    "            \n",
    "            # Preparar información sobre valores V1 y V2 para mostrar\n",
    "            if v1 is not None:\n",
    "                v1_info = f\"{v1:.2f}\"\n",
    "            else:\n",
    "                v1_info = \"No disponible\"\n",
    "            \n",
    "            # Mostrar info sobre el reemplazo\n",
    "            print(f\"DataFrame {i} - {anomalous_date.strftime('%Y-%m-%d')}: \"\n",
    "                  f\"Original: {original_value:.2f} → Reemplazo: {replacement_value:.2f} \"\n",
    "                  f\"(Método: {method}, V1: {v1_info}, V2: {v2:.2f})\")\n",
    "        \n",
    "        # Guardar DataFrame procesado\n",
    "        processed_series.append(df_copy)\n",
    "    \n",
    "    return processed_series, original_values\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# processed_timeseries, original_vals = handle_anomalous_dates_list(timeseries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7bc84fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando fechas anómalas:   0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0 - 2023-11-01: Original: 9.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 0 - 2023-11-02: Original: 24.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 1 - 2023-11-01: Original: 11.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 1 - 2023-11-02: Original: 9.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 2 - 2023-11-01: Original: 13.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 2 - 2023-11-02: Original: 12.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 3 - 2023-11-01: Original: 16.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 3 - 2023-11-02: Original: 15.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 4 - 2023-11-01: Original: 16.00 → Reemplazo: 1.00 (Método: valor_año_anterior, V1: 1.00, V2: 1.00)\n",
      "DataFrame 4 - 2023-11-02: Original: 12.00 → Reemplazo: 1.00 (Método: valor_año_anterior, V1: 1.00, V2: 1.00)\n",
      "DataFrame 5 - 2023-11-01: Original: 19.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 5 - 2023-11-02: Original: 25.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 6 - 2023-11-01: Original: 3.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 6 - 2023-11-02: Original: 1.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 7 - 2023-11-01: Original: 3.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 7 - 2023-11-02: Original: 2.00 → Reemplazo: 1.00 (Método: valor_año_anterior, V1: 1.00, V2: 1.00)\n",
      "DataFrame 8 - 2023-11-01: Original: 17.00 → Reemplazo: 2.00 (Método: valor_año_anterior, V1: 2.00, V2: 2.00)\n",
      "DataFrame 8 - 2023-11-02: Original: 24.00 → Reemplazo: 1.40 (Método: promedio_ponderado, V1: 1.00, V2: 2.00)\n",
      "DataFrame 9 - 2023-11-01: Original: 16.00 → Reemplazo: 2.00 (Método: mediana_dia_semana, V1: No disponible, V2: 2.00)\n",
      "DataFrame 9 - 2023-11-02: Original: 33.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando fechas anómalas: 100%|██████████| 27/27 [00:00<00:00, 334.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 10 - 2023-11-01: Original: 50.00 → Reemplazo: 4.00 (Método: mediana_dia_semana, V1: No disponible, V2: 4.00)\n",
      "DataFrame 10 - 2023-11-02: Original: 83.00 → Reemplazo: 4.00 (Método: mediana_dia_semana, V1: No disponible, V2: 4.00)\n",
      "DataFrame 11 - 2023-11-01: Original: 12.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 11 - 2023-11-02: Original: 34.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 12 - 2023-11-01: Original: 26.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 12 - 2023-11-02: Original: 34.00 → Reemplazo: 2.00 (Método: mediana_dia_semana, V1: No disponible, V2: 2.00)\n",
      "DataFrame 13 - 2023-11-01: Original: 13.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 13 - 2023-11-02: Original: 13.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 14 - 2023-11-01: Original: 107.00 → Reemplazo: 4.00 (Método: mediana_dia_semana, V1: 1.00, V2: 4.00)\n",
      "DataFrame 14 - 2023-11-02: Original: 147.00 → Reemplazo: 4.00 (Método: mediana_dia_semana, V1: No disponible, V2: 4.00)\n",
      "DataFrame 15 - 2023-11-01: Original: 66.00 → Reemplazo: 2.00 (Método: mediana_dia_semana, V1: No disponible, V2: 2.00)\n",
      "DataFrame 15 - 2023-11-02: Original: 214.00 → Reemplazo: 3.00 (Método: mediana_dia_semana, V1: No disponible, V2: 3.00)\n",
      "DataFrame 16 - 2023-11-01: Original: 16.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 16 - 2023-11-02: Original: 34.00 → Reemplazo: 2.00 (Método: mediana_dia_semana, V1: No disponible, V2: 2.00)\n",
      "DataFrame 17 - 2023-11-01: Original: 9.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 17 - 2023-11-02: Original: 34.00 → Reemplazo: 1.00 (Método: valor_año_anterior, V1: 1.00, V2: 1.00)\n",
      "DataFrame 18 - 2023-11-01: Original: 9.00 → Reemplazo: 2.00 (Método: mediana_dia_semana, V1: No disponible, V2: 2.00)\n",
      "DataFrame 18 - 2023-11-02: Original: 37.00 → Reemplazo: 1.40 (Método: promedio_ponderado, V1: 1.00, V2: 2.00)\n",
      "DataFrame 19 - 2023-11-01: Original: 206.00 → Reemplazo: 9.00 (Método: mediana_dia_semana, V1: 3.00, V2: 9.00)\n",
      "DataFrame 19 - 2023-11-02: Original: 154.00 → Reemplazo: 10.00 (Método: valor_año_anterior, V1: 10.00, V2: 8.00)\n",
      "DataFrame 20 - 2023-11-01: Original: 37.00 → Reemplazo: 3.00 (Método: valor_año_anterior, V1: 3.00, V2: 4.00)\n",
      "DataFrame 20 - 2023-11-02: Original: 40.00 → Reemplazo: 3.00 (Método: mediana_dia_semana, V1: No disponible, V2: 3.00)\n",
      "DataFrame 21 - 2023-11-01: Original: 23.00 → Reemplazo: 2.00 (Método: mediana_dia_semana, V1: No disponible, V2: 2.00)\n",
      "DataFrame 21 - 2023-11-02: Original: 64.00 → Reemplazo: 1.40 (Método: promedio_ponderado, V1: 1.00, V2: 2.00)\n",
      "DataFrame 22 - 2023-11-01: Original: 27.00 → Reemplazo: 1.00 (Método: valor_año_anterior, V1: 1.00, V2: 1.00)\n",
      "DataFrame 22 - 2023-11-02: Original: 73.00 → Reemplazo: 2.00 (Método: valor_año_anterior, V1: 2.00, V2: 2.00)\n",
      "DataFrame 23 - 2023-11-01: Original: 35.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 23 - 2023-11-02: Original: 47.00 → Reemplazo: 1.40 (Método: promedio_ponderado, V1: 1.00, V2: 2.00)\n",
      "DataFrame 24 - 2023-11-01: Original: 15.00 → Reemplazo: 1.00 (Método: valor_año_anterior, V1: 1.00, V2: 1.00)\n",
      "DataFrame 24 - 2023-11-02: Original: 28.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 25 - 2023-11-01: Original: 29.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 25 - 2023-11-02: Original: 47.00 → Reemplazo: 1.00 (Método: mediana_dia_semana, V1: No disponible, V2: 1.00)\n",
      "DataFrame 26 - 2023-11-01: Original: 196.00 → Reemplazo: 10.00 (Método: valor_año_anterior, V1: 10.00, V2: 13.00)\n",
      "DataFrame 26 - 2023-11-02: Original: 290.00 → Reemplazo: 11.00 (Método: valor_año_anterior, V1: 11.00, V2: 11.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "timeseries, original_vals = handle_anomalous_dates_list(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "079065d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_y_segmentar_series_list(timeseries_list, umbral_gap=30, puntos_min=10):\n",
    "    \"\"\"\n",
    "    Identifica grupos en series temporales basados en gaps y elimina grupos pequeños.\n",
    "    Solo reincorpora los NaNs que caen dentro del rango de fechas de los datos filtrados.\n",
    "    Adaptado para lista de DataFrames con índice 'fecha'.\n",
    "    \n",
    "    Args:\n",
    "        timeseries_list: Lista de DataFrames con índice 'fecha' y columna 'cantidad'\n",
    "        umbral_gap: Número de días que define un gap entre grupos\n",
    "        puntos_min: Número mínimo de puntos que debe tener un grupo para conservarse\n",
    "    \n",
    "    Returns:\n",
    "        Lista con series filtradas\n",
    "    \"\"\"\n",
    "    resultado = []\n",
    "\n",
    "    for i, df in enumerate(tqdm(timeseries_list, desc=\"Limpiando y segmentando series\")):\n",
    "        # Verificar que es un DataFrame válido\n",
    "        if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "            resultado.append(df)\n",
    "            continue\n",
    "        \n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Verificar que el índice es 'fecha' o convertirlo\n",
    "        if df_copy.index.name != 'fecha':\n",
    "            if 'fecha' in df_copy.columns:\n",
    "                df_copy.set_index('fecha', inplace=True)\n",
    "            else:\n",
    "                print(f\"DataFrame {i}: No se encontró la columna o índice 'fecha'\")\n",
    "                resultado.append(df)\n",
    "                continue\n",
    "        \n",
    "        # Verificar que existe la columna 'cantidad'\n",
    "        if 'cantidad' not in df_copy.columns:\n",
    "            print(f\"DataFrame {i}: No se encontró la columna 'cantidad'\")\n",
    "            resultado.append(df)\n",
    "            continue\n",
    "        \n",
    "        # Asegurarse de que el índice es datetime\n",
    "        if not isinstance(df_copy.index, pd.DatetimeIndex):\n",
    "            df_copy.index = pd.to_datetime(df_copy.index)\n",
    "        \n",
    "        # Resetear índice para trabajar con columnas (temporalmente)\n",
    "        df_work = df_copy.reset_index()\n",
    "\n",
    "        # Paso 1: eliminar registros con 'cantidad' NaN y guardar aparte\n",
    "        df_nan = df_work[df_work['cantidad'].isna()]\n",
    "        df_clean = df_work.dropna(subset=['cantidad']).sort_values('fecha')\n",
    "\n",
    "        if df_clean.empty:\n",
    "            # Solo NaNs, devolver como está (con índice restaurado)\n",
    "            df_nan_final = df_nan.set_index('fecha')\n",
    "            resultado.append(df_nan_final)\n",
    "            continue\n",
    "\n",
    "        # Paso 2 y 3: calcular diferencias entre fechas sucesivas\n",
    "        df_clean['gap'] = df_clean['fecha'].diff().dt.days.fillna(0)\n",
    "\n",
    "        # Paso 4: asignar grupos según umbral_gap\n",
    "        df_clean['grupo'] = (df_clean['gap'] >= umbral_gap).cumsum()\n",
    "\n",
    "        # Paso 5: filtrar grupos con menos de puntos_min registros\n",
    "        grupos_validos = df_clean['grupo'].value_counts()\n",
    "        grupos_validos = grupos_validos[grupos_validos >= puntos_min].index\n",
    "\n",
    "        df_filtrado = df_clean[df_clean['grupo'].isin(grupos_validos)].drop(columns=['gap', 'grupo'])\n",
    "\n",
    "        # MEJORA: Reincorporar solo los NaNs que caen dentro del rango de fechas de los datos filtrados\n",
    "        if not df_filtrado.empty:\n",
    "            # Obtener el rango de fechas de los datos filtrados\n",
    "            fecha_min = df_filtrado['fecha'].min()\n",
    "            fecha_max = df_filtrado['fecha'].max()\n",
    "            \n",
    "            # Filtrar los NaNs para incluir solo los que están dentro del rango\n",
    "            df_nan_filtrado = df_nan[(df_nan['fecha'] >= fecha_min) & (df_nan['fecha'] <= fecha_max)]\n",
    "            \n",
    "            # Concatenar y ordenar\n",
    "            df_final = pd.concat([df_filtrado, df_nan_filtrado]).sort_values('fecha')\n",
    "        else:\n",
    "            # Si no quedaron datos después del filtrado, devolver un DataFrame vacío\n",
    "            df_final = pd.DataFrame(columns=df_work.columns)\n",
    "\n",
    "        # Restaurar el índice 'fecha'\n",
    "        if not df_final.empty:\n",
    "            df_final = df_final.set_index('fecha')\n",
    "        else:\n",
    "            # DataFrame vacío con índice correcto\n",
    "            df_final = pd.DataFrame(columns=df_copy.columns)\n",
    "            df_final.index.name = 'fecha'\n",
    "\n",
    "        resultado.append(df_final)\n",
    "\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a59e6255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Limpiando y segmentando series: 100%|██████████| 27/27 [00:00<00:00, 181.43it/s]\n"
     ]
    }
   ],
   "source": [
    "timeseries = limpiar_y_segmentar_series_list(\n",
    "    timeseries, \n",
    "    umbral_gap=30,    # Días para considerar un gap\n",
    "    puntos_min=10     # Mínimo de puntos por grupo\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d7f35a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeseries[0] → D\n",
      "timeseries[1] → D\n",
      "timeseries[2] → D\n",
      "timeseries[3] → D\n",
      "timeseries[4] → D\n",
      "timeseries[5] → D\n",
      "timeseries[6] → None\n",
      "timeseries[7] → None\n",
      "timeseries[8] → D\n",
      "timeseries[9] → D\n",
      "timeseries[10] → D\n",
      "timeseries[11] → D\n",
      "timeseries[12] → D\n",
      "timeseries[13] → D\n",
      "timeseries[14] → D\n",
      "timeseries[15] → D\n",
      "timeseries[16] → D\n",
      "timeseries[17] → D\n",
      "timeseries[18] → D\n",
      "timeseries[19] → D\n",
      "timeseries[20] → D\n",
      "timeseries[21] → D\n",
      "timeseries[22] → D\n",
      "timeseries[23] → D\n",
      "timeseries[24] → D\n",
      "timeseries[25] → D\n",
      "timeseries[26] → D\n"
     ]
    }
   ],
   "source": [
    "freqs = check_frequencies(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ac723ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Completando series temporales: 100%|██████████| 27/27 [00:00<00:00, 235.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0: 1370 → 1370 registros (0 fechas agregadas)\n",
      "DataFrame 1: 1282 → 1282 registros (0 fechas agregadas)\n",
      "DataFrame 2: 1374 → 1374 registros (0 fechas agregadas)\n",
      "DataFrame 3: 1370 → 1370 registros (0 fechas agregadas)\n",
      "DataFrame 4: 1372 → 1372 registros (0 fechas agregadas)\n",
      "DataFrame 5: 1360 → 1360 registros (0 fechas agregadas)\n",
      "DataFrame 6: 1367 → 1373 registros (6 fechas agregadas)\n",
      "DataFrame 7: 1330 → 1339 registros (9 fechas agregadas)\n",
      "DataFrame 8: 1198 → 1198 registros (0 fechas agregadas)\n",
      "DataFrame 9: 1366 → 1366 registros (0 fechas agregadas)\n",
      "DataFrame 10: 1373 → 1373 registros (0 fechas agregadas)\n",
      "DataFrame 11: 1356 → 1356 registros (0 fechas agregadas)\n",
      "DataFrame 12: 1366 → 1366 registros (0 fechas agregadas)\n",
      "DataFrame 13: 1374 → 1374 registros (0 fechas agregadas)\n",
      "DataFrame 14: 1374 → 1374 registros (0 fechas agregadas)\n",
      "DataFrame 15: 1368 → 1368 registros (0 fechas agregadas)\n",
      "DataFrame 16: 1349 → 1349 registros (0 fechas agregadas)\n",
      "DataFrame 17: 1316 → 1316 registros (0 fechas agregadas)\n",
      "DataFrame 18: 1092 → 1092 registros (0 fechas agregadas)\n",
      "DataFrame 19: 829 → 829 registros (0 fechas agregadas)\n",
      "DataFrame 20: 904 → 904 registros (0 fechas agregadas)\n",
      "DataFrame 21: 1049 → 1049 registros (0 fechas agregadas)\n",
      "DataFrame 22: 1175 → 1175 registros (0 fechas agregadas)\n",
      "DataFrame 23: 1359 → 1359 registros (0 fechas agregadas)\n",
      "DataFrame 24: 1358 → 1358 registros (0 fechas agregadas)\n",
      "DataFrame 25: 1270 → 1270 registros (0 fechas agregadas)\n",
      "DataFrame 26: 882 → 882 registros (0 fechas agregadas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "timeseries = completar_series_temporales_list(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33e385ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeseries[0] → <Day>\n",
      "timeseries[1] → <Day>\n",
      "timeseries[2] → <Day>\n",
      "timeseries[3] → <Day>\n",
      "timeseries[4] → <Day>\n",
      "timeseries[5] → <Day>\n",
      "timeseries[6] → <Day>\n",
      "timeseries[7] → <Day>\n",
      "timeseries[8] → <Day>\n",
      "timeseries[9] → <Day>\n",
      "timeseries[10] → <Day>\n",
      "timeseries[11] → <Day>\n",
      "timeseries[12] → <Day>\n",
      "timeseries[13] → <Day>\n",
      "timeseries[14] → <Day>\n",
      "timeseries[15] → <Day>\n",
      "timeseries[16] → <Day>\n",
      "timeseries[17] → <Day>\n",
      "timeseries[18] → <Day>\n",
      "timeseries[19] → <Day>\n",
      "timeseries[20] → <Day>\n",
      "timeseries[21] → <Day>\n",
      "timeseries[22] → <Day>\n",
      "timeseries[23] → <Day>\n",
      "timeseries[24] → <Day>\n",
      "timeseries[25] → <Day>\n",
      "timeseries[26] → <Day>\n"
     ]
    }
   ],
   "source": [
    "freqs = check_frequencies(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4e52f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataframe_0': 33.0,\n",
       " 'dataframe_1': 20.0,\n",
       " 'dataframe_2': 25.0,\n",
       " 'dataframe_3': 31.0,\n",
       " 'dataframe_4': 28.0,\n",
       " 'dataframe_5': 44.0,\n",
       " 'dataframe_6': 4.0,\n",
       " 'dataframe_7': 5.0,\n",
       " 'dataframe_8': 41.0,\n",
       " 'dataframe_9': 49.0,\n",
       " 'dataframe_10': 133.0,\n",
       " 'dataframe_11': 46.0,\n",
       " 'dataframe_12': 60.0,\n",
       " 'dataframe_13': 26.0,\n",
       " 'dataframe_14': 254.0,\n",
       " 'dataframe_15': 280.0,\n",
       " 'dataframe_16': 50.0,\n",
       " 'dataframe_17': 43.0,\n",
       " 'dataframe_18': 46.0,\n",
       " 'dataframe_19': 360.0,\n",
       " 'dataframe_20': 77.0,\n",
       " 'dataframe_21': 87.0,\n",
       " 'dataframe_22': 100.0,\n",
       " 'dataframe_23': 82.0,\n",
       " 'dataframe_24': 43.0,\n",
       " 'dataframe_25': 76.0,\n",
       " 'dataframe_26': 486.0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d900a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redistribuir_valores_list(timeseries_list, resultados):\n",
    "    \"\"\"\n",
    "    Redistribuye las sumas de 2023-11-01 y 2023-11-02 en el intervalo [2023-09-10, 2023-10-31],\n",
    "    distribuyendo solo en puntos correspondientes al año anterior [2022-09-10, 2022-10-31],\n",
    "    respetando el ratio del año anterior.\n",
    "    \n",
    "    Args:\n",
    "        timeseries_list: Lista de DataFrames con índice 'fecha' y columna 'cantidad'\n",
    "        resultados: Diccionario con sumas por DataFrame {'dataframe_0': suma_total, ...}\n",
    "        \n",
    "    Returns:\n",
    "        Lista con DataFrames ajustados\n",
    "    \"\"\"\n",
    "    resultado_ajustado = []\n",
    "    \n",
    "    # Fechas de outliers y períodos\n",
    "    fecha_outlier1 = pd.Timestamp(\"2023-11-01\")\n",
    "    fecha_outlier2 = pd.Timestamp(\"2023-11-02\")\n",
    "    fecha_destino_inicio = pd.Timestamp(\"2023-09-10\")\n",
    "    fecha_destino_fin = pd.Timestamp(\"2023-10-31\")\n",
    "    fecha_base_inicio = pd.Timestamp(\"2022-09-10\")\n",
    "    fecha_base_fin = pd.Timestamp(\"2022-10-31\")\n",
    "    \n",
    "    # Imprimir las claves disponibles para diagnóstico\n",
    "    print(\"Claves disponibles en resultados:\", list(resultados.keys()))\n",
    "    \n",
    "    for i, df in enumerate(tqdm(timeseries_list, desc=\"Redistribuyendo valores\")):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Procesando DataFrame {i}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Verificar que es un DataFrame válido\n",
    "        if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "            resultado_ajustado.append(df)\n",
    "            continue\n",
    "        \n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Verificar que el índice es 'fecha' o convertirlo\n",
    "        if df_copy.index.name != 'fecha':\n",
    "            if 'fecha' in df_copy.columns:\n",
    "                df_copy.set_index('fecha', inplace=True)\n",
    "            else:\n",
    "                print(f\"DataFrame {i}: No se encontró la columna o índice 'fecha'\")\n",
    "                resultado_ajustado.append(df)\n",
    "                continue\n",
    "        \n",
    "        # Verificar columna 'cantidad'\n",
    "        if 'cantidad' not in df_copy.columns:\n",
    "            print(f\"DataFrame {i}: No se encontró la columna 'cantidad'\")\n",
    "            resultado_ajustado.append(df)\n",
    "            continue\n",
    "        \n",
    "        # Asegurar que el índice es datetime\n",
    "        if not isinstance(df_copy.index, pd.DatetimeIndex):\n",
    "            df_copy.index = pd.to_datetime(df_copy.index)\n",
    "        \n",
    "        # Obtener suma a redistribuir del diccionario resultados\n",
    "        clave_df = f'dataframe_{i}'\n",
    "        print(f\"Buscando clave: {clave_df}\")\n",
    "        \n",
    "        if clave_df not in resultados:\n",
    "            print(f\"DataFrame {i} ({clave_df}): No se encontró en el diccionario de resultados\")\n",
    "            resultado_ajustado.append(df_copy)\n",
    "            continue\n",
    "        \n",
    "        suma_a_redistribuir = resultados[clave_df]\n",
    "        print(f\"Suma a redistribuir: {suma_a_redistribuir}\")\n",
    "        \n",
    "        if suma_a_redistribuir <= 0:\n",
    "            print(\"Suma negativa o cero, no hay necesidad de redistribuir\")\n",
    "            resultado_ajustado.append(df_copy)\n",
    "            continue\n",
    "        \n",
    "        # Paso 1: Obtener datos del período base (2022)\n",
    "        mask_base = (df_copy.index >= fecha_base_inicio) & (df_copy.index <= fecha_base_fin)\n",
    "        df_base = df_copy.loc[mask_base].dropna(subset=['cantidad'])\n",
    "        \n",
    "        if df_base.empty:\n",
    "            print(\"No hay datos válidos en el período base 2022, no se puede redistribuir\")\n",
    "            resultado_ajustado.append(df_copy)\n",
    "            continue\n",
    "        \n",
    "        print(f\"Días con valores en 2022: {len(df_base)}\")\n",
    "        \n",
    "        # Paso 2: Crear mapeo de fechas entre 2022 y 2023\n",
    "        fechas_2022 = pd.date_range(start=fecha_base_inicio, end=fecha_base_fin, freq='D')\n",
    "        fechas_2023 = pd.date_range(start=fecha_destino_inicio, end=fecha_destino_fin, freq='D')\n",
    "        \n",
    "        if len(fechas_2022) != len(fechas_2023):\n",
    "            print(f\"Los períodos tienen diferente duración: 2022 ({len(fechas_2022)} días) vs 2023 ({len(fechas_2023)} días)\")\n",
    "            resultado_ajustado.append(df_copy)\n",
    "            continue\n",
    "        \n",
    "        # Crear mapeo entre fechas\n",
    "        mapa_2022_a_2023 = dict(zip(fechas_2022, fechas_2023))\n",
    "        \n",
    "        # Paso 3: Identificar fechas en 2023 que corresponden a valores existentes en 2022\n",
    "        fechas_destino_correspondientes = []\n",
    "        valores_base_correspondientes = []\n",
    "        \n",
    "        for fecha_2022 in df_base.index:\n",
    "            if fecha_2022 in mapa_2022_a_2023:\n",
    "                fecha_2023 = mapa_2022_a_2023[fecha_2022]\n",
    "                fechas_destino_correspondientes.append(fecha_2023)\n",
    "                valores_base_correspondientes.append(df_base.loc[fecha_2022, 'cantidad'])\n",
    "        \n",
    "        if not fechas_destino_correspondientes:\n",
    "            print(\"No hay fechas correspondientes entre 2022 y 2023\")\n",
    "            resultado_ajustado.append(df_copy)\n",
    "            continue\n",
    "        \n",
    "        # Paso 4: Filtrar fechas que no tienen valores en 2023\n",
    "        mask_destino = (df_copy.index >= fecha_destino_inicio) & (df_copy.index <= fecha_destino_fin)\n",
    "        fechas_con_valores_2023 = set(df_copy.loc[mask_destino].dropna(subset=['cantidad']).index)\n",
    "        \n",
    "        # Fechas disponibles para imputar (que corresponden a 2022 pero no tienen valor en 2023)\n",
    "        fechas_disponibles = []\n",
    "        valores_correspondientes = []\n",
    "        \n",
    "        for fecha_2023, valor_2022 in zip(fechas_destino_correspondientes, valores_base_correspondientes):\n",
    "            if fecha_2023 not in fechas_con_valores_2023:\n",
    "                fechas_disponibles.append(fecha_2023)\n",
    "                valores_correspondientes.append(valor_2022)\n",
    "        \n",
    "        print(f\"Fechas disponibles para imputar: {len(fechas_disponibles)}\")\n",
    "        \n",
    "        if not fechas_disponibles:\n",
    "            print(\"No hay fechas disponibles para imputar\")\n",
    "            resultado_ajustado.append(df_copy)\n",
    "            continue\n",
    "        \n",
    "        # Paso 5: Si hay 20 o más puntos, seleccionar la mitad intercalados\n",
    "        if len(fechas_disponibles) >= 20:\n",
    "            # Calcular cuántos puntos seleccionar (la mitad)\n",
    "            puntos_a_seleccionar = len(fechas_disponibles) // 2\n",
    "            \n",
    "            # Crear DataFrame para ordenar por valor\n",
    "            df_disponibles = pd.DataFrame({\n",
    "                'fecha': fechas_disponibles,\n",
    "                'valor_2022': valores_correspondientes\n",
    "            }).sort_values('valor_2022', ascending=False)\n",
    "            \n",
    "            # Calcular el paso para intercalar\n",
    "            total_disponibles = len(df_disponibles)\n",
    "            paso = max(1, total_disponibles // puntos_a_seleccionar)\n",
    "            \n",
    "            # Tomar puntos intercalados\n",
    "            indices_seleccionados = list(range(0, total_disponibles, paso))[:puntos_a_seleccionar]\n",
    "            df_seleccionados = df_disponibles.iloc[indices_seleccionados]\n",
    "            \n",
    "            fechas_a_imputar = df_seleccionados['fecha'].tolist()\n",
    "            valores_base_para_ratio = df_seleccionados['valor_2022'].tolist()\n",
    "            \n",
    "            print(f\"Seleccionados {len(fechas_a_imputar)} puntos intercalados (mitad) de {len(fechas_disponibles)} disponibles\")\n",
    "        else:\n",
    "            fechas_a_imputar = fechas_disponibles\n",
    "            valores_base_para_ratio = valores_correspondientes\n",
    "            print(f\"Usando todos los {len(fechas_a_imputar)} puntos disponibles\")\n",
    "        \n",
    "        # Paso 6: Calcular ratios basados en valores de 2022\n",
    "        suma_base = sum(valores_base_para_ratio)\n",
    "        if suma_base == 0:\n",
    "            print(\"Suma de valores base es cero, usando distribución uniforme\")\n",
    "            ratios = [1.0 / len(fechas_a_imputar)] * len(fechas_a_imputar)\n",
    "        else:\n",
    "            ratios = [valor / suma_base for valor in valores_base_para_ratio]\n",
    "        \n",
    "        # Paso 7: Calcular valores a imputar\n",
    "        valores_a_imputar = [ratio * suma_a_redistribuir for ratio in ratios]\n",
    "        \n",
    "        print(f\"Valores a imputar - Min: {min(valores_a_imputar):.2f}, Max: {max(valores_a_imputar):.2f}, Total: {sum(valores_a_imputar):.2f}\")\n",
    "        \n",
    "        # Paso 8: Aplicar los valores al DataFrame\n",
    "        df_final = df_copy.copy()\n",
    "        \n",
    "        for fecha, valor in zip(fechas_a_imputar, valores_a_imputar):\n",
    "            if fecha in df_final.index:\n",
    "                df_final.loc[fecha, 'cantidad'] = valor\n",
    "            else:\n",
    "                # Crear nueva fila con las mismas columnas que el DataFrame original\n",
    "                nueva_fila = pd.Series(index=df_final.columns, dtype=object)\n",
    "                nueva_fila['cantidad'] = valor\n",
    "                \n",
    "                # Rellenar otras columnas con valores típicos del DataFrame\n",
    "                for col in df_final.columns:\n",
    "                    if col != 'cantidad':\n",
    "                        valores_no_nulos = df_final[col].dropna()\n",
    "                        if not valores_no_nulos.empty:\n",
    "                            if valores_no_nulos.dtype in ['object', 'string']:\n",
    "                                nueva_fila[col] = valores_no_nulos.mode().iloc[0] if len(valores_no_nulos.mode()) > 0 else valores_no_nulos.iloc[0]\n",
    "                            else:\n",
    "                                moda = valores_no_nulos.mode()\n",
    "                                nueva_fila[col] = moda.iloc[0] if len(moda) > 0 else valores_no_nulos.median()\n",
    "                \n",
    "                # Agregar la nueva fila\n",
    "                nueva_fila.name = fecha\n",
    "                df_final = pd.concat([df_final, nueva_fila.to_frame().T])\n",
    "        \n",
    "        # Ordenar por índice (fecha)\n",
    "        df_final = df_final.sort_index()\n",
    "        \n",
    "        # Verificación final\n",
    "        suma_destino_final = df_final.loc[mask_destino, 'cantidad'].sum()\n",
    "        suma_outliers_final = 0\n",
    "        if fecha_outlier1 in df_final.index:\n",
    "            suma_outliers_final += df_final.loc[fecha_outlier1, 'cantidad']\n",
    "        if fecha_outlier2 in df_final.index:\n",
    "            suma_outliers_final += df_final.loc[fecha_outlier2, 'cantidad']\n",
    "        \n",
    "        print(f\"Verificación final:\")\n",
    "        print(f\"Suma en período destino: {suma_destino_final:.2f}\")\n",
    "        print(f\"Suma en outliers: {suma_outliers_final:.2f}\")\n",
    "        print(f\"Total redistribuido: {suma_destino_final:.2f}\")\n",
    "        print(f\"Diferencia con objetivo: {suma_destino_final - suma_a_redistribuir:.2f}\")\n",
    "        \n",
    "        resultado_ajustado.append(df_final)\n",
    "    \n",
    "    return resultado_ajustado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db93feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claves disponibles en resultados: ['dataframe_0', 'dataframe_1', 'dataframe_2', 'dataframe_3', 'dataframe_4', 'dataframe_5', 'dataframe_6', 'dataframe_7', 'dataframe_8', 'dataframe_9', 'dataframe_10', 'dataframe_11', 'dataframe_12', 'dataframe_13', 'dataframe_14', 'dataframe_15', 'dataframe_16', 'dataframe_17', 'dataframe_18', 'dataframe_19', 'dataframe_20', 'dataframe_21', 'dataframe_22', 'dataframe_23', 'dataframe_24', 'dataframe_25', 'dataframe_26']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Redistribuyendo valores:   0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Procesando DataFrame 0\n",
      "============================================================\n",
      "Buscando clave: dataframe_0\n",
      "Suma a redistribuir: 33.0\n",
      "Días con valores en 2022: 14\n",
      "Fechas disponibles para imputar: 14\n",
      "Usando todos los 14 puntos disponibles\n",
      "Valores a imputar - Min: 1.83, Max: 7.33, Total: 33.00\n",
      "Verificación final:\n",
      "Suma en período destino: 33.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 33.00\n",
      "Diferencia con objetivo: 0.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 1\n",
      "============================================================\n",
      "Buscando clave: dataframe_1\n",
      "Suma a redistribuir: 20.0\n",
      "Días con valores en 2022: 17\n",
      "Fechas disponibles para imputar: 16\n",
      "Usando todos los 16 puntos disponibles\n",
      "Valores a imputar - Min: 0.67, Max: 2.67, Total: 20.00\n",
      "Verificación final:\n",
      "Suma en período destino: 21.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 21.00\n",
      "Diferencia con objetivo: 1.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 2\n",
      "============================================================\n",
      "Buscando clave: dataframe_2\n",
      "Suma a redistribuir: 25.0\n",
      "Días con valores en 2022: 27\n",
      "Fechas disponibles para imputar: 26\n",
      "Seleccionados 13 puntos intercalados (mitad) de 26 disponibles\n",
      "Valores a imputar - Min: 0.93, Max: 5.56, Total: 25.00\n",
      "Verificación final:\n",
      "Suma en período destino: 26.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 26.00\n",
      "Diferencia con objetivo: 1.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 3\n",
      "============================================================\n",
      "Buscando clave: dataframe_3\n",
      "Suma a redistribuir: 31.0\n",
      "Días con valores en 2022: 22\n",
      "Fechas disponibles para imputar: 21\n",
      "Seleccionados 10 puntos intercalados (mitad) de 21 disponibles\n",
      "Valores a imputar - Min: 1.55, Max: 7.75, Total: 31.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Redistribuyendo valores: 100%|██████████| 27/27 [00:00<00:00, 136.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificación final:\n",
      "Suma en período destino: 33.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 33.00\n",
      "Diferencia con objetivo: 2.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 4\n",
      "============================================================\n",
      "Buscando clave: dataframe_4\n",
      "Suma a redistribuir: 28.0\n",
      "Días con valores en 2022: 21\n",
      "Fechas disponibles para imputar: 20\n",
      "Seleccionados 10 puntos intercalados (mitad) de 20 disponibles\n",
      "Valores a imputar - Min: 1.33, Max: 8.00, Total: 28.00\n",
      "Verificación final:\n",
      "Suma en período destino: 30.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 30.00\n",
      "Diferencia con objetivo: 2.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 5\n",
      "============================================================\n",
      "Buscando clave: dataframe_5\n",
      "Suma a redistribuir: 44.0\n",
      "Días con valores en 2022: 33\n",
      "Fechas disponibles para imputar: 33\n",
      "Seleccionados 16 puntos intercalados (mitad) de 33 disponibles\n",
      "Valores a imputar - Min: 1.13, Max: 6.77, Total: 44.00\n",
      "Verificación final:\n",
      "Suma en período destino: 46.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 46.00\n",
      "Diferencia con objetivo: 2.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 6\n",
      "============================================================\n",
      "Buscando clave: dataframe_6\n",
      "Suma a redistribuir: 4.0\n",
      "Días con valores en 2022: 31\n",
      "Fechas disponibles para imputar: 31\n",
      "Seleccionados 15 puntos intercalados (mitad) de 31 disponibles\n",
      "Valores a imputar - Min: 0.14, Max: 0.83, Total: 4.00\n",
      "Verificación final:\n",
      "Suma en período destino: 4.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 4.00\n",
      "Diferencia con objetivo: 0.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 7\n",
      "============================================================\n",
      "Buscando clave: dataframe_7\n",
      "Suma a redistribuir: 5.0\n",
      "Días con valores en 2022: 7\n",
      "Fechas disponibles para imputar: 7\n",
      "Usando todos los 7 puntos disponibles\n",
      "Valores a imputar - Min: 0.29, Max: 2.06, Total: 5.00\n",
      "Verificación final:\n",
      "Suma en período destino: 5.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 5.00\n",
      "Diferencia con objetivo: 0.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 8\n",
      "============================================================\n",
      "Buscando clave: dataframe_8\n",
      "Suma a redistribuir: 41.0\n",
      "Días con valores en 2022: 42\n",
      "Fechas disponibles para imputar: 42\n",
      "Seleccionados 21 puntos intercalados (mitad) de 42 disponibles\n",
      "Valores a imputar - Min: 0.65, Max: 5.86, Total: 41.00\n",
      "Verificación final:\n",
      "Suma en período destino: 41.00\n",
      "Suma en outliers: 3.40\n",
      "Total redistribuido: 41.00\n",
      "Diferencia con objetivo: -0.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 9\n",
      "============================================================\n",
      "Buscando clave: dataframe_9\n",
      "Suma a redistribuir: 49.0\n",
      "Días con valores en 2022: 8\n",
      "Fechas disponibles para imputar: 7\n",
      "Usando todos los 7 puntos disponibles\n",
      "Valores a imputar - Min: 3.77, Max: 15.08, Total: 49.00\n",
      "Verificación final:\n",
      "Suma en período destino: 51.00\n",
      "Suma en outliers: 3.00\n",
      "Total redistribuido: 51.00\n",
      "Diferencia con objetivo: 2.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 10\n",
      "============================================================\n",
      "Buscando clave: dataframe_10\n",
      "Suma a redistribuir: 133.0\n",
      "Días con valores en 2022: 17\n",
      "Fechas disponibles para imputar: 16\n",
      "Usando todos los 16 puntos disponibles\n",
      "Valores a imputar - Min: 3.59, Max: 14.38, Total: 133.00\n",
      "Verificación final:\n",
      "Suma en período destino: 136.00\n",
      "Suma en outliers: 8.00\n",
      "Total redistribuido: 136.00\n",
      "Diferencia con objetivo: 3.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 11\n",
      "============================================================\n",
      "Buscando clave: dataframe_11\n",
      "Suma a redistribuir: 46.0\n",
      "Días con valores en 2022: 23\n",
      "Fechas disponibles para imputar: 23\n",
      "Seleccionados 11 puntos intercalados (mitad) de 23 disponibles\n",
      "Valores a imputar - Min: 2.00, Max: 10.00, Total: 46.00\n",
      "Verificación final:\n",
      "Suma en período destino: 46.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 46.00\n",
      "Diferencia con objetivo: 0.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 12\n",
      "============================================================\n",
      "Buscando clave: dataframe_12\n",
      "Suma a redistribuir: 60.0\n",
      "Días con valores en 2022: 32\n",
      "Fechas disponibles para imputar: 31\n",
      "Seleccionados 15 puntos intercalados (mitad) de 31 disponibles\n",
      "Valores a imputar - Min: 1.30, Max: 7.83, Total: 60.00\n",
      "Verificación final:\n",
      "Suma en período destino: 62.00\n",
      "Suma en outliers: 3.00\n",
      "Total redistribuido: 62.00\n",
      "Diferencia con objetivo: 2.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 13\n",
      "============================================================\n",
      "Buscando clave: dataframe_13\n",
      "Suma a redistribuir: 26.0\n",
      "Días con valores en 2022: 22\n",
      "Fechas disponibles para imputar: 22\n",
      "Seleccionados 11 puntos intercalados (mitad) de 22 disponibles\n",
      "Valores a imputar - Min: 0.93, Max: 12.07, Total: 26.00\n",
      "Verificación final:\n",
      "Suma en período destino: 26.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 26.00\n",
      "Diferencia con objetivo: 0.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 14\n",
      "============================================================\n",
      "Buscando clave: dataframe_14\n",
      "Suma a redistribuir: 254.0\n",
      "Días con valores en 2022: 44\n",
      "Fechas disponibles para imputar: 43\n",
      "Seleccionados 21 puntos intercalados (mitad) de 43 disponibles\n",
      "Valores a imputar - Min: 2.02, Max: 36.29, Total: 254.00\n",
      "Verificación final:\n",
      "Suma en período destino: 259.00\n",
      "Suma en outliers: 8.00\n",
      "Total redistribuido: 259.00\n",
      "Diferencia con objetivo: 5.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 15\n",
      "============================================================\n",
      "Buscando clave: dataframe_15\n",
      "Suma a redistribuir: 280.0\n",
      "Días con valores en 2022: 16\n",
      "Fechas disponibles para imputar: 15\n",
      "Usando todos los 15 puntos disponibles\n",
      "Valores a imputar - Min: 8.00, Max: 48.00, Total: 280.00\n",
      "Verificación final:\n",
      "Suma en período destino: 282.00\n",
      "Suma en outliers: 5.00\n",
      "Total redistribuido: 282.00\n",
      "Diferencia con objetivo: 2.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 16\n",
      "============================================================\n",
      "Buscando clave: dataframe_16\n",
      "Suma a redistribuir: 50.0\n",
      "Días con valores en 2022: 28\n",
      "Fechas disponibles para imputar: 27\n",
      "Seleccionados 13 puntos intercalados (mitad) de 27 disponibles\n",
      "Valores a imputar - Min: 1.85, Max: 12.96, Total: 50.00\n",
      "Verificación final:\n",
      "Suma en período destino: 52.00\n",
      "Suma en outliers: 3.00\n",
      "Total redistribuido: 52.00\n",
      "Diferencia con objetivo: 2.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 17\n",
      "============================================================\n",
      "Buscando clave: dataframe_17\n",
      "Suma a redistribuir: 43.0\n",
      "Días con valores en 2022: 13\n",
      "Fechas disponibles para imputar: 13\n",
      "Usando todos los 13 puntos disponibles\n",
      "Valores a imputar - Min: 2.05, Max: 8.19, Total: 43.00\n",
      "Verificación final:\n",
      "Suma en período destino: 46.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 46.00\n",
      "Diferencia con objetivo: 3.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 18\n",
      "============================================================\n",
      "Buscando clave: dataframe_18\n",
      "Suma a redistribuir: 46.0\n",
      "Días con valores en 2022: 47\n",
      "Fechas disponibles para imputar: 44\n",
      "Seleccionados 22 puntos intercalados (mitad) de 44 disponibles\n",
      "Valores a imputar - Min: 0.41, Max: 4.52, Total: 46.00\n",
      "Verificación final:\n",
      "Suma en período destino: 52.00\n",
      "Suma en outliers: 3.40\n",
      "Total redistribuido: 52.00\n",
      "Diferencia con objetivo: 6.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 19\n",
      "============================================================\n",
      "Buscando clave: dataframe_19\n",
      "Suma a redistribuir: 360.0\n",
      "Días con valores en 2022: 45\n",
      "Fechas disponibles para imputar: 44\n",
      "Seleccionados 22 puntos intercalados (mitad) de 44 disponibles\n",
      "Valores a imputar - Min: 3.44, Max: 65.45, Total: 360.00\n",
      "Verificación final:\n",
      "Suma en período destino: 374.00\n",
      "Suma en outliers: 19.00\n",
      "Total redistribuido: 374.00\n",
      "Diferencia con objetivo: 14.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 20\n",
      "============================================================\n",
      "Buscando clave: dataframe_20\n",
      "Suma a redistribuir: 77.0\n",
      "Días con valores en 2022: 45\n",
      "Fechas disponibles para imputar: 44\n",
      "Seleccionados 22 puntos intercalados (mitad) de 44 disponibles\n",
      "Valores a imputar - Min: 0.62, Max: 11.18, Total: 77.00\n",
      "Verificación final:\n",
      "Suma en período destino: 81.00\n",
      "Suma en outliers: 6.00\n",
      "Total redistribuido: 81.00\n",
      "Diferencia con objetivo: 4.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 21\n",
      "============================================================\n",
      "Buscando clave: dataframe_21\n",
      "Suma a redistribuir: 87.0\n",
      "Días con valores en 2022: 40\n",
      "Fechas disponibles para imputar: 39\n",
      "Seleccionados 19 puntos intercalados (mitad) de 39 disponibles\n",
      "Valores a imputar - Min: 1.64, Max: 14.77, Total: 87.00\n",
      "Verificación final:\n",
      "Suma en período destino: 89.00\n",
      "Suma en outliers: 3.40\n",
      "Total redistribuido: 89.00\n",
      "Diferencia con objetivo: 2.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 22\n",
      "============================================================\n",
      "Buscando clave: dataframe_22\n",
      "Suma a redistribuir: 100.0\n",
      "Días con valores en 2022: 33\n",
      "Fechas disponibles para imputar: 32\n",
      "Seleccionados 16 puntos intercalados (mitad) de 32 disponibles\n",
      "Valores a imputar - Min: 2.63, Max: 18.42, Total: 100.00\n",
      "Verificación final:\n",
      "Suma en período destino: 101.00\n",
      "Suma en outliers: 3.00\n",
      "Total redistribuido: 101.00\n",
      "Diferencia con objetivo: 1.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 23\n",
      "============================================================\n",
      "Buscando clave: dataframe_23\n",
      "Suma a redistribuir: 82.0\n",
      "Días con valores en 2022: 29\n",
      "Fechas disponibles para imputar: 28\n",
      "Seleccionados 14 puntos intercalados (mitad) de 28 disponibles\n",
      "Valores a imputar - Min: 2.65, Max: 15.87, Total: 82.00\n",
      "Verificación final:\n",
      "Suma en período destino: 83.00\n",
      "Suma en outliers: 2.40\n",
      "Total redistribuido: 83.00\n",
      "Diferencia con objetivo: 1.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 24\n",
      "============================================================\n",
      "Buscando clave: dataframe_24\n",
      "Suma a redistribuir: 43.0\n",
      "Días con valores en 2022: 25\n",
      "Fechas disponibles para imputar: 24\n",
      "Seleccionados 12 puntos intercalados (mitad) de 24 disponibles\n",
      "Valores a imputar - Min: 2.69, Max: 10.75, Total: 43.00\n",
      "Verificación final:\n",
      "Suma en período destino: 44.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 44.00\n",
      "Diferencia con objetivo: 1.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 25\n",
      "============================================================\n",
      "Buscando clave: dataframe_25\n",
      "Suma a redistribuir: 76.0\n",
      "Días con valores en 2022: 26\n",
      "Fechas disponibles para imputar: 25\n",
      "Seleccionados 12 puntos intercalados (mitad) de 25 disponibles\n",
      "Valores a imputar - Min: 4.00, Max: 16.00, Total: 76.00\n",
      "Verificación final:\n",
      "Suma en período destino: 78.00\n",
      "Suma en outliers: 2.00\n",
      "Total redistribuido: 78.00\n",
      "Diferencia con objetivo: 2.00\n",
      "\n",
      "============================================================\n",
      "Procesando DataFrame 26\n",
      "============================================================\n",
      "Buscando clave: dataframe_26\n",
      "Suma a redistribuir: 486.0\n",
      "Días con valores en 2022: 15\n",
      "Fechas disponibles para imputar: 15\n",
      "Usando todos los 15 puntos disponibles\n",
      "Valores a imputar - Min: 1.87, Max: 80.38, Total: 486.00\n",
      "Verificación final:\n",
      "Suma en período destino: 492.00\n",
      "Suma en outliers: 21.00\n",
      "Total redistribuido: 492.00\n",
      "Diferencia con objetivo: 6.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "timeseries = redistribuir_valores_list(timeseries, resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1af9215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def codificar_columnas_categoricas(lista_dataframes):\n",
    "    \"\"\"\n",
    "    Codifica las columnas categóricas usando códigos numéricos únicos para cada valor.\n",
    "    Las columnas a codificar son: Tipo_Producto, segmento_producto, supergrupo_producto, \n",
    "    grupo_producto y subgrupo_producto.\n",
    "    \n",
    "    Args:\n",
    "        lista_dataframes: Lista de dataframes con columnas categóricas\n",
    "    \n",
    "    Returns:\n",
    "        Lista de dataframes con columnas categóricas codificadas y diccionario de mapeos\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Columnas categóricas a codificar\n",
    "    columnas_categoricas = [\n",
    "        'Tipo_Producto', \n",
    "        'segmento_producto', \n",
    "        'supergrupo_producto', \n",
    "        'grupo_producto', \n",
    "        'subgrupo_producto'\n",
    "    ]\n",
    "    \n",
    "    # Diccionario para almacenar los mapeos para cada columna\n",
    "    mapeos = {col: {} for col in columnas_categoricas}\n",
    "    \n",
    "    # Lista para almacenar los dataframes codificados\n",
    "    dataframes_codificados = []\n",
    "    \n",
    "    # Crear mapeos para cada columna categórica\n",
    "    codigo_actual = {}\n",
    "    for col in columnas_categoricas:\n",
    "        codigo_actual[col] = 0\n",
    "    \n",
    "    # Iterar sobre cada dataframe para crear los mapeos\n",
    "    for df in lista_dataframes:\n",
    "        for col in columnas_categoricas:\n",
    "            if col in df.columns:\n",
    "                # Obtener el valor único en esta columna para este dataframe\n",
    "                # (asumiendo que cada dataframe tiene un solo valor para cada columna categórica)\n",
    "                if len(df) > 0:\n",
    "                    valor = df[col].iloc[0]\n",
    "                    \n",
    "                    # Si el valor no está en el mapeo, asignarle un código\n",
    "                    if valor not in mapeos[col]:\n",
    "                        mapeos[col][valor] = codigo_actual[col]\n",
    "                        codigo_actual[col] += 1\n",
    "    \n",
    "    # Aplicar los mapeos a cada dataframe\n",
    "    for df in lista_dataframes:\n",
    "        df_codificado = df.copy()\n",
    "        \n",
    "        for col in columnas_categoricas:\n",
    "            if col in df.columns:\n",
    "                if len(df) > 0:\n",
    "                    valor = df[col].iloc[0]\n",
    "                    codigo = mapeos[col][valor]\n",
    "                    \n",
    "                    # Reemplazar el valor categórico con su código\n",
    "                    df_codificado[col] = codigo\n",
    "        \n",
    "        dataframes_codificados.append(df_codificado)\n",
    "    \n",
    "    return dataframes_codificados, mapeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e366cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries,mapeos = codificar_columnas_categoricas(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68d455c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregar_columnas_temporales_con_vectores(lista_dataframes):\n",
    "    \"\"\"\n",
    "    Agrega columnas temporales y vector dinámico V1 a cada dataframe.\n",
    "    \n",
    "    Args:\n",
    "        lista_dataframes: Lista de dataframes con índice de fecha\n",
    "    \n",
    "    Returns:\n",
    "        Lista de dataframes con las nuevas columnas temporales y vector V1\n",
    "        \n",
    "    Columnas agregadas:\n",
    "        V1: Vector de 0s excepto en fechas 2023-09-10 hasta 2023-11-02 (valor = 1)\n",
    "        day: Día del mes (1-31)\n",
    "        weekday: Día de la semana (0=lunes, 6=domingo)\n",
    "        week: Número de semana del año (1-53)\n",
    "        month: Mes (1-12)\n",
    "        quarter: Trimestre (1-4)\n",
    "    \"\"\"\n",
    "    resultado = []\n",
    "    \n",
    "    # Definir el rango de fechas especial para V1\n",
    "    fecha_inicio_v1 = pd.Timestamp(\"2023-09-10\")\n",
    "    fecha_fin_v1 = pd.Timestamp(\"2023-11-02\")\n",
    "    \n",
    "    for i, df in enumerate(lista_dataframes):\n",
    "        print(f\"Procesando DataFrame {i}...\")\n",
    "        \n",
    "        # Verificar que es un DataFrame válido\n",
    "        if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "            resultado.append(df)\n",
    "            continue\n",
    "        \n",
    "        # Crear una copia del dataframe\n",
    "        df_nuevo = df.copy()\n",
    "        \n",
    "        # Asegurarse de que el índice es de tipo datetime\n",
    "        if not isinstance(df_nuevo.index, pd.DatetimeIndex):\n",
    "            df_nuevo.index = pd.to_datetime(df_nuevo.index)\n",
    "        \n",
    "        # V1: Vector binario para el rango 2023-09-10 hasta 2023-11-02\n",
    "        df_nuevo['V1'] = 0  # Inicializar todo con 0\n",
    "        mask_v1 = (df_nuevo.index >= fecha_inicio_v1) & (df_nuevo.index <= fecha_fin_v1)\n",
    "        df_nuevo.loc[mask_v1, 'V1'] = 1\n",
    "        \n",
    "        # Columnas temporales estándar\n",
    "        df_nuevo['day'] = df_nuevo.index.day\n",
    "        df_nuevo['weekday'] = df_nuevo.index.dayofweek\n",
    "        df_nuevo['week'] = df_nuevo.index.isocalendar().week\n",
    "        df_nuevo['month'] = df_nuevo.index.month\n",
    "        df_nuevo['quarter'] = df_nuevo.index.quarter\n",
    "        \n",
    "        # Mostrar información sobre V1\n",
    "        count_v1_ones = df_nuevo['V1'].sum()\n",
    "        print(f\"  DataFrame {i}: V1 tiene {count_v1_ones} fechas marcadas (2023-09-10 a 2023-11-02)\")\n",
    "        \n",
    "        # Añadir a la lista de resultados\n",
    "        resultado.append(df_nuevo)\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "def mostrar_info_vectores(lista_dataframes_con_vectores, indices_muestra=[0]):\n",
    "    \"\"\"\n",
    "    Muestra información detallada sobre las columnas temporales creadas\n",
    "    \n",
    "    Args:\n",
    "        lista_dataframes_con_vectores: Lista de DataFrames con columnas temporales\n",
    "        indices_muestra: Lista de índices de DataFrames para mostrar como muestra\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INFORMACIÓN DE COLUMNAS TEMPORALES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx in indices_muestra:\n",
    "        if idx < len(lista_dataframes_con_vectores):\n",
    "            df = lista_dataframes_con_vectores[idx]\n",
    "            \n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                print(f\"\\nDataFrame {idx}:\")\n",
    "                print(f\"Rango de fechas: {df.index.min()} a {df.index.max()}\")\n",
    "                print(f\"Total de registros: {len(df)}\")\n",
    "                \n",
    "                # Información de V1\n",
    "                v1_count = df['V1'].sum()\n",
    "                fechas_v1 = df[df['V1'] == 1].index\n",
    "                print(f\"\\nV1 (período especial 2023-09-10 a 2023-11-02):\")\n",
    "                print(f\"  Fechas marcadas: {v1_count}\")\n",
    "                if len(fechas_v1) > 0:\n",
    "                    print(f\"  Primera fecha: {fechas_v1.min()}\")\n",
    "                    print(f\"  Última fecha: {fechas_v1.max()}\")\n",
    "                \n",
    "                # Rangos de otras columnas\n",
    "                print(f\"\\nRangos de columnas temporales:\")\n",
    "                print(f\"  day: {df['day'].min()} - {df['day'].max()}\")\n",
    "                print(f\"  weekday: {df['weekday'].min()} - {df['weekday'].max()} (0=lun, 6=dom)\")\n",
    "                print(f\"  week: {df['week'].min()} - {df['week'].max()}\")\n",
    "                print(f\"  month: {df['month'].min()} - {df['month'].max()}\")\n",
    "                print(f\"  quarter: {df['quarter'].min()} - {df['quarter'].max()}\")\n",
    "                \n",
    "                # Muestra de los primeros registros\n",
    "                print(f\"\\nMuestra de registros (primeros 3):\")\n",
    "                columnas_temporales = ['V1', 'day', 'weekday', 'week', 'month', 'quarter']\n",
    "                if all(col in df.columns for col in columnas_temporales):\n",
    "                    print(df[columnas_temporales].head(3))\n",
    "\n",
    "def verificar_periodo_v1(lista_dataframes_con_vectores):\n",
    "    \"\"\"\n",
    "    Verifica específicamente el vector V1 en el período de interés\n",
    "    \n",
    "    Args:\n",
    "        lista_dataframes_con_vectores: Lista de DataFrames con V1\n",
    "    \"\"\"\n",
    "    fecha_inicio = pd.Timestamp(\"2023-09-10\")\n",
    "    fecha_fin = pd.Timestamp(\"2023-11-02\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VERIFICACIÓN DEL VECTOR V1 (PERÍODO 2023-09-10 a 2023-11-02)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, df in enumerate(lista_dataframes_con_vectores[:3]):  # Solo los primeros 3 para no saturar\n",
    "        if isinstance(df, pd.DataFrame) and not df.empty and 'V1' in df.columns:\n",
    "            # Filtrar el período de interés\n",
    "            mask_periodo = (df.index >= fecha_inicio) & (df.index <= fecha_fin)\n",
    "            df_periodo = df.loc[mask_periodo]\n",
    "            \n",
    "            if not df_periodo.empty:\n",
    "                print(f\"\\nDataFrame {i} - Período 2023-09-10 a 2023-11-02:\")\n",
    "                print(f\"  Registros en el período: {len(df_periodo)}\")\n",
    "                print(f\"  V1 = 1: {df_periodo['V1'].sum()}\")\n",
    "                print(f\"  V1 = 0: {(df_periodo['V1'] == 0).sum()}\")\n",
    "                \n",
    "                # Mostrar algunas fechas específicas\n",
    "                fechas_ejemplo = [\n",
    "                    pd.Timestamp(\"2023-09-10\"),\n",
    "                    pd.Timestamp(\"2023-10-15\"),\n",
    "                    pd.Timestamp(\"2023-11-01\"),\n",
    "                    pd.Timestamp(\"2023-11-02\")\n",
    "                ]\n",
    "                \n",
    "                print(\"  Valores de V1 en fechas específicas:\")\n",
    "                for fecha in fechas_ejemplo:\n",
    "                    if fecha in df.index:\n",
    "                        valor_v1 = df.loc[fecha, 'V1']\n",
    "                        print(f\"    {fecha.date()}: V1 = {valor_v1}\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# dataframes_con_vectores = agregar_columnas_temporales_con_vectores(lista_dataframes)\n",
    "# mostrar_info_vectores(dataframes_con_vectores, indices_muestra=[0, 1, 2])\n",
    "# verificar_periodo_v1(dataframes_con_vectores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66063cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando DataFrame 0...\n",
      "  DataFrame 0: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 1...\n",
      "  DataFrame 1: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 2...\n",
      "  DataFrame 2: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 3...\n",
      "  DataFrame 3: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 4...\n",
      "  DataFrame 4: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 5...\n",
      "  DataFrame 5: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 6...\n",
      "  DataFrame 6: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 7...\n",
      "  DataFrame 7: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 8...\n",
      "  DataFrame 8: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 9...\n",
      "  DataFrame 9: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 10...\n",
      "  DataFrame 10: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 11...\n",
      "  DataFrame 11: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 12...\n",
      "  DataFrame 12: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 13...\n",
      "  DataFrame 13: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 14...\n",
      "  DataFrame 14: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 15...\n",
      "  DataFrame 15: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 16...\n",
      "  DataFrame 16: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 17...\n",
      "  DataFrame 17: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 18...\n",
      "  DataFrame 18: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 19...\n",
      "  DataFrame 19: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 20...\n",
      "  DataFrame 20: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 21...\n",
      "  DataFrame 21: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 22...\n",
      "  DataFrame 22: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 23...\n",
      "  DataFrame 23: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 24...\n",
      "  DataFrame 24: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 25...\n",
      "  DataFrame 25: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "Procesando DataFrame 26...\n",
      "  DataFrame 26: V1 tiene 54 fechas marcadas (2023-09-10 a 2023-11-02)\n",
      "\n",
      "================================================================================\n",
      "INFORMACIÓN DE COLUMNAS TEMPORALES\n",
      "================================================================================\n",
      "\n",
      "DataFrame 0:\n",
      "Rango de fechas: 2021-08-14 00:00:00 a 2025-05-14 00:00:00\n",
      "Total de registros: 1370\n",
      "\n",
      "V1 (período especial 2023-09-10 a 2023-11-02):\n",
      "  Fechas marcadas: 54\n",
      "  Primera fecha: 2023-09-10 00:00:00\n",
      "  Última fecha: 2023-11-02 00:00:00\n",
      "\n",
      "Rangos de columnas temporales:\n",
      "  day: 1 - 31\n",
      "  weekday: 0 - 6 (0=lun, 6=dom)\n",
      "  week: 1 - 52\n",
      "  month: 1 - 12\n",
      "  quarter: 1 - 4\n",
      "\n",
      "Muestra de registros (primeros 3):\n",
      "            V1  day  weekday  week  month  quarter\n",
      "fecha                                             \n",
      "2021-08-14   0   14        5    32      8        3\n",
      "2021-08-15   0   15        6    32      8        3\n",
      "2021-08-16   0   16        0    33      8        3\n",
      "\n",
      "DataFrame 1:\n",
      "Rango de fechas: 2021-08-20 00:00:00 a 2025-02-21 00:00:00\n",
      "Total de registros: 1282\n",
      "\n",
      "V1 (período especial 2023-09-10 a 2023-11-02):\n",
      "  Fechas marcadas: 54\n",
      "  Primera fecha: 2023-09-10 00:00:00\n",
      "  Última fecha: 2023-11-02 00:00:00\n",
      "\n",
      "Rangos de columnas temporales:\n",
      "  day: 1 - 31\n",
      "  weekday: 0 - 6 (0=lun, 6=dom)\n",
      "  week: 1 - 52\n",
      "  month: 1 - 12\n",
      "  quarter: 1 - 4\n",
      "\n",
      "Muestra de registros (primeros 3):\n",
      "            V1  day  weekday  week  month  quarter\n",
      "fecha                                             \n",
      "2021-08-20   0   20        4    33      8        3\n",
      "2021-08-21   0   21        5    33      8        3\n",
      "2021-08-22   0   22        6    33      8        3\n",
      "\n",
      "DataFrame 2:\n",
      "Rango de fechas: 2021-08-13 00:00:00 a 2025-05-17 00:00:00\n",
      "Total de registros: 1374\n",
      "\n",
      "V1 (período especial 2023-09-10 a 2023-11-02):\n",
      "  Fechas marcadas: 54\n",
      "  Primera fecha: 2023-09-10 00:00:00\n",
      "  Última fecha: 2023-11-02 00:00:00\n",
      "\n",
      "Rangos de columnas temporales:\n",
      "  day: 1 - 31\n",
      "  weekday: 0 - 6 (0=lun, 6=dom)\n",
      "  week: 1 - 52\n",
      "  month: 1 - 12\n",
      "  quarter: 1 - 4\n",
      "\n",
      "Muestra de registros (primeros 3):\n",
      "            V1  day  weekday  week  month  quarter\n",
      "fecha                                             \n",
      "2021-08-13   0   13        4    32      8        3\n",
      "2021-08-14   0   14        5    32      8        3\n",
      "2021-08-15   0   15        6    32      8        3\n",
      "\n",
      "================================================================================\n",
      "VERIFICACIÓN DEL VECTOR V1 (PERÍODO 2023-09-10 a 2023-11-02)\n",
      "================================================================================\n",
      "\n",
      "DataFrame 0 - Período 2023-09-10 a 2023-11-02:\n",
      "  Registros en el período: 54\n",
      "  V1 = 1: 54\n",
      "  V1 = 0: 0\n",
      "  Valores de V1 en fechas específicas:\n",
      "    2023-09-10: V1 = 1\n",
      "    2023-10-15: V1 = 1\n",
      "    2023-11-01: V1 = 1\n",
      "    2023-11-02: V1 = 1\n",
      "\n",
      "DataFrame 1 - Período 2023-09-10 a 2023-11-02:\n",
      "  Registros en el período: 54\n",
      "  V1 = 1: 54\n",
      "  V1 = 0: 0\n",
      "  Valores de V1 en fechas específicas:\n",
      "    2023-09-10: V1 = 1\n",
      "    2023-10-15: V1 = 1\n",
      "    2023-11-01: V1 = 1\n",
      "    2023-11-02: V1 = 1\n",
      "\n",
      "DataFrame 2 - Período 2023-09-10 a 2023-11-02:\n",
      "  Registros en el período: 54\n",
      "  V1 = 1: 54\n",
      "  V1 = 0: 0\n",
      "  Valores de V1 en fechas específicas:\n",
      "    2023-09-10: V1 = 1\n",
      "    2023-10-15: V1 = 1\n",
      "    2023-11-01: V1 = 1\n",
      "    2023-11-02: V1 = 1\n"
     ]
    }
   ],
   "source": [
    "timeseries = agregar_columnas_temporales_con_vectores(timeseries)\n",
    "mostrar_info_vectores(timeseries, indices_muestra=[0, 1, 2])\n",
    "verificar_periodo_v1(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4223fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aproximar_cantidad_a_enteros(timeseries_list):\n",
    "    \"\"\"\n",
    "    Aproxima la columna 'cantidad' al entero más cercano en cada DataFrame.\n",
    "    Si el valor es 0 o menor, se aproxima a 1.\n",
    "    \n",
    "    Args:\n",
    "        timeseries_list: Lista de DataFrames con columna 'cantidad'\n",
    "    \n",
    "    Returns:\n",
    "        Lista de DataFrames con la columna 'cantidad' aproximada a enteros\n",
    "    \"\"\"\n",
    "    dataframes_modificados = []\n",
    "    \n",
    "    print(f\"Procesando {len(timeseries_list)} DataFrames para aproximar 'cantidad' a enteros...\")\n",
    "    \n",
    "    for i, df in enumerate(tqdm(timeseries_list, desc=\"Aproximando cantidades\")):\n",
    "        # Verificar que es un DataFrame válido\n",
    "        if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "            print(f\"Advertencia: DataFrame {i} está vacío o no es válido. Se mantiene sin cambios.\")\n",
    "            dataframes_modificados.append(df)\n",
    "            continue\n",
    "        \n",
    "        # Crear copia del DataFrame\n",
    "        df_modificado = df.copy()\n",
    "        \n",
    "        # Verificar que existe la columna 'cantidad'\n",
    "        if 'cantidad' not in df_modificado.columns:\n",
    "            print(f\"Advertencia: DataFrame {i} no tiene columna 'cantidad'. Se mantiene sin cambios.\")\n",
    "            dataframes_modificados.append(df_modificado)\n",
    "            continue\n",
    "        \n",
    "        # Obtener estadísticas antes de la modificación\n",
    "        cantidad_original = df_modificado['cantidad'].copy()\n",
    "        valores_nan = cantidad_original.isna().sum()\n",
    "        valores_originales = cantidad_original.dropna()\n",
    "        \n",
    "        if len(valores_originales) == 0:\n",
    "            print(f\"Advertencia: DataFrame {i} no tiene valores válidos en 'cantidad'. Se mantiene sin cambios.\")\n",
    "            dataframes_modificados.append(df_modificado)\n",
    "            continue\n",
    "        \n",
    "        # Estadísticas originales\n",
    "        min_original = valores_originales.min()\n",
    "        max_original = valores_originales.max()\n",
    "        promedio_original = valores_originales.mean()\n",
    "        ceros_negativos_original = (valores_originales <= 0).sum()\n",
    "        \n",
    "        # Aplicar aproximación\n",
    "        # Paso 1: Redondear al entero más cercano\n",
    "        cantidad_redondeada = np.round(cantidad_original)\n",
    "        \n",
    "        # Paso 2: Convertir valores <= 0 a 1\n",
    "        cantidad_redondeada = np.where(cantidad_redondeada <= 0, 1, cantidad_redondeada)\n",
    "        \n",
    "        # Paso 3: Convertir a enteros (manteniendo NaN como NaN)\n",
    "        mask_no_nan = ~cantidad_original.isna()\n",
    "        df_modificado.loc[mask_no_nan, 'cantidad'] = cantidad_redondeada[mask_no_nan].astype(int)\n",
    "        \n",
    "        # Estadísticas después de la modificación\n",
    "        cantidad_nueva = df_modificado['cantidad'].dropna()\n",
    "        min_nuevo = cantidad_nueva.min()\n",
    "        max_nuevo = cantidad_nueva.max()\n",
    "        promedio_nuevo = cantidad_nueva.mean()\n",
    "        valores_cambiados_a_1 = (cantidad_redondeada[mask_no_nan] == 1).sum() - (valores_originales == 1).sum()\n",
    "        \n",
    "        # Mostrar estadísticas de cambio (solo para los primeros DataFrames para no saturar)\n",
    "        if i < 5:\n",
    "            print(f\"\\nDataFrame {i} - Estadísticas de aproximación:\")\n",
    "            print(f\"  Valores NaN: {valores_nan}\")\n",
    "            print(f\"  Rango original: [{min_original:.2f}, {max_original:.2f}] -> Nuevo: [{min_nuevo}, {max_nuevo}]\")\n",
    "            print(f\"  Promedio: {promedio_original:.2f} -> {promedio_nuevo:.2f}\")\n",
    "            print(f\"  Valores ≤ 0 originales: {ceros_negativos_original}\")\n",
    "            print(f\"  Valores cambiados a 1: {valores_cambiados_a_1}\")\n",
    "        \n",
    "        dataframes_modificados.append(df_modificado)\n",
    "    \n",
    "    print(f\"\\nProcesamiento completado. {len(dataframes_modificados)} DataFrames procesados.\")\n",
    "    return dataframes_modificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9033672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 27 DataFrames para aproximar 'cantidad' a enteros...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aproximando cantidades: 100%|██████████| 27/27 [00:00<00:00, 660.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame 0 - Estadísticas de aproximación:\n",
      "  Valores NaN: 840\n",
      "  Rango original: [1.00, 7.33] -> Nuevo: [1.0, 7.0]\n",
      "  Promedio: 1.70 -> 1.71\n",
      "  Valores ≤ 0 originales: 0\n",
      "  Valores cambiados a 1: 0\n",
      "\n",
      "DataFrame 1 - Estadísticas de aproximación:\n",
      "  Valores NaN: 951\n",
      "  Rango original: [0.67, 5.00] -> Nuevo: [1.0, 5.0]\n",
      "  Promedio: 1.47 -> 1.47\n",
      "  Valores ≤ 0 originales: 0\n",
      "  Valores cambiados a 1: 12\n",
      "\n",
      "DataFrame 2 - Estadísticas de aproximación:\n",
      "  Valores NaN: 822\n",
      "  Rango original: [0.93, 8.00] -> Nuevo: [1.0, 8.0]\n",
      "  Promedio: 1.88 -> 1.89\n",
      "  Valores ≤ 0 originales: 0\n",
      "  Valores cambiados a 1: 6\n",
      "\n",
      "DataFrame 3 - Estadísticas de aproximación:\n",
      "  Valores NaN: 828\n",
      "  Rango original: [1.00, 7.75] -> Nuevo: [1.0, 8.0]\n",
      "  Promedio: 1.76 -> 1.76\n",
      "  Valores ≤ 0 originales: 0\n",
      "  Valores cambiados a 1: 0\n",
      "\n",
      "DataFrame 4 - Estadísticas de aproximación:\n",
      "  Valores NaN: 972\n",
      "  Rango original: [1.00, 8.00] -> Nuevo: [1.0, 8.0]\n",
      "  Promedio: 1.82 -> 1.81\n",
      "  Valores ≤ 0 originales: 0\n",
      "  Valores cambiados a 1: 6\n",
      "\n",
      "Procesamiento completado. 27 DataFrames procesados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "timeseries = aproximar_cantidad_a_enteros(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3cda4681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeseries[0] → <Day>\n",
      "timeseries[1] → <Day>\n",
      "timeseries[2] → <Day>\n",
      "timeseries[3] → <Day>\n",
      "timeseries[4] → <Day>\n",
      "timeseries[5] → <Day>\n",
      "timeseries[6] → <Day>\n",
      "timeseries[7] → <Day>\n",
      "timeseries[8] → <Day>\n",
      "timeseries[9] → <Day>\n",
      "timeseries[10] → <Day>\n",
      "timeseries[11] → <Day>\n",
      "timeseries[12] → <Day>\n",
      "timeseries[13] → <Day>\n",
      "timeseries[14] → <Day>\n",
      "timeseries[15] → <Day>\n",
      "timeseries[16] → <Day>\n",
      "timeseries[17] → <Day>\n",
      "timeseries[18] → <Day>\n",
      "timeseries[19] → <Day>\n",
      "timeseries[20] → <Day>\n",
      "timeseries[21] → <Day>\n",
      "timeseries[22] → <Day>\n",
      "timeseries[23] → <Day>\n",
      "timeseries[24] → <Day>\n",
      "timeseries[25] → <Day>\n",
      "timeseries[26] → <Day>\n"
     ]
    }
   ],
   "source": [
    "freqs = check_frequencies(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51e2a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_vectores_categoricos(lista_dataframes):\n",
    "    \"\"\"\n",
    "    Extrae los valores categóricos del primer registro de cada dataframe y crea\n",
    "    un vector con estos valores.\n",
    "    \n",
    "    Args:\n",
    "        lista_dataframes: Lista de dataframes que contienen columnas categóricas\n",
    "    \n",
    "    Returns:\n",
    "        Lista de vectores categóricos, uno por cada dataframe\n",
    "    \"\"\"\n",
    "    # Definir las columnas categóricas\n",
    "    columnas_categoricas = [\n",
    "        'Tipo_Producto', \n",
    "        'segmento_producto', \n",
    "        'supergrupo_producto', \n",
    "        'grupo_producto', \n",
    "        'subgrupo_producto'\n",
    "    ]\n",
    "    \n",
    "    vectores_categoricos = []\n",
    "    \n",
    "    for i, df in enumerate(lista_dataframes):\n",
    "        # Verificar que el dataframe tenga registros\n",
    "        if len(df) == 0:\n",
    "            print(f\"Advertencia: Dataframe {i} está vacío, se usará un vector de ceros.\")\n",
    "            vectores_categoricos.append([0] * len(columnas_categoricas))\n",
    "            continue\n",
    "        \n",
    "        # Crear el vector para este dataframe\n",
    "        vector = []\n",
    "        \n",
    "        for col in columnas_categoricas:\n",
    "            # Verificar si la columna existe en el dataframe\n",
    "            if col in df.columns:\n",
    "                # Obtener el valor del primer registro\n",
    "                valor = df[col].iloc[0]\n",
    "                \n",
    "                # Convertir a entero si es posible, de lo contrario usar un código hash\n",
    "                try:\n",
    "                    valor_numerico = int(valor)\n",
    "                except (ValueError, TypeError):\n",
    "                    # Si no se puede convertir a entero, usar un código hash simple\n",
    "                    if valor is None:\n",
    "                        valor_numerico = 0\n",
    "                    else:\n",
    "                        # Hash simple basado en la representación string del valor\n",
    "                        valor_numerico = hash(str(valor)) % 10000  # Limitar a 4 dígitos\n",
    "            else:\n",
    "                # Si la columna no existe, usar 0\n",
    "                valor_numerico = 0\n",
    "            \n",
    "            vector.append(valor_numerico)\n",
    "        \n",
    "        vectores_categoricos.append(vector)\n",
    "        \n",
    "    return vectores_categoricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1bfc9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectores_cat = extraer_vectores_categoricos(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad0fccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_vectores_cantidad(lista_dataframes):\n",
    "    \"\"\"\n",
    "    Extrae los valores de la columna 'cantidad' de cada dataframe y crea \n",
    "    un vector con estos valores.\n",
    "    \n",
    "    Args:\n",
    "        lista_dataframes: Lista de dataframes que contienen la columna 'cantidad'\n",
    "    \n",
    "    Returns:\n",
    "        Lista de vectores, donde cada vector contiene los valores de 'cantidad' de un dataframe\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    vectores_cantidad = []\n",
    "    \n",
    "    for i, df in enumerate(lista_dataframes):\n",
    "        # Verificar que el dataframe tenga registros\n",
    "        if len(df) == 0:\n",
    "            print(f\"Advertencia: Dataframe {i} está vacío.\")\n",
    "            vectores_cantidad.append([])\n",
    "            continue\n",
    "        \n",
    "        # Verificar que exista la columna 'cantidad'\n",
    "        if 'cantidad' not in df.columns:\n",
    "            print(f\"Advertencia: Dataframe {i} no tiene columna 'cantidad'.\")\n",
    "            vectores_cantidad.append([])\n",
    "            continue\n",
    "        \n",
    "        # Extraer los valores de 'cantidad' como una lista\n",
    "        valores = df['cantidad'].tolist()\n",
    "        \n",
    "        # Opcionalmente, puedes manejar valores NaN\n",
    "        # valores = [0 if pd.isna(x) else x for x in valores]  # Convierte NaN a 0\n",
    "        # O simplemente:\n",
    "        valores = df['cantidad'].fillna(0).tolist()  # Rellena NaN con 0\n",
    "        \n",
    "        vectores_cantidad.append(valores)\n",
    "    \n",
    "    return vectores_cantidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6bf6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectores_target = extraer_vectores_cantidad(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3d5319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_vectores_temporales(lista_dataframes):\n",
    "    \"\"\"\n",
    "    Extrae los valores de las columnas temporales de cada dataframe\n",
    "    y crea un conjunto de vectores con estos valores.\n",
    "    \n",
    "    Args:\n",
    "        lista_dataframes: Lista de dataframes que contienen columnas temporales\n",
    "    \n",
    "    Returns:\n",
    "        Lista de conjuntos de vectores, donde cada conjunto contiene los vectores\n",
    "        de V1, day, weekday, week, month, quarter para un dataframe\n",
    "    \"\"\"\n",
    "    conjuntos_vectores = []\n",
    "    \n",
    "    # Columnas temporales esperadas\n",
    "    columnas_esperadas = ['V1', 'day', 'weekday', 'week', 'month', 'quarter']\n",
    "    \n",
    "    for i, df in enumerate(lista_dataframes):\n",
    "        # Verificar que el dataframe tenga registros\n",
    "        if len(df) == 0:\n",
    "            print(f\"Advertencia: Dataframe {i} está vacío.\")\n",
    "            # Crear vectores vacíos para todas las columnas\n",
    "            vectores_vacios = tuple([] for _ in columnas_esperadas)\n",
    "            conjuntos_vectores.append(vectores_vacios)\n",
    "            continue\n",
    "        \n",
    "        # Verificar que existan las columnas necesarias\n",
    "        columnas_faltantes = []\n",
    "        for col in columnas_esperadas:\n",
    "            if col not in df.columns:\n",
    "                columnas_faltantes.append(col)\n",
    "        \n",
    "        if columnas_faltantes:\n",
    "            print(f\"Advertencia: Dataframe {i} no tiene las columnas: {columnas_faltantes}\")\n",
    "            \n",
    "            # Si faltan las columnas, podemos crearlas a partir del índice si es de tipo fecha\n",
    "            df_temp = df.copy()\n",
    "            \n",
    "            # Verificar si el índice es de tipo fecha\n",
    "            if isinstance(df_temp.index, pd.DatetimeIndex):\n",
    "                # Crear columnas temporales faltantes\n",
    "                if 'day' not in df_temp.columns:\n",
    "                    df_temp['day'] = df_temp.index.day\n",
    "                if 'weekday' not in df_temp.columns:\n",
    "                    df_temp['weekday'] = df_temp.index.dayofweek\n",
    "                if 'week' not in df_temp.columns:\n",
    "                    df_temp['week'] = df_temp.index.isocalendar().week\n",
    "                if 'month' not in df_temp.columns:\n",
    "                    df_temp['month'] = df_temp.index.month\n",
    "                if 'quarter' not in df_temp.columns:\n",
    "                    df_temp['quarter'] = df_temp.index.quarter\n",
    "                if 'V1' not in df_temp.columns:\n",
    "                    # Crear V1 basado en el período 2023-09-10 a 2023-11-02\n",
    "                    fecha_inicio_v1 = pd.Timestamp(\"2023-09-10\")\n",
    "                    fecha_fin_v1 = pd.Timestamp(\"2023-11-02\")\n",
    "                    df_temp['V1'] = 0\n",
    "                    mask_v1 = (df_temp.index >= fecha_inicio_v1) & (df_temp.index <= fecha_fin_v1)\n",
    "                    df_temp.loc[mask_v1, 'V1'] = 1\n",
    "            else:\n",
    "                # Si el índice no es de tipo fecha, tratar de convertirlo\n",
    "                try:\n",
    "                    df_temp.index = pd.to_datetime(df_temp.index)\n",
    "                    # Crear columnas temporales después de convertir el índice\n",
    "                    if 'day' not in df_temp.columns:\n",
    "                        df_temp['day'] = df_temp.index.day\n",
    "                    if 'weekday' not in df_temp.columns:\n",
    "                        df_temp['weekday'] = df_temp.index.dayofweek\n",
    "                    if 'week' not in df_temp.columns:\n",
    "                        df_temp['week'] = df_temp.index.isocalendar().week\n",
    "                    if 'month' not in df_temp.columns:\n",
    "                        df_temp['month'] = df_temp.index.month\n",
    "                    if 'quarter' not in df_temp.columns:\n",
    "                        df_temp['quarter'] = df_temp.index.quarter\n",
    "                    if 'V1' not in df_temp.columns:\n",
    "                        fecha_inicio_v1 = pd.Timestamp(\"2023-09-10\")\n",
    "                        fecha_fin_v1 = pd.Timestamp(\"2023-11-02\")\n",
    "                        df_temp['V1'] = 0\n",
    "                        mask_v1 = (df_temp.index >= fecha_inicio_v1) & (df_temp.index <= fecha_fin_v1)\n",
    "                        df_temp.loc[mask_v1, 'V1'] = 1\n",
    "                except:\n",
    "                    # Si no se puede convertir, usar vectores vacíos\n",
    "                    print(f\"Error: No se pudo convertir el índice del dataframe {i} a fecha\")\n",
    "                    vectores_vacios = tuple([] for _ in columnas_esperadas)\n",
    "                    conjuntos_vectores.append(vectores_vacios)\n",
    "                    continue\n",
    "            \n",
    "            # Usar el dataframe temporal con las columnas agregadas\n",
    "            df = df_temp\n",
    "        \n",
    "        # Extraer los vectores en el orden especificado\n",
    "        V1_vector = df['V1'].tolist()\n",
    "        day_vector = df['day'].tolist()\n",
    "        weekday_vector = df['weekday'].tolist()\n",
    "        week_vector = df['week'].tolist()\n",
    "        month_vector = df['month'].tolist()\n",
    "        quarter_vector = df['quarter'].tolist()\n",
    "        \n",
    "        # Guardar el conjunto de vectores como tupla\n",
    "        conjunto_vectores = (V1_vector, day_vector, weekday_vector, week_vector, month_vector, quarter_vector)\n",
    "        conjuntos_vectores.append(conjunto_vectores)\n",
    "        \n",
    "        # Información de diagnóstico\n",
    "        print(f\"Dataframe {i}: Extraídos {len(V1_vector)} registros\")\n",
    "        print(f\"  V1 valores únicos: {len(set(V1_vector))} - rango: {min(V1_vector)} a {max(V1_vector)}\")\n",
    "        print(f\"  day rango: {min(day_vector)} a {max(day_vector)}\")\n",
    "        print(f\"  weekday rango: {min(weekday_vector)} a {max(weekday_vector)}\")\n",
    "        print(f\"  week rango: {min(week_vector)} a {max(week_vector)}\")\n",
    "        print(f\"  month rango: {min(month_vector)} a {max(month_vector)}\")\n",
    "        print(f\"  quarter rango: {min(quarter_vector)} a {max(quarter_vector)}\")\n",
    "    \n",
    "    return conjuntos_vectores\n",
    "\n",
    "def mostrar_resumen_vectores(conjuntos_vectores, indices_muestra=[0, 1, 2]):\n",
    "    \"\"\"\n",
    "    Muestra un resumen de los vectores extraídos\n",
    "    \n",
    "    Args:\n",
    "        conjuntos_vectores: Lista de tuplas con vectores temporales\n",
    "        indices_muestra: Índices de dataframes para mostrar como muestra\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESUMEN DE VECTORES EXTRAÍDOS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    nombres_vectores = ['V1', 'day', 'weekday', 'week', 'month', 'quarter']\n",
    "    \n",
    "    for idx in indices_muestra:\n",
    "        if idx < len(conjuntos_vectores):\n",
    "            vectores = conjuntos_vectores[idx]\n",
    "            \n",
    "            if len(vectores) == 6:  # Verificar que tenemos todos los vectores\n",
    "                print(f\"\\nDataframe {idx}:\")\n",
    "                print(f\"Número de elementos: {len(vectores[0])}\")\n",
    "                \n",
    "                for i, (nombre, vector) in enumerate(zip(nombres_vectores, vectores)):\n",
    "                    if vector:  # Si el vector no está vacío\n",
    "                        valores_unicos = len(set(vector))\n",
    "                        rango_min, rango_max = min(vector), max(vector)\n",
    "                        print(f\"  {nombre}: {valores_unicos} valores únicos, rango [{rango_min}, {rango_max}]\")\n",
    "                        \n",
    "                        # Para V1, mostrar cuántos 1s y 0s hay\n",
    "                        if nombre == 'V1':\n",
    "                            count_ones = vector.count(1)\n",
    "                            count_zeros = vector.count(0)\n",
    "                            print(f\"    V1 detalles: {count_ones} unos, {count_zeros} ceros\")\n",
    "                    else:\n",
    "                        print(f\"  {nombre}: vector vacío\")\n",
    "            else:\n",
    "                print(f\"\\nDataframe {idx}: Estructura de vectores incompleta\")\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# vectores_temporales = extraer_vectores_temporales(lista_dataframes)\n",
    "# mostrar_resumen_vectores(vectores_temporales, indices_muestra=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38f5434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe 0: Extraídos 1370 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 1: Extraídos 1282 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 2: Extraídos 1374 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 3: Extraídos 1370 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 4: Extraídos 1372 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 5: Extraídos 1360 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 6: Extraídos 1373 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 7: Extraídos 1339 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 8: Extraídos 1198 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 9: Extraídos 1366 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 10: Extraídos 1373 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 11: Extraídos 1356 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 12: Extraídos 1366 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 13: Extraídos 1374 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 14: Extraídos 1374 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 15: Extraídos 1368 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 16: Extraídos 1349 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 17: Extraídos 1316 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 18: Extraídos 1092 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 19: Extraídos 829 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 20: Extraídos 904 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 21: Extraídos 1049 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 22: Extraídos 1175 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 23: Extraídos 1359 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 24: Extraídos 1358 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 25: Extraídos 1270 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "Dataframe 26: Extraídos 882 registros\n",
      "  V1 valores únicos: 2 - rango: 0 a 1\n",
      "  day rango: 1 a 31\n",
      "  weekday rango: 0 a 6\n",
      "  week rango: 1 a 52\n",
      "  month rango: 1 a 12\n",
      "  quarter rango: 1 a 4\n",
      "\n",
      "======================================================================\n",
      "RESUMEN DE VECTORES EXTRAÍDOS\n",
      "======================================================================\n",
      "\n",
      "Dataframe 0:\n",
      "Número de elementos: 1370\n",
      "  V1: 2 valores únicos, rango [0, 1]\n",
      "    V1 detalles: 54 unos, 1316 ceros\n",
      "  day: 31 valores únicos, rango [1, 31]\n",
      "  weekday: 7 valores únicos, rango [0, 6]\n",
      "  week: 52 valores únicos, rango [1, 52]\n",
      "  month: 12 valores únicos, rango [1, 12]\n",
      "  quarter: 4 valores únicos, rango [1, 4]\n",
      "\n",
      "Dataframe 1:\n",
      "Número de elementos: 1282\n",
      "  V1: 2 valores únicos, rango [0, 1]\n",
      "    V1 detalles: 54 unos, 1228 ceros\n",
      "  day: 31 valores únicos, rango [1, 31]\n",
      "  weekday: 7 valores únicos, rango [0, 6]\n",
      "  week: 52 valores únicos, rango [1, 52]\n",
      "  month: 12 valores únicos, rango [1, 12]\n",
      "  quarter: 4 valores únicos, rango [1, 4]\n",
      "\n",
      "Dataframe 2:\n",
      "Número de elementos: 1374\n",
      "  V1: 2 valores únicos, rango [0, 1]\n",
      "    V1 detalles: 54 unos, 1320 ceros\n",
      "  day: 31 valores únicos, rango [1, 31]\n",
      "  weekday: 7 valores únicos, rango [0, 6]\n",
      "  week: 52 valores únicos, rango [1, 52]\n",
      "  month: 12 valores únicos, rango [1, 12]\n",
      "  quarter: 4 valores únicos, rango [1, 4]\n"
     ]
    }
   ],
   "source": [
    "vectores_dynamic = extraer_vectores_temporales(timeseries)\n",
    "mostrar_resumen_vectores(vectores_dynamic, indices_muestra=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "487cd757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_primeros_indices(lista_dataframes):\n",
    "    \"\"\"\n",
    "    Extrae el primer índice de cada dataframe y lo devuelve en formato \"YYYY-MM-DD 00:00:00\".\n",
    "    \n",
    "    Args:\n",
    "        lista_dataframes: Lista de dataframes con índices de fecha\n",
    "    \n",
    "    Returns:\n",
    "        Lista de strings con los primeros índices en formato \"YYYY-MM-DD 00:00:00\"\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    \n",
    "    primeros_indices = []\n",
    "    \n",
    "    for i, df in enumerate(lista_dataframes):\n",
    "        # Verificar que el dataframe tenga registros\n",
    "        if len(df) == 0:\n",
    "            print(f\"Advertencia: Dataframe {i} está vacío. Se usará fecha por defecto.\")\n",
    "            primeros_indices.append(\"2000-01-01 00:00:00\")\n",
    "            continue\n",
    "        \n",
    "        # Obtener el primer índice\n",
    "        primer_indice = df.index[0]\n",
    "        \n",
    "        # Convertir a datetime si no lo es\n",
    "        if not isinstance(primer_indice, pd.Timestamp) and not isinstance(primer_indice, datetime):\n",
    "            try:\n",
    "                primer_indice = pd.to_datetime(primer_indice)\n",
    "            except:\n",
    "                print(f\"Advertencia: No se pudo convertir el índice del Dataframe {i} a fecha. Se usará fecha por defecto.\")\n",
    "                primeros_indices.append(\"2000-01-01 00:00:00\")\n",
    "                continue\n",
    "        \n",
    "        # Formatear a \"YYYY-MM-DD 00:00:00\"\n",
    "        indice_formateado = primer_indice.strftime(\"%Y-%m-%d 00:00:00\")\n",
    "        \n",
    "        primeros_indices.append(indice_formateado)\n",
    "    \n",
    "    return primeros_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2e73dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = extraer_primeros_indices(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95f67d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_diccionarios_test(start, vectores_target, vectores_cat, vectores_dynamic):\n",
    "    \"\"\"\n",
    "    Crea una lista de diccionarios con la estructura requerida para entrenamiento,\n",
    "    donde start son las fechas de inicio de cada serie (un valor por dataframe).\n",
    "    \n",
    "    Args:\n",
    "        start: Lista de strings con las fechas de inicio (un valor por dataframe)\n",
    "        vectores_target: Lista de vectores con los valores de 'cantidad' para cada serie\n",
    "        vectores_cat: Lista de vectores con las características categóricas\n",
    "        vectores_dynamic: Lista de tuplas (V1, day, weekday, week, month, quarter) con características dinámicas\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con la estructura {start, target, cat, dynamic_feat}\n",
    "    \"\"\"\n",
    "    \n",
    "    def convert_nans_to_string(values_list):\n",
    "        \"\"\"Convierte valores NaN en la lista a 'NaN' como string\"\"\"\n",
    "        return ['NaN' if (isinstance(x, float) and np.isnan(x)) else float(x) for x in values_list]\n",
    "    \n",
    "    diccionarios = []\n",
    "    \n",
    "    # Verificar que tengamos el mismo número de series en todas las listas\n",
    "    num_series = len(start)\n",
    "    if not (num_series == len(vectores_target) == len(vectores_cat) == len(vectores_dynamic)):\n",
    "        print(f\"Error: Las listas tienen diferentes longitudes - start: {len(start)}, target: {len(vectores_target)}, \" \n",
    "              f\"cat: {len(vectores_cat)}, dynamic: {len(vectores_dynamic)}\")\n",
    "        return []\n",
    "    \n",
    "    for i in range(num_series):\n",
    "        # Verificar que haya datos para esta serie\n",
    "        if not vectores_target[i]:\n",
    "            print(f\"Advertencia: Serie {i} no tiene valores target. Se omitirá.\")\n",
    "            continue\n",
    "        \n",
    "        # Obtener los datos para esta serie\n",
    "        fecha_inicio = start[i]\n",
    "        target_data = vectores_target[i]\n",
    "        cat_data = vectores_cat[i]\n",
    "        \n",
    "        # Extraer todos los vectores dinámicos de la tupla\n",
    "        if len(vectores_dynamic[i]) == 6:\n",
    "            V1_vector, day_vector, weekday_vector, week_vector, month_vector, quarter_vector = vectores_dynamic[i]\n",
    "        else:\n",
    "            print(f\"Error: Serie {i} no tiene la estructura correcta de vectores dinámicos (esperados 6, encontrados {len(vectores_dynamic[i])})\")\n",
    "            continue\n",
    "        \n",
    "        # Verificar longitudes de vectores target y dynamic\n",
    "        vectores_para_verificar = [V1_vector, day_vector, weekday_vector, week_vector, month_vector, quarter_vector]\n",
    "        longitudes = [len(v) for v in vectores_para_verificar]\n",
    "        \n",
    "        if not all(len(target_data) == longitud for longitud in longitudes):\n",
    "            print(f\"Advertencia: Serie {i} tiene longitudes inconsistentes:\")\n",
    "            print(f\"  target: {len(target_data)}\")\n",
    "            print(f\"  V1: {len(V1_vector)}, day: {len(day_vector)}, weekday: {len(weekday_vector)}\")\n",
    "            print(f\"  week: {len(week_vector)}, month: {len(month_vector)}, quarter: {len(quarter_vector)}\")\n",
    "        \n",
    "        # Crear el diccionario\n",
    "        diccionario = {\n",
    "            \"start\": fecha_inicio,\n",
    "            \"target\": convert_nans_to_string(target_data),\n",
    "            \"cat\": cat_data,\n",
    "            \"dynamic_feat\": [V1_vector, day_vector, weekday_vector, week_vector, month_vector, quarter_vector]\n",
    "        }\n",
    "        \n",
    "        diccionarios.append(diccionario)\n",
    "        \n",
    "        # Información de diagnóstico\n",
    "        print(f\"Serie {i} procesada correctamente:\")\n",
    "        print(f\"  Fecha inicio: {fecha_inicio}\")\n",
    "        print(f\"  Longitud target: {len(target_data)}\")\n",
    "        print(f\"  Características categóricas: {len(cat_data) if isinstance(cat_data, list) else 'valor único'}\")\n",
    "        print(f\"  Vectores dinámicos: 6 vectores de longitud {len(V1_vector)}\")\n",
    "        print(f\"  V1 - unos: {V1_vector.count(1)}, ceros: {V1_vector.count(0)}\")\n",
    "    \n",
    "    print(f\"\\nTotal de diccionarios creados: {len(diccionarios)}\")\n",
    "    return diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8a47525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serie 0 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-14 00:00:00\n",
      "  Longitud target: 1370\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1370\n",
      "  V1 - unos: 54, ceros: 1316\n",
      "Serie 1 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-20 00:00:00\n",
      "  Longitud target: 1282\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1282\n",
      "  V1 - unos: 54, ceros: 1228\n",
      "Serie 2 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-13 00:00:00\n",
      "  Longitud target: 1374\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1374\n",
      "  V1 - unos: 54, ceros: 1320\n",
      "Serie 3 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-13 00:00:00\n",
      "  Longitud target: 1370\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1370\n",
      "  V1 - unos: 54, ceros: 1316\n",
      "Serie 4 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-14 00:00:00\n",
      "  Longitud target: 1372\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1372\n",
      "  V1 - unos: 54, ceros: 1318\n",
      "Serie 5 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-21 00:00:00\n",
      "  Longitud target: 1360\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1360\n",
      "  V1 - unos: 54, ceros: 1306\n",
      "Serie 6 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-14 00:00:00\n",
      "  Longitud target: 1373\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1373\n",
      "  V1 - unos: 54, ceros: 1319\n",
      "Serie 7 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-15 00:00:00\n",
      "  Longitud target: 1339\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1339\n",
      "  V1 - unos: 54, ceros: 1285\n",
      "Serie 8 procesada correctamente:\n",
      "  Fecha inicio: 2022-02-05 00:00:00\n",
      "  Longitud target: 1198\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1198\n",
      "  V1 - unos: 54, ceros: 1144\n",
      "Serie 9 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-21 00:00:00\n",
      "  Longitud target: 1366\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1366\n",
      "  V1 - unos: 54, ceros: 1312\n",
      "Serie 10 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-13 00:00:00\n",
      "  Longitud target: 1373\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1373\n",
      "  V1 - unos: 54, ceros: 1319\n",
      "Serie 11 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-31 00:00:00\n",
      "  Longitud target: 1356\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1356\n",
      "  V1 - unos: 54, ceros: 1302\n",
      "Serie 12 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-21 00:00:00\n",
      "  Longitud target: 1366\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1366\n",
      "  V1 - unos: 54, ceros: 1312\n",
      "Serie 13 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-13 00:00:00\n",
      "  Longitud target: 1374\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1374\n",
      "  V1 - unos: 54, ceros: 1320\n",
      "Serie 14 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-13 00:00:00\n",
      "  Longitud target: 1374\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1374\n",
      "  V1 - unos: 54, ceros: 1320\n",
      "Serie 15 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-15 00:00:00\n",
      "  Longitud target: 1368\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1368\n",
      "  V1 - unos: 54, ceros: 1314\n",
      "Serie 16 procesada correctamente:\n",
      "  Fecha inicio: 2021-09-01 00:00:00\n",
      "  Longitud target: 1349\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1349\n",
      "  V1 - unos: 54, ceros: 1295\n",
      "Serie 17 procesada correctamente:\n",
      "  Fecha inicio: 2021-09-23 00:00:00\n",
      "  Longitud target: 1316\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1316\n",
      "  V1 - unos: 54, ceros: 1262\n",
      "Serie 18 procesada correctamente:\n",
      "  Fecha inicio: 2021-11-08 00:00:00\n",
      "  Longitud target: 1092\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1092\n",
      "  V1 - unos: 54, ceros: 1038\n",
      "Serie 19 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-13 00:00:00\n",
      "  Longitud target: 829\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 829\n",
      "  V1 - unos: 54, ceros: 775\n",
      "Serie 20 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-13 00:00:00\n",
      "  Longitud target: 904\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 904\n",
      "  V1 - unos: 54, ceros: 850\n",
      "Serie 21 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-13 00:00:00\n",
      "  Longitud target: 1049\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1049\n",
      "  V1 - unos: 54, ceros: 995\n",
      "Serie 22 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-13 00:00:00\n",
      "  Longitud target: 1175\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1175\n",
      "  V1 - unos: 54, ceros: 1121\n",
      "Serie 23 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-14 00:00:00\n",
      "  Longitud target: 1359\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1359\n",
      "  V1 - unos: 54, ceros: 1305\n",
      "Serie 24 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-15 00:00:00\n",
      "  Longitud target: 1358\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1358\n",
      "  V1 - unos: 54, ceros: 1304\n",
      "Serie 25 procesada correctamente:\n",
      "  Fecha inicio: 2021-08-14 00:00:00\n",
      "  Longitud target: 1270\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 1270\n",
      "  V1 - unos: 54, ceros: 1216\n",
      "Serie 26 procesada correctamente:\n",
      "  Fecha inicio: 2021-09-04 00:00:00\n",
      "  Longitud target: 882\n",
      "  Características categóricas: 5\n",
      "  Vectores dinámicos: 6 vectores de longitud 882\n",
      "  V1 - unos: 54, ceros: 828\n",
      "\n",
      "Total de diccionarios creados: 27\n"
     ]
    }
   ],
   "source": [
    "test = crear_diccionarios_test(start, vectores_target, vectores_cat, vectores_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60578029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_diccionarios_entrenamiento(start, vectores_target, vectores_cat, vectores_dynamic, puntos_a_excluir=30):\n",
    "    \"\"\"\n",
    "    Crea una lista de diccionarios excluyendo los últimos 'puntos_a_excluir' valores de \n",
    "    target y dynamic_feat para cada serie.\n",
    "    \n",
    "    Args:\n",
    "        start: Lista de strings con las fechas de inicio (un valor por dataframe)\n",
    "        vectores_target: Lista de vectores con los valores de 'cantidad' para cada serie\n",
    "        vectores_cat: Lista de vectores con las características categóricas\n",
    "        vectores_dynamic: Lista de tuplas (V1, day, weekday, week, month, quarter) con características dinámicas\n",
    "        puntos_a_excluir: Número de puntos a excluir del final de las series (default=6)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con la estructura {start, target, cat, dynamic_feat}\n",
    "    \"\"\"\n",
    "    \n",
    "    def convert_nans_to_string(values_list):\n",
    "        \"\"\"Convierte valores NaN en la lista a 'NaN' como string\"\"\"\n",
    "        return ['NaN' if (isinstance(x, float) and np.isnan(x)) else float(x) for x in values_list]\n",
    "    \n",
    "    diccionarios = []\n",
    "    \n",
    "    # Verificar que tengamos el mismo número de series en todas las listas\n",
    "    num_series = len(start)\n",
    "    if not (num_series == len(vectores_target) == len(vectores_cat) == len(vectores_dynamic)):\n",
    "        print(f\"Error: Las listas tienen diferentes longitudes - start: {len(start)}, target: {len(vectores_target)}, \" \n",
    "              f\"cat: {len(vectores_cat)}, dynamic: {len(vectores_dynamic)}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Procesando {num_series} series para entrenamiento (excluyendo últimos {puntos_a_excluir} puntos)\")\n",
    "    \n",
    "    for i in range(num_series):\n",
    "        # Verificar que haya datos para esta serie\n",
    "        if not vectores_target[i]:\n",
    "            print(f\"Advertencia: Serie {i} no tiene valores target. Se omitirá.\")\n",
    "            continue\n",
    "        \n",
    "        # Obtener los datos para esta serie\n",
    "        fecha_inicio = start[i]\n",
    "        target_data = vectores_target[i]\n",
    "        cat_data = vectores_cat[i]\n",
    "        \n",
    "        # Extraer todos los vectores dinámicos de la tupla\n",
    "        if len(vectores_dynamic[i]) == 6:\n",
    "            V1_vector, day_vector, weekday_vector, week_vector, month_vector, quarter_vector = vectores_dynamic[i]\n",
    "        else:\n",
    "            print(f\"Error: Serie {i} no tiene la estructura correcta de vectores dinámicos (esperados 6, encontrados {len(vectores_dynamic[i])})\")\n",
    "            continue\n",
    "        \n",
    "        # Verificar que hay suficientes puntos para excluir\n",
    "        if len(target_data) <= puntos_a_excluir:\n",
    "            print(f\"Advertencia: Serie {i} tiene menos puntos ({len(target_data)}) que los requeridos a excluir ({puntos_a_excluir}). Se omitirá.\")\n",
    "            continue\n",
    "        \n",
    "        # Excluir los últimos 'puntos_a_excluir' valores de todos los vectores\n",
    "        target_data_recortado = target_data[:-puntos_a_excluir]\n",
    "        V1_vector_recortado = V1_vector[:-puntos_a_excluir]\n",
    "        day_vector_recortado = day_vector[:-puntos_a_excluir]\n",
    "        weekday_vector_recortado = weekday_vector[:-puntos_a_excluir]\n",
    "        week_vector_recortado = week_vector[:-puntos_a_excluir]\n",
    "        month_vector_recortado = month_vector[:-puntos_a_excluir]\n",
    "        quarter_vector_recortado = quarter_vector[:-puntos_a_excluir]\n",
    "        \n",
    "        # Verificar longitudes después del recorte\n",
    "        vectores_recortados = [\n",
    "            V1_vector_recortado, day_vector_recortado, weekday_vector_recortado,\n",
    "            week_vector_recortado, month_vector_recortado, quarter_vector_recortado\n",
    "        ]\n",
    "        nombres_vectores = ['V1', 'day', 'weekday', 'week', 'month', 'quarter']\n",
    "        longitudes = [len(v) for v in vectores_recortados]\n",
    "        \n",
    "        # Verificar consistencia de longitudes\n",
    "        if not all(len(target_data_recortado) == longitud for longitud in longitudes):\n",
    "            print(f\"Advertencia: Serie {i} tiene longitudes inconsistentes después del recorte:\")\n",
    "            print(f\"  target: {len(target_data_recortado)}\")\n",
    "            for nombre, longitud in zip(nombres_vectores, longitudes):\n",
    "                print(f\"  {nombre}: {longitud}\")\n",
    "            \n",
    "            # Ajustar a la longitud mínima\n",
    "            min_len = min(len(target_data_recortado), *longitudes)\n",
    "            print(f\"  Ajustando todos los vectores a longitud: {min_len}\")\n",
    "            \n",
    "            target_data_recortado = target_data_recortado[:min_len]\n",
    "            vectores_recortados = [v[:min_len] for v in vectores_recortados]\n",
    "            V1_vector_recortado, day_vector_recortado, weekday_vector_recortado, week_vector_recortado, month_vector_recortado, quarter_vector_recortado = vectores_recortados\n",
    "        \n",
    "        # Crear el diccionario\n",
    "        diccionario = {\n",
    "            \"start\": fecha_inicio,\n",
    "            \"target\": convert_nans_to_string(target_data_recortado),\n",
    "            \"cat\": cat_data,\n",
    "            \"dynamic_feat\": [V1_vector_recortado, day_vector_recortado, weekday_vector_recortado, \n",
    "                           week_vector_recortado, month_vector_recortado, quarter_vector_recortado]\n",
    "        }\n",
    "        \n",
    "        diccionarios.append(diccionario)\n",
    "        \n",
    "        # Información de diagnóstico\n",
    "        print(f\"Serie {i} procesada para entrenamiento:\")\n",
    "        print(f\"  Longitud original: {len(target_data)} -> Longitud final: {len(target_data_recortado)}\")\n",
    "        print(f\"  Puntos excluidos: {puntos_a_excluir}\")\n",
    "        print(f\"  V1 en datos entrenamiento - unos: {V1_vector_recortado.count(1)}, ceros: {V1_vector_recortado.count(0)}\")\n",
    "    \n",
    "    print(f\"\\nTotal de diccionarios de entrenamiento creados: {len(diccionarios)}\")\n",
    "    return diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57505398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 27 series para entrenamiento (excluyendo últimos 30 puntos)\n",
      "Serie 0 procesada para entrenamiento:\n",
      "  Longitud original: 1370 -> Longitud final: 1340\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1286\n",
      "Serie 1 procesada para entrenamiento:\n",
      "  Longitud original: 1282 -> Longitud final: 1252\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1198\n",
      "Serie 2 procesada para entrenamiento:\n",
      "  Longitud original: 1374 -> Longitud final: 1344\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1290\n",
      "Serie 3 procesada para entrenamiento:\n",
      "  Longitud original: 1370 -> Longitud final: 1340\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1286\n",
      "Serie 4 procesada para entrenamiento:\n",
      "  Longitud original: 1372 -> Longitud final: 1342\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1288\n",
      "Serie 5 procesada para entrenamiento:\n",
      "  Longitud original: 1360 -> Longitud final: 1330\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1276\n",
      "Serie 6 procesada para entrenamiento:\n",
      "  Longitud original: 1373 -> Longitud final: 1343\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1289\n",
      "Serie 7 procesada para entrenamiento:\n",
      "  Longitud original: 1339 -> Longitud final: 1309\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1255\n",
      "Serie 8 procesada para entrenamiento:\n",
      "  Longitud original: 1198 -> Longitud final: 1168\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1114\n",
      "Serie 9 procesada para entrenamiento:\n",
      "  Longitud original: 1366 -> Longitud final: 1336\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1282\n",
      "Serie 10 procesada para entrenamiento:\n",
      "  Longitud original: 1373 -> Longitud final: 1343\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1289\n",
      "Serie 11 procesada para entrenamiento:\n",
      "  Longitud original: 1356 -> Longitud final: 1326\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1272\n",
      "Serie 12 procesada para entrenamiento:\n",
      "  Longitud original: 1366 -> Longitud final: 1336\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1282\n",
      "Serie 13 procesada para entrenamiento:\n",
      "  Longitud original: 1374 -> Longitud final: 1344\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1290\n",
      "Serie 14 procesada para entrenamiento:\n",
      "  Longitud original: 1374 -> Longitud final: 1344\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1290\n",
      "Serie 15 procesada para entrenamiento:\n",
      "  Longitud original: 1368 -> Longitud final: 1338\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1284\n",
      "Serie 16 procesada para entrenamiento:\n",
      "  Longitud original: 1349 -> Longitud final: 1319\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1265\n",
      "Serie 17 procesada para entrenamiento:\n",
      "  Longitud original: 1316 -> Longitud final: 1286\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1232\n",
      "Serie 18 procesada para entrenamiento:\n",
      "  Longitud original: 1092 -> Longitud final: 1062\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1008\n",
      "Serie 19 procesada para entrenamiento:\n",
      "  Longitud original: 829 -> Longitud final: 799\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 41, ceros: 758\n",
      "Serie 20 procesada para entrenamiento:\n",
      "  Longitud original: 904 -> Longitud final: 874\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 820\n",
      "Serie 21 procesada para entrenamiento:\n",
      "  Longitud original: 1049 -> Longitud final: 1019\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 965\n",
      "Serie 22 procesada para entrenamiento:\n",
      "  Longitud original: 1175 -> Longitud final: 1145\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1091\n",
      "Serie 23 procesada para entrenamiento:\n",
      "  Longitud original: 1359 -> Longitud final: 1329\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1275\n",
      "Serie 24 procesada para entrenamiento:\n",
      "  Longitud original: 1358 -> Longitud final: 1328\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1274\n",
      "Serie 25 procesada para entrenamiento:\n",
      "  Longitud original: 1270 -> Longitud final: 1240\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 1186\n",
      "Serie 26 procesada para entrenamiento:\n",
      "  Longitud original: 882 -> Longitud final: 852\n",
      "  Puntos excluidos: 30\n",
      "  V1 en datos entrenamiento - unos: 54, ceros: 798\n",
      "\n",
      "Total de diccionarios de entrenamiento creados: 27\n"
     ]
    }
   ],
   "source": [
    "train = crear_diccionarios_entrenamiento(start, vectores_target, vectores_cat, vectores_dynamic, puntos_a_excluir=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "da6ebd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a44f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 43.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"data_json/diario/train.json\", train)\n",
    "write_dicts_to_file(\"data/json/diario/test.json\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1293bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/22/25 12:12:03] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py#1352\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1352</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/22/25 12:12:03]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=89412;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=733976;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py#1352\u001b\\\u001b[2m1352\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boto_session = boto3.Session(profile_name='lilipink', region_name='us-east-1')\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "s3_client = boto_session.client('s3')\n",
    "sm_client= boto_session.client('sagemaker')\n",
    "s3 = boto_session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5b30f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name, region=None):\n",
    "    try:\n",
    "        if region is None:\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration=location\n",
    "            )\n",
    "        print(f\"Bucket S3 '{bucket_name}' creado exitosamente en {region if region else 'la región por defecto'}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        print(f\"Error al crear el bucket S3: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aaeb08b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket S3 'forecasting-diario-27-v1' creado exitosamente en la región por defecto\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name = \"forecasting-diario-27-v1\"\n",
    "create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f951448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = bucket_name  # replace with an existing bucket if needed\n",
    "s3_bucket_prefix = (\n",
    "        \"lilipink\"  \n",
    "    )\n",
    "default_bucket_prefix = sagemaker_session.default_bucket_prefix\n",
    "if default_bucket_prefix:\n",
    "    s3_prefix = f\"{default_bucket_prefix}/{s3_bucket_prefix}\"\n",
    "else:\n",
    "    s3_prefix = s3_bucket_prefix\n",
    "\n",
    "role = \"arn:aws:iam::844598627082:role/service-role/AmazonSageMaker-ExecutionRole-20250513T105052\"  # IAM role to use by SageMaker\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2df21544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/22/25 12:12:05] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Same images used for training and inference. Defaulting to image     <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\image_uris.py#393\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">393</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         scope: inference.                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/22/25 12:12:05]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Same images used for training and inference. Defaulting to image     \u001b]8;id=635868;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=918545;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\image_uris.py#393\u001b\\\u001b[2m393\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         scope: inference.                                                    \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\image_uris.py#530\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">530</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=839339;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=413463;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\image_uris.py#530\u001b\\\u001b[2m530\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb23bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f8ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file\n",
      "Uploading file to s3://forecasting-diario-27-v1/lilipink/data/train/train.json\n",
      "Overwriting existing file\n",
      "Uploading file to s3://forecasting-diario-27-v1/lilipink/data/test/test.json\n"
     ]
    }
   ],
   "source": [
    "local_file = 'data_json/diario/'\n",
    "copy_to_s3(local_file + 'train.json', s3_data_path + \"/train/train.json\",override=True)\n",
    "copy_to_s3(local_file + 'test.json', s3_data_path + \"/test/test.json\",override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0bbc5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######OPTIMIZACION DE HYPERPARAMETROS###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b611169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/22/25 12:12:07] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py#1352\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1352</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/22/25 12:12:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=633020;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=860717;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\botocore\\credentials.py#1352\u001b\\\u001b[2m1352\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.2xlarge\",\n",
    "   # use_spot_instances=True,\n",
    "   # max_run=1800,  # max training time in seconds\n",
    "   # max_wait=1800,  # seconds to wait for spot instance\n",
    "    base_job_name=\"lilipink-forecasting\",\n",
    "    output_path=s3_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "09b8b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq= \"D\"\n",
    "context_length = 90\n",
    "prediction_length = 30\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"likelihood\": \"student-T\",\n",
    "    #\"learning_rate\": \"0.0001\",\n",
    "}\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "882af81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/22/25 10:32:57] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\telemetry\\telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\telemetry\\telemetry_logging.py#91\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/22/25 10:32:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=117384;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\telemetry\\telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=414108;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\telemetry\\telemetry_logging.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name:                                       <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         lilipink-forecasting-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-05-22-15-32-57-063                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name:                                       \u001b]8;id=892412;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=541686;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         lilipink-forecasting-\u001b[1;36m2025\u001b[0m-05-22-15-32-57-063                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-22 15:32:58 Starting - Starting the training job...\n",
      "2025-05-22 15:33:20 Starting - Preparing the instances for training...\n",
      "2025-05-22 15:33:59 Downloading - Downloading the training image.........\n",
      "2025-05-22 15:35:35 Training - Training image download completed. Training in progress..Docker entrypoint called with argument(s): train\n",
      "Running default environment configuration script\n",
      "Running custom environment configuration script\n",
      "/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '90', 'early_stopping_patience': '40', 'epochs': '400', 'likelihood': 'student-T', 'prediction_length': '30', 'time_freq': 'D'}\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-T', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '90', 'epochs': '400', 'prediction_length': '30', 'time_freq': 'D'}\n",
      "Process 7 is a worker.\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Detected entry point for worker worker\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Using early stopping with patience 40\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] random_seed is None\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] [cardinality=auto] Inferred value of cardinality=[2, 5, 7, 10, 7] from dataset.\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=6 from dataset.\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Training set statistics:\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Integer time series\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] number of time series: 27\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] number of observations: 33388\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] mean target length: 1236.5925925925926\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] min/mean/max target: 0.0/2.095543308973284/185.0\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] mean abs(target): 2.095543308973284\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] contains missing values: no\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Small number of time series. Doing 48 passes over dataset with prob 0.9876543209876543 per epoch.\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Test set statistics:\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Integer time series\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] number of time series: 27\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] number of observations: 34198\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] mean target length: 1266.5925925925926\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] min/mean/max target: 0.0/2.072898999941517/185.0\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] mean abs(target): 2.072898999941517\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] contains missing values: no\n",
      "/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] #memory_usage::<batchbuffer> = 82.7978515625 mb\n",
      "/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] nvidia-smi: took 0.031 seconds to run.\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] nvidia-smi identified 0 GPUs.\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Number of GPUs being used: 0\n",
      "[05/22/2025 15:35:50 INFO 140473021069120] Create Store: local\n",
      "#metrics {\"StartTime\": 1747928150.9887235, \"EndTime\": 1747928151.3480797, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 356.83393478393555, \"count\": 1, \"min\": 356.83393478393555, \"max\": 356.83393478393555}}}\n",
      "[05/22/2025 15:35:51 INFO 140473021069120] Number of GPUs being used: 0\n",
      "[05/22/2025 15:35:51 INFO 140473021069120] #memory_usage::<model> = 113 mb\n",
      "#metrics {\"StartTime\": 1747928151.3481748, \"EndTime\": 1747928151.7892244, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 800.3880977630615, \"count\": 1, \"min\": 800.3880977630615, \"max\": 800.3880977630615}}}\n",
      "[15:35:53] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.461.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 20480 bytes with malloc directly\n",
      "[05/22/2025 15:35:53 INFO 140473021069120] Epoch[0] Batch[0] avg_epoch_loss=1.838120\n",
      "[05/22/2025 15:35:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.8381197452545166\n",
      "[05/22/2025 15:35:55 INFO 140473021069120] Epoch[0] Batch[5] avg_epoch_loss=1.765857\n",
      "[05/22/2025 15:35:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.7658566037813823\n",
      "[05/22/2025 15:35:55 INFO 140473021069120] Epoch[0] Batch [5]#011Speed: 308.71 samples/sec#011loss=1.765857\n",
      "[05/22/2025 15:35:56 INFO 140473021069120] processed a total of 1247 examples\n",
      "#metrics {\"StartTime\": 1747928151.789283, \"EndTime\": 1747928156.9792464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 5189.887523651123, \"count\": 1, \"min\": 5189.887523651123, \"max\": 5189.887523651123}}}\n",
      "[05/22/2025 15:35:56 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=240.27035393487887 records/second\n",
      "[05/22/2025 15:35:56 INFO 140473021069120] #progress_metric: host=algo-1, completed 0.25 % of epochs\n",
      "[05/22/2025 15:35:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.713522469997406\n",
      "[05/22/2025 15:35:56 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:35:57 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_095f75ff-519a-420a-b836-20a9651741bc-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928156.9793153, \"EndTime\": 1747928157.0151792, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 35.500526428222656, \"count\": 1, \"min\": 35.500526428222656, \"max\": 35.500526428222656}}}\n",
      "[05/22/2025 15:35:58 INFO 140473021069120] Epoch[1] Batch[0] avg_epoch_loss=1.658592\n",
      "[05/22/2025 15:35:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.6585917472839355\n",
      "[05/22/2025 15:36:00 INFO 140473021069120] Epoch[1] Batch[5] avg_epoch_loss=1.695647\n",
      "[05/22/2025 15:36:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.6956467827161152\n",
      "[05/22/2025 15:36:00 INFO 140473021069120] Epoch[1] Batch [5]#011Speed: 375.79 samples/sec#011loss=1.695647\n",
      "[05/22/2025 15:36:01 INFO 140473021069120] processed a total of 1232 examples\n",
      "#metrics {\"StartTime\": 1747928157.015237, \"EndTime\": 1747928161.8049924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4789.701223373413, \"count\": 1, \"min\": 4789.701223373413, \"max\": 4789.701223373413}}}\n",
      "[05/22/2025 15:36:01 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=257.2134150833475 records/second\n",
      "[05/22/2025 15:36:01 INFO 140473021069120] #progress_metric: host=algo-1, completed 0.5 % of epochs\n",
      "[05/22/2025 15:36:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.6830418348312377\n",
      "[05/22/2025 15:36:01 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:36:01 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_f34f262a-f02a-47eb-ae38-064fd4843112-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928161.8050575, \"EndTime\": 1747928161.8414059, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 35.92705726623535, \"count\": 1, \"min\": 35.92705726623535, \"max\": 35.92705726623535}}}\n",
      "[05/22/2025 15:36:03 INFO 140473021069120] Epoch[2] Batch[0] avg_epoch_loss=1.609653\n",
      "[05/22/2025 15:36:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.6096527576446533\n",
      "[05/22/2025 15:36:05 INFO 140473021069120] Epoch[2] Batch[5] avg_epoch_loss=1.655764\n",
      "[05/22/2025 15:36:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.6557644406954448\n",
      "[05/22/2025 15:36:05 INFO 140473021069120] Epoch[2] Batch [5]#011Speed: 375.89 samples/sec#011loss=1.655764\n",
      "[05/22/2025 15:36:06 INFO 140473021069120] processed a total of 1280 examples\n",
      "#metrics {\"StartTime\": 1747928161.8414645, \"EndTime\": 1747928166.4732826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4631.763696670532, \"count\": 1, \"min\": 4631.763696670532, \"max\": 4631.763696670532}}}\n",
      "[05/22/2025 15:36:06 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=276.3477764483243 records/second\n",
      "[05/22/2025 15:36:06 INFO 140473021069120] #progress_metric: host=algo-1, completed 0.75 % of epochs\n",
      "[05/22/2025 15:36:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.6420965671539307\n",
      "[05/22/2025 15:36:06 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:36:06 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_c59977ef-2a4d-4b6d-9fd7-e90a0c0af790-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928166.4733355, \"EndTime\": 1747928166.5078855, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 34.104108810424805, \"count\": 1, \"min\": 34.104108810424805, \"max\": 34.104108810424805}}}\n",
      "[05/22/2025 15:36:08 INFO 140473021069120] Epoch[3] Batch[0] avg_epoch_loss=1.546216\n",
      "[05/22/2025 15:36:08 INFO 140473021069120] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.546216368675232\n",
      "[05/22/2025 15:36:09 INFO 140473021069120] Epoch[3] Batch[5] avg_epoch_loss=1.590846\n",
      "[05/22/2025 15:36:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.5908464590708415\n",
      "[05/22/2025 15:36:09 INFO 140473021069120] Epoch[3] Batch [5]#011Speed: 376.06 samples/sec#011loss=1.590846\n",
      "[05/22/2025 15:36:11 INFO 140473021069120] Epoch[3] Batch[10] avg_epoch_loss=1.529662\n",
      "[05/22/2025 15:36:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=1.4562397003173828\n",
      "[05/22/2025 15:36:11 INFO 140473021069120] Epoch[3] Batch [10]#011Speed: 367.60 samples/sec#011loss=1.456240\n",
      "[05/22/2025 15:36:11 INFO 140473021069120] processed a total of 1300 examples\n",
      "#metrics {\"StartTime\": 1747928166.5079522, \"EndTime\": 1747928171.4702103, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4962.200880050659, \"count\": 1, \"min\": 4962.200880050659, \"max\": 4962.200880050659}}}\n",
      "[05/22/2025 15:36:11 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.9759323633915 records/second\n",
      "[05/22/2025 15:36:11 INFO 140473021069120] #progress_metric: host=algo-1, completed 1.0 % of epochs\n",
      "[05/22/2025 15:36:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.5296615687283603\n",
      "[05/22/2025 15:36:11 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:36:11 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_1d227f3c-4273-4259-8b58-9356a9b470bc-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928171.4702687, \"EndTime\": 1747928171.5067272, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 36.170005798339844, \"count\": 1, \"min\": 36.170005798339844, \"max\": 36.170005798339844}}}\n",
      "[05/22/2025 15:36:13 INFO 140473021069120] Epoch[4] Batch[0] avg_epoch_loss=1.485207\n",
      "[05/22/2025 15:36:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.4852070808410645\n",
      "[05/22/2025 15:36:14 INFO 140473021069120] Epoch[4] Batch[5] avg_epoch_loss=1.542105\n",
      "[05/22/2025 15:36:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.5421054760615032\n",
      "[05/22/2025 15:36:14 INFO 140473021069120] Epoch[4] Batch [5]#011Speed: 380.64 samples/sec#011loss=1.542105\n",
      "[05/22/2025 15:36:16 INFO 140473021069120] Epoch[4] Batch[10] avg_epoch_loss=1.563174\n",
      "[05/22/2025 15:36:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=1.5884559869766235\n",
      "[05/22/2025 15:36:16 INFO 140473021069120] Epoch[4] Batch [10]#011Speed: 369.87 samples/sec#011loss=1.588456\n",
      "[05/22/2025 15:36:16 INFO 140473021069120] processed a total of 1317 examples\n",
      "#metrics {\"StartTime\": 1747928171.5067863, \"EndTime\": 1747928176.5003328, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4993.492126464844, \"count\": 1, \"min\": 4993.492126464844, \"max\": 4993.492126464844}}}\n",
      "[05/22/2025 15:36:16 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.73866016290316 records/second\n",
      "[05/22/2025 15:36:16 INFO 140473021069120] #progress_metric: host=algo-1, completed 1.25 % of epochs\n",
      "[05/22/2025 15:36:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.5631738901138306\n",
      "[05/22/2025 15:36:16 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:36:18 INFO 140473021069120] Epoch[5] Batch[0] avg_epoch_loss=1.605776\n",
      "[05/22/2025 15:36:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.6057761907577515\n",
      "[05/22/2025 15:36:19 INFO 140473021069120] Epoch[5] Batch[5] avg_epoch_loss=1.583091\n",
      "[05/22/2025 15:36:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.5830905834833782\n",
      "[05/22/2025 15:36:19 INFO 140473021069120] Epoch[5] Batch [5]#011Speed: 382.95 samples/sec#011loss=1.583091\n",
      "[05/22/2025 15:36:21 INFO 140473021069120] processed a total of 1275 examples\n",
      "#metrics {\"StartTime\": 1747928176.5003922, \"EndTime\": 1747928181.0935757, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4592.806100845337, \"count\": 1, \"min\": 4592.806100845337, \"max\": 4592.806100845337}}}\n",
      "[05/22/2025 15:36:21 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=277.6024128300966 records/second\n",
      "[05/22/2025 15:36:21 INFO 140473021069120] #progress_metric: host=algo-1, completed 1.5 % of epochs\n",
      "[05/22/2025 15:36:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.5691081285476685\n",
      "[05/22/2025 15:36:21 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:36:22 INFO 140473021069120] Epoch[6] Batch[0] avg_epoch_loss=1.574448\n",
      "[05/22/2025 15:36:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.574447512626648\n",
      "[05/22/2025 15:36:24 INFO 140473021069120] Epoch[6] Batch[5] avg_epoch_loss=1.516523\n",
      "[05/22/2025 15:36:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.5165226856867473\n",
      "[05/22/2025 15:36:24 INFO 140473021069120] Epoch[6] Batch [5]#011Speed: 376.83 samples/sec#011loss=1.516523\n",
      "[05/22/2025 15:36:26 INFO 140473021069120] Epoch[6] Batch[10] avg_epoch_loss=1.466718\n",
      "[05/22/2025 15:36:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=1.4069531202316283\n",
      "[05/22/2025 15:36:26 INFO 140473021069120] Epoch[6] Batch [10]#011Speed: 360.47 samples/sec#011loss=1.406953\n",
      "[05/22/2025 15:36:26 INFO 140473021069120] processed a total of 1324 examples\n",
      "#metrics {\"StartTime\": 1747928181.0936382, \"EndTime\": 1747928186.0823965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4988.438844680786, \"count\": 1, \"min\": 4988.438844680786, \"max\": 4988.438844680786}}}\n",
      "[05/22/2025 15:36:26 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=265.4092453513725 records/second\n",
      "[05/22/2025 15:36:26 INFO 140473021069120] #progress_metric: host=algo-1, completed 1.75 % of epochs\n",
      "[05/22/2025 15:36:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.4667183377526023\n",
      "[05/22/2025 15:36:26 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:36:26 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_4d2c6a12-12e8-4931-b26f-9fb6afd28228-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928186.0824513, \"EndTime\": 1747928186.119111, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 36.345720291137695, \"count\": 1, \"min\": 36.345720291137695, \"max\": 36.345720291137695}}}\n",
      "[05/22/2025 15:36:27 INFO 140473021069120] Epoch[7] Batch[0] avg_epoch_loss=1.533214\n",
      "[05/22/2025 15:36:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.5332143306732178\n",
      "[05/22/2025 15:36:29 INFO 140473021069120] Epoch[7] Batch[5] avg_epoch_loss=1.532068\n",
      "[05/22/2025 15:36:29 INFO 140473021069120] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.5320682724316914\n",
      "[05/22/2025 15:36:29 INFO 140473021069120] Epoch[7] Batch [5]#011Speed: 379.24 samples/sec#011loss=1.532068\n",
      "[05/22/2025 15:36:31 INFO 140473021069120] Epoch[7] Batch[10] avg_epoch_loss=1.655691\n",
      "[05/22/2025 15:36:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=1.8040385246276855\n",
      "[05/22/2025 15:36:31 INFO 140473021069120] Epoch[7] Batch [10]#011Speed: 372.06 samples/sec#011loss=1.804039\n",
      "[05/22/2025 15:36:31 INFO 140473021069120] processed a total of 1292 examples\n",
      "#metrics {\"StartTime\": 1747928186.119173, \"EndTime\": 1747928191.0529141, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4933.686017990112, \"count\": 1, \"min\": 4933.686017990112, \"max\": 4933.686017990112}}}\n",
      "[05/22/2025 15:36:31 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.86855157771356 records/second\n",
      "[05/22/2025 15:36:31 INFO 140473021069120] #progress_metric: host=algo-1, completed 2.0 % of epochs\n",
      "[05/22/2025 15:36:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.6556911143389614\n",
      "[05/22/2025 15:36:31 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:36:32 INFO 140473021069120] Epoch[8] Batch[0] avg_epoch_loss=1.561948\n",
      "[05/22/2025 15:36:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.5619475841522217\n",
      "[05/22/2025 15:36:34 INFO 140473021069120] Epoch[8] Batch[5] avg_epoch_loss=1.519197\n",
      "[05/22/2025 15:36:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.5191968083381653\n",
      "[05/22/2025 15:36:34 INFO 140473021069120] Epoch[8] Batch [5]#011Speed: 376.51 samples/sec#011loss=1.519197\n",
      "[05/22/2025 15:36:35 INFO 140473021069120] processed a total of 1253 examples\n",
      "#metrics {\"StartTime\": 1747928191.0529702, \"EndTime\": 1747928195.6557567, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4602.432727813721, \"count\": 1, \"min\": 4602.432727813721, \"max\": 4602.432727813721}}}\n",
      "[05/22/2025 15:36:35 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=272.24181107377996 records/second\n",
      "[05/22/2025 15:36:35 INFO 140473021069120] #progress_metric: host=algo-1, completed 2.25 % of epochs\n",
      "[05/22/2025 15:36:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.4781713366508484\n",
      "[05/22/2025 15:36:35 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:36:37 INFO 140473021069120] Epoch[9] Batch[0] avg_epoch_loss=1.415660\n",
      "[05/22/2025 15:36:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.4156595468521118\n",
      "[05/22/2025 15:36:38 INFO 140473021069120] Epoch[9] Batch[5] avg_epoch_loss=1.424871\n",
      "[05/22/2025 15:36:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.4248713453610737\n",
      "[05/22/2025 15:36:38 INFO 140473021069120] Epoch[9] Batch [5]#011Speed: 385.69 samples/sec#011loss=1.424871\n",
      "[05/22/2025 15:36:40 INFO 140473021069120] Epoch[9] Batch[10] avg_epoch_loss=1.473894\n",
      "[05/22/2025 15:36:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=1.5327213764190675\n",
      "[05/22/2025 15:36:40 INFO 140473021069120] Epoch[9] Batch [10]#011Speed: 368.43 samples/sec#011loss=1.532721\n",
      "[05/22/2025 15:36:40 INFO 140473021069120] processed a total of 1294 examples\n",
      "#metrics {\"StartTime\": 1747928195.6558187, \"EndTime\": 1747928200.5708554, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4914.651155471802, \"count\": 1, \"min\": 4914.651155471802, \"max\": 4914.651155471802}}}\n",
      "[05/22/2025 15:36:40 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.2898525969983 records/second\n",
      "[05/22/2025 15:36:40 INFO 140473021069120] #progress_metric: host=algo-1, completed 2.5 % of epochs\n",
      "[05/22/2025 15:36:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.4738940867510708\n",
      "[05/22/2025 15:36:40 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:36:42 INFO 140473021069120] Epoch[10] Batch[0] avg_epoch_loss=1.481344\n",
      "[05/22/2025 15:36:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.4813436269760132\n",
      "[05/22/2025 15:36:43 INFO 140473021069120] Epoch[10] Batch[5] avg_epoch_loss=1.434829\n",
      "[05/22/2025 15:36:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.434828797976176\n",
      "[05/22/2025 15:36:43 INFO 140473021069120] Epoch[10] Batch [5]#011Speed: 382.21 samples/sec#011loss=1.434829\n",
      "[05/22/2025 15:36:45 INFO 140473021069120] Epoch[10] Batch[10] avg_epoch_loss=1.282830\n",
      "[05/22/2025 15:36:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=1.1004304483532905\n",
      "[05/22/2025 15:36:45 INFO 140473021069120] Epoch[10] Batch [10]#011Speed: 362.52 samples/sec#011loss=1.100430\n",
      "[05/22/2025 15:36:45 INFO 140473021069120] processed a total of 1305 examples\n",
      "#metrics {\"StartTime\": 1747928200.5709124, \"EndTime\": 1747928205.5433419, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4972.1360206604, \"count\": 1, \"min\": 4972.1360206604, \"max\": 4972.1360206604}}}\n",
      "[05/22/2025 15:36:45 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.4581956445561 records/second\n",
      "[05/22/2025 15:36:45 INFO 140473021069120] #progress_metric: host=algo-1, completed 2.75 % of epochs\n",
      "[05/22/2025 15:36:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.2828295481475918\n",
      "[05/22/2025 15:36:45 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:36:45 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_bceb0a66-cf94-4419-8c8d-c6a0a27f84ab-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928205.5433974, \"EndTime\": 1747928205.577668, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 33.919334411621094, \"count\": 1, \"min\": 33.919334411621094, \"max\": 33.919334411621094}}}\n",
      "[05/22/2025 15:36:47 INFO 140473021069120] Epoch[11] Batch[0] avg_epoch_loss=1.358055\n",
      "[05/22/2025 15:36:47 INFO 140473021069120] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.3580549955368042\n",
      "[05/22/2025 15:36:48 INFO 140473021069120] Epoch[11] Batch[5] avg_epoch_loss=1.403486\n",
      "[05/22/2025 15:36:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.4034862319628398\n",
      "[05/22/2025 15:36:48 INFO 140473021069120] Epoch[11] Batch [5]#011Speed: 376.92 samples/sec#011loss=1.403486\n",
      "[05/22/2025 15:36:50 INFO 140473021069120] Epoch[11] Batch[10] avg_epoch_loss=1.463814\n",
      "[05/22/2025 15:36:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=1.5362065315246582\n",
      "[05/22/2025 15:36:50 INFO 140473021069120] Epoch[11] Batch [10]#011Speed: 358.93 samples/sec#011loss=1.536207\n",
      "[05/22/2025 15:36:50 INFO 140473021069120] processed a total of 1298 examples\n",
      "#metrics {\"StartTime\": 1747928205.577717, \"EndTime\": 1747928210.5905337, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5012.770652770996, \"count\": 1, \"min\": 5012.770652770996, \"max\": 5012.770652770996}}}\n",
      "[05/22/2025 15:36:50 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=258.934264915943 records/second\n",
      "[05/22/2025 15:36:50 INFO 140473021069120] #progress_metric: host=algo-1, completed 3.0 % of epochs\n",
      "[05/22/2025 15:36:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.4638136408545754\n",
      "[05/22/2025 15:36:50 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:36:52 INFO 140473021069120] Epoch[12] Batch[0] avg_epoch_loss=1.449086\n",
      "[05/22/2025 15:36:52 INFO 140473021069120] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.4490858316421509\n",
      "[05/22/2025 15:36:53 INFO 140473021069120] Epoch[12] Batch[5] avg_epoch_loss=1.442408\n",
      "[05/22/2025 15:36:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.4424079259236653\n",
      "[05/22/2025 15:36:53 INFO 140473021069120] Epoch[12] Batch [5]#011Speed: 382.28 samples/sec#011loss=1.442408\n",
      "[05/22/2025 15:36:55 INFO 140473021069120] Epoch[12] Batch[10] avg_epoch_loss=1.305004\n",
      "[05/22/2025 15:36:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=1.1401194125413894\n",
      "[05/22/2025 15:36:55 INFO 140473021069120] Epoch[12] Batch [10]#011Speed: 364.65 samples/sec#011loss=1.140119\n",
      "[05/22/2025 15:36:55 INFO 140473021069120] processed a total of 1317 examples\n",
      "#metrics {\"StartTime\": 1747928210.5905914, \"EndTime\": 1747928215.5330274, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4942.116260528564, \"count\": 1, \"min\": 4942.116260528564, \"max\": 4942.116260528564}}}\n",
      "[05/22/2025 15:36:55 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=266.48040199337186 records/second\n",
      "[05/22/2025 15:36:55 INFO 140473021069120] #progress_metric: host=algo-1, completed 3.25 % of epochs\n",
      "[05/22/2025 15:36:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.3050040562044491\n",
      "[05/22/2025 15:36:55 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:36:57 INFO 140473021069120] Epoch[13] Batch[0] avg_epoch_loss=1.371842\n",
      "[05/22/2025 15:36:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.3718416690826416\n",
      "[05/22/2025 15:36:58 INFO 140473021069120] Epoch[13] Batch[5] avg_epoch_loss=1.390186\n",
      "[05/22/2025 15:36:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.3901859919230144\n",
      "[05/22/2025 15:36:58 INFO 140473021069120] Epoch[13] Batch [5]#011Speed: 380.54 samples/sec#011loss=1.390186\n",
      "[05/22/2025 15:37:00 INFO 140473021069120] processed a total of 1255 examples\n",
      "#metrics {\"StartTime\": 1747928215.533085, \"EndTime\": 1747928220.1313572, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4598.016023635864, \"count\": 1, \"min\": 4598.016023635864, \"max\": 4598.016023635864}}}\n",
      "[05/22/2025 15:37:00 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=272.93827376605503 records/second\n",
      "[05/22/2025 15:37:00 INFO 140473021069120] #progress_metric: host=algo-1, completed 3.5 % of epochs\n",
      "[05/22/2025 15:37:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.3440600693225861\n",
      "[05/22/2025 15:37:00 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:01 INFO 140473021069120] Epoch[14] Batch[0] avg_epoch_loss=1.456962\n",
      "[05/22/2025 15:37:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=1.4569618701934814\n",
      "[05/22/2025 15:37:03 INFO 140473021069120] Epoch[14] Batch[5] avg_epoch_loss=1.415258\n",
      "[05/22/2025 15:37:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.4152578115463257\n",
      "[05/22/2025 15:37:03 INFO 140473021069120] Epoch[14] Batch [5]#011Speed: 381.93 samples/sec#011loss=1.415258\n",
      "[05/22/2025 15:37:04 INFO 140473021069120] processed a total of 1244 examples\n",
      "#metrics {\"StartTime\": 1747928220.1314204, \"EndTime\": 1747928224.7475348, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4615.7495975494385, \"count\": 1, \"min\": 4615.7495975494385, \"max\": 4615.7495975494385}}}\n",
      "[05/22/2025 15:37:04 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.5013990254444 records/second\n",
      "[05/22/2025 15:37:04 INFO 140473021069120] #progress_metric: host=algo-1, completed 3.75 % of epochs\n",
      "[05/22/2025 15:37:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=14, train loss <loss>=1.4635392069816588\n",
      "[05/22/2025 15:37:04 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:06 INFO 140473021069120] Epoch[15] Batch[0] avg_epoch_loss=1.436368\n",
      "[05/22/2025 15:37:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.4363681077957153\n",
      "[05/22/2025 15:37:07 INFO 140473021069120] Epoch[15] Batch[5] avg_epoch_loss=1.307949\n",
      "[05/22/2025 15:37:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=1.307949165503184\n",
      "[05/22/2025 15:37:07 INFO 140473021069120] Epoch[15] Batch [5]#011Speed: 377.16 samples/sec#011loss=1.307949\n",
      "[05/22/2025 15:37:09 INFO 140473021069120] Epoch[15] Batch[10] avg_epoch_loss=1.325684\n",
      "[05/22/2025 15:37:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=1.346964955329895\n",
      "[05/22/2025 15:37:09 INFO 140473021069120] Epoch[15] Batch [10]#011Speed: 375.98 samples/sec#011loss=1.346965\n",
      "[05/22/2025 15:37:09 INFO 140473021069120] processed a total of 1283 examples\n",
      "#metrics {\"StartTime\": 1747928224.747666, \"EndTime\": 1747928229.6872118, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4939.150094985962, \"count\": 1, \"min\": 4939.150094985962, \"max\": 4939.150094985962}}}\n",
      "[05/22/2025 15:37:09 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.7568010858896 records/second\n",
      "[05/22/2025 15:37:09 INFO 140473021069120] #progress_metric: host=algo-1, completed 4.0 % of epochs\n",
      "[05/22/2025 15:37:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.3256836154244163\n",
      "[05/22/2025 15:37:09 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:11 INFO 140473021069120] Epoch[16] Batch[0] avg_epoch_loss=1.447093\n",
      "[05/22/2025 15:37:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=1.4470934867858887\n",
      "[05/22/2025 15:37:12 INFO 140473021069120] Epoch[16] Batch[5] avg_epoch_loss=1.329326\n",
      "[05/22/2025 15:37:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=1.3293257753054302\n",
      "[05/22/2025 15:37:12 INFO 140473021069120] Epoch[16] Batch [5]#011Speed: 383.36 samples/sec#011loss=1.329326\n",
      "[05/22/2025 15:37:14 INFO 140473021069120] Epoch[16] Batch[10] avg_epoch_loss=1.221334\n",
      "[05/22/2025 15:37:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=1.0917433857917787\n",
      "[05/22/2025 15:37:14 INFO 140473021069120] Epoch[16] Batch [10]#011Speed: 368.84 samples/sec#011loss=1.091743\n",
      "[05/22/2025 15:37:14 INFO 140473021069120] processed a total of 1304 examples\n",
      "#metrics {\"StartTime\": 1747928229.6872675, \"EndTime\": 1747928234.5991163, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4911.559104919434, \"count\": 1, \"min\": 4911.559104919434, \"max\": 4911.559104919434}}}\n",
      "[05/22/2025 15:37:14 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=265.4916713181582 records/second\n",
      "[05/22/2025 15:37:14 INFO 140473021069120] #progress_metric: host=algo-1, completed 4.25 % of epochs\n",
      "[05/22/2025 15:37:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=16, train loss <loss>=1.2213337800719521\n",
      "[05/22/2025 15:37:14 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:37:14 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_f13e22a7-8eb5-4d6e-b4fc-5f9ea87cfad1-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928234.5991724, \"EndTime\": 1747928234.6343699, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 34.91950035095215, \"count\": 1, \"min\": 34.91950035095215, \"max\": 34.91950035095215}}}\n",
      "[05/22/2025 15:37:16 INFO 140473021069120] Epoch[17] Batch[0] avg_epoch_loss=1.444599\n",
      "[05/22/2025 15:37:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.4445993900299072\n",
      "[05/22/2025 15:37:17 INFO 140473021069120] Epoch[17] Batch[5] avg_epoch_loss=1.435317\n",
      "[05/22/2025 15:37:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=1.4353174964586894\n",
      "[05/22/2025 15:37:17 INFO 140473021069120] Epoch[17] Batch [5]#011Speed: 370.50 samples/sec#011loss=1.435317\n",
      "[05/22/2025 15:37:19 INFO 140473021069120] Epoch[17] Batch[10] avg_epoch_loss=1.483378\n",
      "[05/22/2025 15:37:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=1.5410504579544066\n",
      "[05/22/2025 15:37:19 INFO 140473021069120] Epoch[17] Batch [10]#011Speed: 367.42 samples/sec#011loss=1.541050\n",
      "[05/22/2025 15:37:19 INFO 140473021069120] processed a total of 1314 examples\n",
      "#metrics {\"StartTime\": 1747928234.634427, \"EndTime\": 1747928239.6171157, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4982.634782791138, \"count\": 1, \"min\": 4982.634782791138, \"max\": 4982.634782791138}}}\n",
      "[05/22/2025 15:37:19 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.71141717514297 records/second\n",
      "[05/22/2025 15:37:19 INFO 140473021069120] #progress_metric: host=algo-1, completed 4.5 % of epochs\n",
      "[05/22/2025 15:37:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=17, train loss <loss>=1.4833779335021973\n",
      "[05/22/2025 15:37:19 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:21 INFO 140473021069120] Epoch[18] Batch[0] avg_epoch_loss=1.447894\n",
      "[05/22/2025 15:37:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=1.447893500328064\n",
      "[05/22/2025 15:37:22 INFO 140473021069120] Epoch[18] Batch[5] avg_epoch_loss=1.396854\n",
      "[05/22/2025 15:37:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=1.3968536257743835\n",
      "[05/22/2025 15:37:22 INFO 140473021069120] Epoch[18] Batch [5]#011Speed: 367.24 samples/sec#011loss=1.396854\n",
      "[05/22/2025 15:37:24 INFO 140473021069120] processed a total of 1264 examples\n",
      "#metrics {\"StartTime\": 1747928239.6171696, \"EndTime\": 1747928244.3236587, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4706.163644790649, \"count\": 1, \"min\": 4706.163644790649, \"max\": 4706.163644790649}}}\n",
      "[05/22/2025 15:37:24 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.57859819364995 records/second\n",
      "[05/22/2025 15:37:24 INFO 140473021069120] #progress_metric: host=algo-1, completed 4.75 % of epochs\n",
      "[05/22/2025 15:37:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=18, train loss <loss>=1.353439700603485\n",
      "[05/22/2025 15:37:24 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:25 INFO 140473021069120] Epoch[19] Batch[0] avg_epoch_loss=1.353936\n",
      "[05/22/2025 15:37:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=1.3539361953735352\n",
      "[05/22/2025 15:37:27 INFO 140473021069120] Epoch[19] Batch[5] avg_epoch_loss=1.364001\n",
      "[05/22/2025 15:37:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=1.3640013734499614\n",
      "[05/22/2025 15:37:27 INFO 140473021069120] Epoch[19] Batch [5]#011Speed: 371.41 samples/sec#011loss=1.364001\n",
      "[05/22/2025 15:37:28 INFO 140473021069120] processed a total of 1246 examples\n",
      "#metrics {\"StartTime\": 1747928244.3237226, \"EndTime\": 1747928248.9732642, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4649.240255355835, \"count\": 1, \"min\": 4649.240255355835, \"max\": 4649.240255355835}}}\n",
      "[05/22/2025 15:37:28 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=267.9954719604355 records/second\n",
      "[05/22/2025 15:37:28 INFO 140473021069120] #progress_metric: host=algo-1, completed 5.0 % of epochs\n",
      "[05/22/2025 15:37:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=19, train loss <loss>=1.3561826825141907\n",
      "[05/22/2025 15:37:28 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:30 INFO 140473021069120] Epoch[20] Batch[0] avg_epoch_loss=1.338450\n",
      "[05/22/2025 15:37:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=1.3384498357772827\n",
      "[05/22/2025 15:37:32 INFO 140473021069120] Epoch[20] Batch[5] avg_epoch_loss=1.300714\n",
      "[05/22/2025 15:37:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=1.300714413324992\n",
      "[05/22/2025 15:37:32 INFO 140473021069120] Epoch[20] Batch [5]#011Speed: 370.76 samples/sec#011loss=1.300714\n",
      "[05/22/2025 15:37:33 INFO 140473021069120] processed a total of 1270 examples\n",
      "#metrics {\"StartTime\": 1747928248.973326, \"EndTime\": 1747928253.6698103, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4696.191310882568, \"count\": 1, \"min\": 4696.191310882568, \"max\": 4696.191310882568}}}\n",
      "[05/22/2025 15:37:33 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=270.4264213393128 records/second\n",
      "[05/22/2025 15:37:33 INFO 140473021069120] #progress_metric: host=algo-1, completed 5.25 % of epochs\n",
      "[05/22/2025 15:37:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=20, train loss <loss>=1.2932867169380189\n",
      "[05/22/2025 15:37:33 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:35 INFO 140473021069120] Epoch[21] Batch[0] avg_epoch_loss=1.442183\n",
      "[05/22/2025 15:37:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=1.4421825408935547\n",
      "[05/22/2025 15:37:36 INFO 140473021069120] Epoch[21] Batch[5] avg_epoch_loss=1.371838\n",
      "[05/22/2025 15:37:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=1.3718382716178894\n",
      "[05/22/2025 15:37:36 INFO 140473021069120] Epoch[21] Batch [5]#011Speed: 364.04 samples/sec#011loss=1.371838\n",
      "[05/22/2025 15:37:38 INFO 140473021069120] Epoch[21] Batch[10] avg_epoch_loss=1.277499\n",
      "[05/22/2025 15:37:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=1.1642921447753907\n",
      "[05/22/2025 15:37:38 INFO 140473021069120] Epoch[21] Batch [10]#011Speed: 358.27 samples/sec#011loss=1.164292\n",
      "[05/22/2025 15:37:38 INFO 140473021069120] processed a total of 1293 examples\n",
      "#metrics {\"StartTime\": 1747928253.6698754, \"EndTime\": 1747928258.7558308, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5085.608005523682, \"count\": 1, \"min\": 5085.608005523682, \"max\": 5085.608005523682}}}\n",
      "[05/22/2025 15:37:38 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=254.2413200135971 records/second\n",
      "[05/22/2025 15:37:38 INFO 140473021069120] #progress_metric: host=algo-1, completed 5.5 % of epochs\n",
      "[05/22/2025 15:37:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=21, train loss <loss>=1.2774991230531172\n",
      "[05/22/2025 15:37:38 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:40 INFO 140473021069120] Epoch[22] Batch[0] avg_epoch_loss=1.462404\n",
      "[05/22/2025 15:37:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=1.4624042510986328\n",
      "[05/22/2025 15:37:42 INFO 140473021069120] Epoch[22] Batch[5] avg_epoch_loss=1.308905\n",
      "[05/22/2025 15:37:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=1.3089049061139424\n",
      "[05/22/2025 15:37:42 INFO 140473021069120] Epoch[22] Batch [5]#011Speed: 370.25 samples/sec#011loss=1.308905\n",
      "[05/22/2025 15:37:43 INFO 140473021069120] processed a total of 1262 examples\n",
      "#metrics {\"StartTime\": 1747928258.7559135, \"EndTime\": 1747928263.4701269, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4713.767528533936, \"count\": 1, \"min\": 4713.767528533936, \"max\": 4713.767528533936}}}\n",
      "[05/22/2025 15:37:43 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=267.7211302023032 records/second\n",
      "[05/22/2025 15:37:43 INFO 140473021069120] #progress_metric: host=algo-1, completed 5.75 % of epochs\n",
      "[05/22/2025 15:37:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=22, train loss <loss>=1.2901718020439148\n",
      "[05/22/2025 15:37:43 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:45 INFO 140473021069120] Epoch[23] Batch[0] avg_epoch_loss=1.307763\n",
      "[05/22/2025 15:37:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=1.3077634572982788\n",
      "[05/22/2025 15:37:46 INFO 140473021069120] Epoch[23] Batch[5] avg_epoch_loss=1.355861\n",
      "[05/22/2025 15:37:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=1.3558613061904907\n",
      "[05/22/2025 15:37:46 INFO 140473021069120] Epoch[23] Batch [5]#011Speed: 372.08 samples/sec#011loss=1.355861\n",
      "[05/22/2025 15:37:48 INFO 140473021069120] processed a total of 1236 examples\n",
      "#metrics {\"StartTime\": 1747928263.4701893, \"EndTime\": 1747928268.1366584, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4666.137456893921, \"count\": 1, \"min\": 4666.137456893921, \"max\": 4666.137456893921}}}\n",
      "[05/22/2025 15:37:48 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=264.88203859279827 records/second\n",
      "[05/22/2025 15:37:48 INFO 140473021069120] #progress_metric: host=algo-1, completed 6.0 % of epochs\n",
      "[05/22/2025 15:37:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=23, train loss <loss>=1.21187162399292\n",
      "[05/22/2025 15:37:48 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:37:48 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_84842aa5-f803-4b64-b320-30cf85ee5319-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928268.1367204, \"EndTime\": 1747928268.1725795, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 35.54987907409668, \"count\": 1, \"min\": 35.54987907409668, \"max\": 35.54987907409668}}}\n",
      "[05/22/2025 15:37:49 INFO 140473021069120] Epoch[24] Batch[0] avg_epoch_loss=1.253200\n",
      "[05/22/2025 15:37:49 INFO 140473021069120] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=1.2532004117965698\n",
      "[05/22/2025 15:37:51 INFO 140473021069120] Epoch[24] Batch[5] avg_epoch_loss=1.258884\n",
      "[05/22/2025 15:37:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=1.2588844497998555\n",
      "[05/22/2025 15:37:51 INFO 140473021069120] Epoch[24] Batch [5]#011Speed: 372.86 samples/sec#011loss=1.258884\n",
      "[05/22/2025 15:37:53 INFO 140473021069120] Epoch[24] Batch[10] avg_epoch_loss=1.230283\n",
      "[05/22/2025 15:37:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=1.1959611296653747\n",
      "[05/22/2025 15:37:53 INFO 140473021069120] Epoch[24] Batch [10]#011Speed: 351.89 samples/sec#011loss=1.195961\n",
      "[05/22/2025 15:37:53 INFO 140473021069120] processed a total of 1321 examples\n",
      "#metrics {\"StartTime\": 1747928268.1726413, \"EndTime\": 1747928273.2589998, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5086.3049030303955, \"count\": 1, \"min\": 5086.3049030303955, \"max\": 5086.3049030303955}}}\n",
      "[05/22/2025 15:37:53 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.7126467777598 records/second\n",
      "[05/22/2025 15:37:53 INFO 140473021069120] #progress_metric: host=algo-1, completed 6.25 % of epochs\n",
      "[05/22/2025 15:37:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=24, train loss <loss>=1.2302829406478188\n",
      "[05/22/2025 15:37:53 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:54 INFO 140473021069120] Epoch[25] Batch[0] avg_epoch_loss=1.213531\n",
      "[05/22/2025 15:37:54 INFO 140473021069120] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=1.2135306596755981\n",
      "[05/22/2025 15:37:56 INFO 140473021069120] Epoch[25] Batch[5] avg_epoch_loss=1.270795\n",
      "[05/22/2025 15:37:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=1.2707950870196025\n",
      "[05/22/2025 15:37:56 INFO 140473021069120] Epoch[25] Batch [5]#011Speed: 372.67 samples/sec#011loss=1.270795\n",
      "[05/22/2025 15:37:58 INFO 140473021069120] Epoch[25] Batch[10] avg_epoch_loss=1.236249\n",
      "[05/22/2025 15:37:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=1.1947944641113282\n",
      "[05/22/2025 15:37:58 INFO 140473021069120] Epoch[25] Batch [10]#011Speed: 356.82 samples/sec#011loss=1.194794\n",
      "[05/22/2025 15:37:58 INFO 140473021069120] processed a total of 1325 examples\n",
      "#metrics {\"StartTime\": 1747928273.2590578, \"EndTime\": 1747928278.3068812, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5047.566652297974, \"count\": 1, \"min\": 5047.566652297974, \"max\": 5047.566652297974}}}\n",
      "[05/22/2025 15:37:58 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.49761648250586 records/second\n",
      "[05/22/2025 15:37:58 INFO 140473021069120] #progress_metric: host=algo-1, completed 6.5 % of epochs\n",
      "[05/22/2025 15:37:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=25, train loss <loss>=1.2362493493340232\n",
      "[05/22/2025 15:37:58 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:37:59 INFO 140473021069120] Epoch[26] Batch[0] avg_epoch_loss=1.194875\n",
      "[05/22/2025 15:37:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=1.1948747634887695\n",
      "[05/22/2025 15:38:01 INFO 140473021069120] Epoch[26] Batch[5] avg_epoch_loss=1.282276\n",
      "[05/22/2025 15:38:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=1.28227565685908\n",
      "[05/22/2025 15:38:01 INFO 140473021069120] Epoch[26] Batch [5]#011Speed: 360.96 samples/sec#011loss=1.282276\n",
      "[05/22/2025 15:38:03 INFO 140473021069120] Epoch[26] Batch[10] avg_epoch_loss=1.319166\n",
      "[05/22/2025 15:38:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=1.3634346961975097\n",
      "[05/22/2025 15:38:03 INFO 140473021069120] Epoch[26] Batch [10]#011Speed: 351.93 samples/sec#011loss=1.363435\n",
      "[05/22/2025 15:38:03 INFO 140473021069120] processed a total of 1297 examples\n",
      "#metrics {\"StartTime\": 1747928278.3069468, \"EndTime\": 1747928283.4518483, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5144.553661346436, \"count\": 1, \"min\": 5144.553661346436, \"max\": 5144.553661346436}}}\n",
      "[05/22/2025 15:38:03 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=252.10690865909274 records/second\n",
      "[05/22/2025 15:38:03 INFO 140473021069120] #progress_metric: host=algo-1, completed 6.75 % of epochs\n",
      "[05/22/2025 15:38:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=26, train loss <loss>=1.319166129285639\n",
      "[05/22/2025 15:38:03 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:05 INFO 140473021069120] Epoch[27] Batch[0] avg_epoch_loss=1.260641\n",
      "[05/22/2025 15:38:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=1.2606414556503296\n",
      "[05/22/2025 15:38:06 INFO 140473021069120] Epoch[27] Batch[5] avg_epoch_loss=1.229142\n",
      "[05/22/2025 15:38:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=1.229141632715861\n",
      "[05/22/2025 15:38:06 INFO 140473021069120] Epoch[27] Batch [5]#011Speed: 363.72 samples/sec#011loss=1.229142\n",
      "[05/22/2025 15:38:08 INFO 140473021069120] Epoch[27] Batch[10] avg_epoch_loss=1.446237\n",
      "[05/22/2025 15:38:08 INFO 140473021069120] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=1.7067512273788452\n",
      "[05/22/2025 15:38:08 INFO 140473021069120] Epoch[27] Batch [10]#011Speed: 356.09 samples/sec#011loss=1.706751\n",
      "[05/22/2025 15:38:08 INFO 140473021069120] processed a total of 1282 examples\n",
      "#metrics {\"StartTime\": 1747928283.451908, \"EndTime\": 1747928288.5710068, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5118.812799453735, \"count\": 1, \"min\": 5118.812799453735, \"max\": 5118.812799453735}}}\n",
      "[05/22/2025 15:38:08 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=250.44421848827872 records/second\n",
      "[05/22/2025 15:38:08 INFO 140473021069120] #progress_metric: host=algo-1, completed 7.0 % of epochs\n",
      "[05/22/2025 15:38:08 INFO 140473021069120] #quality_metric: host=algo-1, epoch=27, train loss <loss>=1.4462369030172175\n",
      "[05/22/2025 15:38:08 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:10 INFO 140473021069120] Epoch[28] Batch[0] avg_epoch_loss=1.273740\n",
      "[05/22/2025 15:38:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=1.2737401723861694\n",
      "[05/22/2025 15:38:11 INFO 140473021069120] Epoch[28] Batch[5] avg_epoch_loss=1.268748\n",
      "[05/22/2025 15:38:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=1.2687475482622783\n",
      "[05/22/2025 15:38:11 INFO 140473021069120] Epoch[28] Batch [5]#011Speed: 366.47 samples/sec#011loss=1.268748\n",
      "[05/22/2025 15:38:13 INFO 140473021069120] Epoch[28] Batch[10] avg_epoch_loss=1.084695\n",
      "[05/22/2025 15:38:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=0.8638315558433532\n",
      "[05/22/2025 15:38:13 INFO 140473021069120] Epoch[28] Batch [10]#011Speed: 356.91 samples/sec#011loss=0.863832\n",
      "[05/22/2025 15:38:13 INFO 140473021069120] processed a total of 1308 examples\n",
      "#metrics {\"StartTime\": 1747928288.5710661, \"EndTime\": 1747928293.6436543, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5072.25227355957, \"count\": 1, \"min\": 5072.25227355957, \"max\": 5072.25227355957}}}\n",
      "[05/22/2025 15:38:13 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=257.8690515942134 records/second\n",
      "[05/22/2025 15:38:13 INFO 140473021069120] #progress_metric: host=algo-1, completed 7.25 % of epochs\n",
      "[05/22/2025 15:38:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=28, train loss <loss>=1.0846948244354941\n",
      "[05/22/2025 15:38:13 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:38:13 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_31c7c91f-a26c-4bca-b685-29fe27913872-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928293.6437137, \"EndTime\": 1747928293.6806734, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 36.67593002319336, \"count\": 1, \"min\": 36.67593002319336, \"max\": 36.67593002319336}}}\n",
      "[05/22/2025 15:38:15 INFO 140473021069120] Epoch[29] Batch[0] avg_epoch_loss=1.299953\n",
      "[05/22/2025 15:38:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=1.2999529838562012\n",
      "[05/22/2025 15:38:16 INFO 140473021069120] Epoch[29] Batch[5] avg_epoch_loss=1.242529\n",
      "[05/22/2025 15:38:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=1.2425286372502644\n",
      "[05/22/2025 15:38:16 INFO 140473021069120] Epoch[29] Batch [5]#011Speed: 366.01 samples/sec#011loss=1.242529\n",
      "[05/22/2025 15:38:18 INFO 140473021069120] processed a total of 1229 examples\n",
      "#metrics {\"StartTime\": 1747928293.6807337, \"EndTime\": 1747928298.3622854, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4681.49733543396, \"count\": 1, \"min\": 4681.49733543396, \"max\": 4681.49733543396}}}\n",
      "[05/22/2025 15:38:18 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.517537464313 records/second\n",
      "[05/22/2025 15:38:18 INFO 140473021069120] #progress_metric: host=algo-1, completed 7.5 % of epochs\n",
      "[05/22/2025 15:38:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=29, train loss <loss>=1.1320313453674316\n",
      "[05/22/2025 15:38:18 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:19 INFO 140473021069120] Epoch[30] Batch[0] avg_epoch_loss=1.174572\n",
      "[05/22/2025 15:38:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=1.174572467803955\n",
      "[05/22/2025 15:38:21 INFO 140473021069120] Epoch[30] Batch[5] avg_epoch_loss=1.252588\n",
      "[05/22/2025 15:38:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=1.252587616443634\n",
      "[05/22/2025 15:38:21 INFO 140473021069120] Epoch[30] Batch [5]#011Speed: 369.71 samples/sec#011loss=1.252588\n",
      "[05/22/2025 15:38:23 INFO 140473021069120] Epoch[30] Batch[10] avg_epoch_loss=1.275732\n",
      "[05/22/2025 15:38:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=1.3035054683685303\n",
      "[05/22/2025 15:38:23 INFO 140473021069120] Epoch[30] Batch [10]#011Speed: 356.96 samples/sec#011loss=1.303505\n",
      "[05/22/2025 15:38:23 INFO 140473021069120] processed a total of 1289 examples\n",
      "#metrics {\"StartTime\": 1747928298.3623502, \"EndTime\": 1747928303.416834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5054.188013076782, \"count\": 1, \"min\": 5054.188013076782, \"max\": 5054.188013076782}}}\n",
      "[05/22/2025 15:38:23 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=255.03176216221826 records/second\n",
      "[05/22/2025 15:38:23 INFO 140473021069120] #progress_metric: host=algo-1, completed 7.75 % of epochs\n",
      "[05/22/2025 15:38:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=30, train loss <loss>=1.2757320945913142\n",
      "[05/22/2025 15:38:23 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:24 INFO 140473021069120] Epoch[31] Batch[0] avg_epoch_loss=1.207890\n",
      "[05/22/2025 15:38:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=1.2078903913497925\n",
      "[05/22/2025 15:38:26 INFO 140473021069120] Epoch[31] Batch[5] avg_epoch_loss=1.333879\n",
      "[05/22/2025 15:38:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=1.3338788549105327\n",
      "[05/22/2025 15:38:26 INFO 140473021069120] Epoch[31] Batch [5]#011Speed: 368.07 samples/sec#011loss=1.333879\n",
      "[05/22/2025 15:38:28 INFO 140473021069120] processed a total of 1231 examples\n",
      "#metrics {\"StartTime\": 1747928303.4168901, \"EndTime\": 1747928308.1323283, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4715.182304382324, \"count\": 1, \"min\": 4715.182304382324, \"max\": 4715.182304382324}}}\n",
      "[05/22/2025 15:38:28 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.0664514029851 records/second\n",
      "[05/22/2025 15:38:28 INFO 140473021069120] #progress_metric: host=algo-1, completed 8.0 % of epochs\n",
      "[05/22/2025 15:38:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=31, train loss <loss>=1.1629742577672004\n",
      "[05/22/2025 15:38:28 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:29 INFO 140473021069120] Epoch[32] Batch[0] avg_epoch_loss=1.031161\n",
      "[05/22/2025 15:38:29 INFO 140473021069120] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=1.0311607122421265\n",
      "[05/22/2025 15:38:31 INFO 140473021069120] Epoch[32] Batch[5] avg_epoch_loss=1.248890\n",
      "[05/22/2025 15:38:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=1.2488897442817688\n",
      "[05/22/2025 15:38:31 INFO 140473021069120] Epoch[32] Batch [5]#011Speed: 368.92 samples/sec#011loss=1.248890\n",
      "[05/22/2025 15:38:32 INFO 140473021069120] processed a total of 1271 examples\n",
      "#metrics {\"StartTime\": 1747928308.1323898, \"EndTime\": 1747928312.8238788, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4691.034555435181, \"count\": 1, \"min\": 4691.034555435181, \"max\": 4691.034555435181}}}\n",
      "[05/22/2025 15:38:32 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=270.9367474632717 records/second\n",
      "[05/22/2025 15:38:32 INFO 140473021069120] #progress_metric: host=algo-1, completed 8.25 % of epochs\n",
      "[05/22/2025 15:38:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=32, train loss <loss>=1.2289226174354553\n",
      "[05/22/2025 15:38:32 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:34 INFO 140473021069120] Epoch[33] Batch[0] avg_epoch_loss=1.282273\n",
      "[05/22/2025 15:38:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=1.2822729349136353\n",
      "[05/22/2025 15:38:36 INFO 140473021069120] Epoch[33] Batch[5] avg_epoch_loss=1.193190\n",
      "[05/22/2025 15:38:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=1.1931898196538289\n",
      "[05/22/2025 15:38:36 INFO 140473021069120] Epoch[33] Batch [5]#011Speed: 364.92 samples/sec#011loss=1.193190\n",
      "[05/22/2025 15:38:37 INFO 140473021069120] Epoch[33] Batch[10] avg_epoch_loss=1.362372\n",
      "[05/22/2025 15:38:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=1.5653903484344482\n",
      "[05/22/2025 15:38:37 INFO 140473021069120] Epoch[33] Batch [10]#011Speed: 352.47 samples/sec#011loss=1.565390\n",
      "[05/22/2025 15:38:37 INFO 140473021069120] processed a total of 1332 examples\n",
      "#metrics {\"StartTime\": 1747928312.8239439, \"EndTime\": 1747928317.9359179, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5111.6042137146, \"count\": 1, \"min\": 5111.6042137146, \"max\": 5111.6042137146}}}\n",
      "[05/22/2025 15:38:37 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.5789247324266 records/second\n",
      "[05/22/2025 15:38:37 INFO 140473021069120] #progress_metric: host=algo-1, completed 8.5 % of epochs\n",
      "[05/22/2025 15:38:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=33, train loss <loss>=1.3623718781904741\n",
      "[05/22/2025 15:38:37 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:39 INFO 140473021069120] Epoch[34] Batch[0] avg_epoch_loss=1.242535\n",
      "[05/22/2025 15:38:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=1.2425352334976196\n",
      "[05/22/2025 15:38:41 INFO 140473021069120] Epoch[34] Batch[5] avg_epoch_loss=1.248780\n",
      "[05/22/2025 15:38:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=1.2487802505493164\n",
      "[05/22/2025 15:38:41 INFO 140473021069120] Epoch[34] Batch [5]#011Speed: 363.86 samples/sec#011loss=1.248780\n",
      "[05/22/2025 15:38:43 INFO 140473021069120] Epoch[34] Batch[10] avg_epoch_loss=1.252027\n",
      "[05/22/2025 15:38:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=1.255923295021057\n",
      "[05/22/2025 15:38:43 INFO 140473021069120] Epoch[34] Batch [10]#011Speed: 358.74 samples/sec#011loss=1.255923\n",
      "[05/22/2025 15:38:43 INFO 140473021069120] processed a total of 1318 examples\n",
      "#metrics {\"StartTime\": 1747928317.9359782, \"EndTime\": 1747928323.014702, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5078.319311141968, \"count\": 1, \"min\": 5078.319311141968, \"max\": 5078.319311141968}}}\n",
      "[05/22/2025 15:38:43 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.53033458758375 records/second\n",
      "[05/22/2025 15:38:43 INFO 140473021069120] #progress_metric: host=algo-1, completed 8.75 % of epochs\n",
      "[05/22/2025 15:38:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=34, train loss <loss>=1.2520270889455622\n",
      "[05/22/2025 15:38:43 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:44 INFO 140473021069120] Epoch[35] Batch[0] avg_epoch_loss=1.288984\n",
      "[05/22/2025 15:38:44 INFO 140473021069120] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=1.288983702659607\n",
      "[05/22/2025 15:38:46 INFO 140473021069120] Epoch[35] Batch[5] avg_epoch_loss=1.264330\n",
      "[05/22/2025 15:38:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=1.2643296519915264\n",
      "[05/22/2025 15:38:46 INFO 140473021069120] Epoch[35] Batch [5]#011Speed: 363.59 samples/sec#011loss=1.264330\n",
      "[05/22/2025 15:38:48 INFO 140473021069120] Epoch[35] Batch[10] avg_epoch_loss=1.225451\n",
      "[05/22/2025 15:38:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=1.1787955522537232\n",
      "[05/22/2025 15:38:48 INFO 140473021069120] Epoch[35] Batch [10]#011Speed: 360.45 samples/sec#011loss=1.178796\n",
      "[05/22/2025 15:38:48 INFO 140473021069120] processed a total of 1284 examples\n",
      "#metrics {\"StartTime\": 1747928323.0147583, \"EndTime\": 1747928328.1159189, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5100.785732269287, \"count\": 1, \"min\": 5100.785732269287, \"max\": 5100.785732269287}}}\n",
      "[05/22/2025 15:38:48 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=251.72158208969873 records/second\n",
      "[05/22/2025 15:38:48 INFO 140473021069120] #progress_metric: host=algo-1, completed 9.0 % of epochs\n",
      "[05/22/2025 15:38:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=35, train loss <loss>=1.2254505157470703\n",
      "[05/22/2025 15:38:48 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:49 INFO 140473021069120] Epoch[36] Batch[0] avg_epoch_loss=1.268950\n",
      "[05/22/2025 15:38:49 INFO 140473021069120] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=1.2689496278762817\n",
      "[05/22/2025 15:38:51 INFO 140473021069120] Epoch[36] Batch[5] avg_epoch_loss=1.192496\n",
      "[05/22/2025 15:38:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=1.192495584487915\n",
      "[05/22/2025 15:38:51 INFO 140473021069120] Epoch[36] Batch [5]#011Speed: 371.12 samples/sec#011loss=1.192496\n",
      "[05/22/2025 15:38:53 INFO 140473021069120] Epoch[36] Batch[10] avg_epoch_loss=1.143372\n",
      "[05/22/2025 15:38:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=1.0844247341156006\n",
      "[05/22/2025 15:38:53 INFO 140473021069120] Epoch[36] Batch [10]#011Speed: 363.14 samples/sec#011loss=1.084425\n",
      "[05/22/2025 15:38:53 INFO 140473021069120] processed a total of 1284 examples\n",
      "#metrics {\"StartTime\": 1747928328.1159785, \"EndTime\": 1747928333.1542127, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5037.949800491333, \"count\": 1, \"min\": 5037.949800491333, \"max\": 5037.949800491333}}}\n",
      "[05/22/2025 15:38:53 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=254.86096098466544 records/second\n",
      "[05/22/2025 15:38:53 INFO 140473021069120] #progress_metric: host=algo-1, completed 9.25 % of epochs\n",
      "[05/22/2025 15:38:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=36, train loss <loss>=1.1433724706823176\n",
      "[05/22/2025 15:38:53 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:54 INFO 140473021069120] Epoch[37] Batch[0] avg_epoch_loss=1.379771\n",
      "[05/22/2025 15:38:54 INFO 140473021069120] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=1.3797712326049805\n",
      "[05/22/2025 15:38:56 INFO 140473021069120] Epoch[37] Batch[5] avg_epoch_loss=1.173560\n",
      "[05/22/2025 15:38:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=1.1735596458117168\n",
      "[05/22/2025 15:38:56 INFO 140473021069120] Epoch[37] Batch [5]#011Speed: 366.26 samples/sec#011loss=1.173560\n",
      "[05/22/2025 15:38:58 INFO 140473021069120] Epoch[37] Batch[10] avg_epoch_loss=1.203324\n",
      "[05/22/2025 15:38:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=1.2390402793884276\n",
      "[05/22/2025 15:38:58 INFO 140473021069120] Epoch[37] Batch [10]#011Speed: 350.38 samples/sec#011loss=1.239040\n",
      "[05/22/2025 15:38:58 INFO 140473021069120] processed a total of 1336 examples\n",
      "#metrics {\"StartTime\": 1747928333.15427, \"EndTime\": 1747928338.2476106, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5093.007564544678, \"count\": 1, \"min\": 5093.007564544678, \"max\": 5093.007564544678}}}\n",
      "[05/22/2025 15:38:58 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.3160221743469 records/second\n",
      "[05/22/2025 15:38:58 INFO 140473021069120] #progress_metric: host=algo-1, completed 9.5 % of epochs\n",
      "[05/22/2025 15:38:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=37, train loss <loss>=1.203323570164767\n",
      "[05/22/2025 15:38:58 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:38:59 INFO 140473021069120] Epoch[38] Batch[0] avg_epoch_loss=1.028567\n",
      "[05/22/2025 15:38:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=1.0285671949386597\n",
      "[05/22/2025 15:39:01 INFO 140473021069120] Epoch[38] Batch[5] avg_epoch_loss=1.205576\n",
      "[05/22/2025 15:39:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=1.2055763999621074\n",
      "[05/22/2025 15:39:01 INFO 140473021069120] Epoch[38] Batch [5]#011Speed: 365.76 samples/sec#011loss=1.205576\n",
      "[05/22/2025 15:39:02 INFO 140473021069120] processed a total of 1266 examples\n",
      "#metrics {\"StartTime\": 1747928338.2476695, \"EndTime\": 1747928342.99089, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4742.939710617065, \"count\": 1, \"min\": 4742.939710617065, \"max\": 4742.939710617065}}}\n",
      "[05/22/2025 15:39:02 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=266.91779095759046 records/second\n",
      "[05/22/2025 15:39:02 INFO 140473021069120] #progress_metric: host=algo-1, completed 9.75 % of epochs\n",
      "[05/22/2025 15:39:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=38, train loss <loss>=1.178564327955246\n",
      "[05/22/2025 15:39:02 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:04 INFO 140473021069120] Epoch[39] Batch[0] avg_epoch_loss=1.124811\n",
      "[05/22/2025 15:39:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=1.1248112916946411\n",
      "[05/22/2025 15:39:06 INFO 140473021069120] Epoch[39] Batch[5] avg_epoch_loss=1.141965\n",
      "[05/22/2025 15:39:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=1.1419645349184673\n",
      "[05/22/2025 15:39:06 INFO 140473021069120] Epoch[39] Batch [5]#011Speed: 382.71 samples/sec#011loss=1.141965\n",
      "[05/22/2025 15:39:07 INFO 140473021069120] Epoch[39] Batch[10] avg_epoch_loss=1.261089\n",
      "[05/22/2025 15:39:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=1.4040390968322753\n",
      "[05/22/2025 15:39:07 INFO 140473021069120] Epoch[39] Batch [10]#011Speed: 375.65 samples/sec#011loss=1.404039\n",
      "[05/22/2025 15:39:07 INFO 140473021069120] processed a total of 1290 examples\n",
      "#metrics {\"StartTime\": 1747928342.990953, \"EndTime\": 1747928347.9025414, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4911.2958908081055, \"count\": 1, \"min\": 4911.2958908081055, \"max\": 4911.2958908081055}}}\n",
      "[05/22/2025 15:39:07 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.65521051923344 records/second\n",
      "[05/22/2025 15:39:07 INFO 140473021069120] #progress_metric: host=algo-1, completed 10.0 % of epochs\n",
      "[05/22/2025 15:39:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=39, train loss <loss>=1.2610893357883801\n",
      "[05/22/2025 15:39:07 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:09 INFO 140473021069120] Epoch[40] Batch[0] avg_epoch_loss=1.192848\n",
      "[05/22/2025 15:39:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=1.1928480863571167\n",
      "[05/22/2025 15:39:11 INFO 140473021069120] Epoch[40] Batch[5] avg_epoch_loss=1.236631\n",
      "[05/22/2025 15:39:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=1.2366310358047485\n",
      "[05/22/2025 15:39:11 INFO 140473021069120] Epoch[40] Batch [5]#011Speed: 382.66 samples/sec#011loss=1.236631\n",
      "[05/22/2025 15:39:12 INFO 140473021069120] Epoch[40] Batch[10] avg_epoch_loss=1.194400\n",
      "[05/22/2025 15:39:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=1.1437236309051513\n",
      "[05/22/2025 15:39:12 INFO 140473021069120] Epoch[40] Batch [10]#011Speed: 377.25 samples/sec#011loss=1.143724\n",
      "[05/22/2025 15:39:12 INFO 140473021069120] processed a total of 1291 examples\n",
      "#metrics {\"StartTime\": 1747928347.9026003, \"EndTime\": 1747928352.8080661, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4905.213356018066, \"count\": 1, \"min\": 4905.213356018066, \"max\": 4905.213356018066}}}\n",
      "[05/22/2025 15:39:12 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.1836621059509 records/second\n",
      "[05/22/2025 15:39:12 INFO 140473021069120] #progress_metric: host=algo-1, completed 10.25 % of epochs\n",
      "[05/22/2025 15:39:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=40, train loss <loss>=1.1944003972140225\n",
      "[05/22/2025 15:39:12 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:14 INFO 140473021069120] Epoch[41] Batch[0] avg_epoch_loss=1.260808\n",
      "[05/22/2025 15:39:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=1.2608084678649902\n",
      "[05/22/2025 15:39:16 INFO 140473021069120] Epoch[41] Batch[5] avg_epoch_loss=1.190052\n",
      "[05/22/2025 15:39:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=1.1900515953699748\n",
      "[05/22/2025 15:39:16 INFO 140473021069120] Epoch[41] Batch [5]#011Speed: 375.87 samples/sec#011loss=1.190052\n",
      "[05/22/2025 15:39:17 INFO 140473021069120] Epoch[41] Batch[10] avg_epoch_loss=1.258932\n",
      "[05/22/2025 15:39:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=1.3415881872177124\n",
      "[05/22/2025 15:39:17 INFO 140473021069120] Epoch[41] Batch [10]#011Speed: 372.99 samples/sec#011loss=1.341588\n",
      "[05/22/2025 15:39:17 INFO 140473021069120] processed a total of 1323 examples\n",
      "#metrics {\"StartTime\": 1747928352.8081446, \"EndTime\": 1747928357.7437115, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4935.297727584839, \"count\": 1, \"min\": 4935.297727584839, \"max\": 4935.297727584839}}}\n",
      "[05/22/2025 15:39:17 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.06484166652376 records/second\n",
      "[05/22/2025 15:39:17 INFO 140473021069120] #progress_metric: host=algo-1, completed 10.5 % of epochs\n",
      "[05/22/2025 15:39:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=41, train loss <loss>=1.2589318643916736\n",
      "[05/22/2025 15:39:17 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:19 INFO 140473021069120] Epoch[42] Batch[0] avg_epoch_loss=1.033544\n",
      "[05/22/2025 15:39:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=1.0335443019866943\n",
      "[05/22/2025 15:39:20 INFO 140473021069120] Epoch[42] Batch[5] avg_epoch_loss=1.230901\n",
      "[05/22/2025 15:39:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=1.2309008638064067\n",
      "[05/22/2025 15:39:20 INFO 140473021069120] Epoch[42] Batch [5]#011Speed: 382.79 samples/sec#011loss=1.230901\n",
      "[05/22/2025 15:39:22 INFO 140473021069120] processed a total of 1258 examples\n",
      "#metrics {\"StartTime\": 1747928357.7437577, \"EndTime\": 1747928362.2947612, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4550.769567489624, \"count\": 1, \"min\": 4550.769567489624, \"max\": 4550.769567489624}}}\n",
      "[05/22/2025 15:39:22 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=276.43199641233804 records/second\n",
      "[05/22/2025 15:39:22 INFO 140473021069120] #progress_metric: host=algo-1, completed 10.75 % of epochs\n",
      "[05/22/2025 15:39:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=42, train loss <loss>=1.1884396195411682\n",
      "[05/22/2025 15:39:22 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:23 INFO 140473021069120] Epoch[43] Batch[0] avg_epoch_loss=1.127590\n",
      "[05/22/2025 15:39:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=1.1275898218154907\n",
      "[05/22/2025 15:39:25 INFO 140473021069120] Epoch[43] Batch[5] avg_epoch_loss=1.194936\n",
      "[05/22/2025 15:39:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=1.1949355999628704\n",
      "[05/22/2025 15:39:25 INFO 140473021069120] Epoch[43] Batch [5]#011Speed: 384.60 samples/sec#011loss=1.194936\n",
      "[05/22/2025 15:39:27 INFO 140473021069120] Epoch[43] Batch[10] avg_epoch_loss=1.180963\n",
      "[05/22/2025 15:39:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=1.1641953587532043\n",
      "[05/22/2025 15:39:27 INFO 140473021069120] Epoch[43] Batch [10]#011Speed: 366.84 samples/sec#011loss=1.164195\n",
      "[05/22/2025 15:39:27 INFO 140473021069120] processed a total of 1287 examples\n",
      "#metrics {\"StartTime\": 1747928362.2948148, \"EndTime\": 1747928367.242569, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4947.422504425049, \"count\": 1, \"min\": 4947.422504425049, \"max\": 4947.422504425049}}}\n",
      "[05/22/2025 15:39:27 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.13099138128365 records/second\n",
      "[05/22/2025 15:39:27 INFO 140473021069120] #progress_metric: host=algo-1, completed 11.0 % of epochs\n",
      "[05/22/2025 15:39:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=43, train loss <loss>=1.1809627630493857\n",
      "[05/22/2025 15:39:27 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:28 INFO 140473021069120] Epoch[44] Batch[0] avg_epoch_loss=1.120319\n",
      "[05/22/2025 15:39:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=1.1203190088272095\n",
      "[05/22/2025 15:39:30 INFO 140473021069120] Epoch[44] Batch[5] avg_epoch_loss=1.155521\n",
      "[05/22/2025 15:39:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=1.1555211345354717\n",
      "[05/22/2025 15:39:30 INFO 140473021069120] Epoch[44] Batch [5]#011Speed: 380.94 samples/sec#011loss=1.155521\n",
      "[05/22/2025 15:39:32 INFO 140473021069120] Epoch[44] Batch[10] avg_epoch_loss=1.226980\n",
      "[05/22/2025 15:39:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=1.3127297163009644\n",
      "[05/22/2025 15:39:32 INFO 140473021069120] Epoch[44] Batch [10]#011Speed: 368.47 samples/sec#011loss=1.312730\n",
      "[05/22/2025 15:39:32 INFO 140473021069120] processed a total of 1305 examples\n",
      "#metrics {\"StartTime\": 1747928367.242627, \"EndTime\": 1747928372.1759424, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4932.913303375244, \"count\": 1, \"min\": 4932.913303375244, \"max\": 4932.913303375244}}}\n",
      "[05/22/2025 15:39:32 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=264.54480747670345 records/second\n",
      "[05/22/2025 15:39:32 INFO 140473021069120] #progress_metric: host=algo-1, completed 11.25 % of epochs\n",
      "[05/22/2025 15:39:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=44, train loss <loss>=1.2269795807925137\n",
      "[05/22/2025 15:39:32 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:33 INFO 140473021069120] Epoch[45] Batch[0] avg_epoch_loss=1.223879\n",
      "[05/22/2025 15:39:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=1.223879337310791\n",
      "[05/22/2025 15:39:35 INFO 140473021069120] Epoch[45] Batch[5] avg_epoch_loss=1.191763\n",
      "[05/22/2025 15:39:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=1.191762884457906\n",
      "[05/22/2025 15:39:35 INFO 140473021069120] Epoch[45] Batch [5]#011Speed: 384.90 samples/sec#011loss=1.191763\n",
      "[05/22/2025 15:39:36 INFO 140473021069120] processed a total of 1253 examples\n",
      "#metrics {\"StartTime\": 1747928372.1760027, \"EndTime\": 1747928376.7524886, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4576.195478439331, \"count\": 1, \"min\": 4576.195478439331, \"max\": 4576.195478439331}}}\n",
      "[05/22/2025 15:39:36 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=273.8027984922219 records/second\n",
      "[05/22/2025 15:39:36 INFO 140473021069120] #progress_metric: host=algo-1, completed 11.5 % of epochs\n",
      "[05/22/2025 15:39:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=45, train loss <loss>=1.16590359210968\n",
      "[05/22/2025 15:39:36 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:38 INFO 140473021069120] Epoch[46] Batch[0] avg_epoch_loss=1.084192\n",
      "[05/22/2025 15:39:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=1.0841917991638184\n",
      "[05/22/2025 15:39:39 INFO 140473021069120] Epoch[46] Batch[5] avg_epoch_loss=1.096066\n",
      "[05/22/2025 15:39:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=1.0960660179456074\n",
      "[05/22/2025 15:39:39 INFO 140473021069120] Epoch[46] Batch [5]#011Speed: 371.01 samples/sec#011loss=1.096066\n",
      "[05/22/2025 15:39:41 INFO 140473021069120] Epoch[46] Batch[10] avg_epoch_loss=1.131792\n",
      "[05/22/2025 15:39:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=1.1746636867523192\n",
      "[05/22/2025 15:39:41 INFO 140473021069120] Epoch[46] Batch [10]#011Speed: 339.69 samples/sec#011loss=1.174664\n",
      "[05/22/2025 15:39:41 INFO 140473021069120] processed a total of 1331 examples\n",
      "#metrics {\"StartTime\": 1747928376.7525494, \"EndTime\": 1747928381.8761287, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5123.239755630493, \"count\": 1, \"min\": 5123.239755630493, \"max\": 5123.239755630493}}}\n",
      "[05/22/2025 15:39:41 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.79204996910715 records/second\n",
      "[05/22/2025 15:39:41 INFO 140473021069120] #progress_metric: host=algo-1, completed 11.75 % of epochs\n",
      "[05/22/2025 15:39:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=46, train loss <loss>=1.1317922310395674\n",
      "[05/22/2025 15:39:41 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:43 INFO 140473021069120] Epoch[47] Batch[0] avg_epoch_loss=1.142429\n",
      "[05/22/2025 15:39:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=1.142429232597351\n",
      "[05/22/2025 15:39:45 INFO 140473021069120] Epoch[47] Batch[5] avg_epoch_loss=1.203228\n",
      "[05/22/2025 15:39:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=1.203227976957957\n",
      "[05/22/2025 15:39:45 INFO 140473021069120] Epoch[47] Batch [5]#011Speed: 365.42 samples/sec#011loss=1.203228\n",
      "[05/22/2025 15:39:46 INFO 140473021069120] Epoch[47] Batch[10] avg_epoch_loss=1.261097\n",
      "[05/22/2025 15:39:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=1.330539882183075\n",
      "[05/22/2025 15:39:46 INFO 140473021069120] Epoch[47] Batch [10]#011Speed: 357.95 samples/sec#011loss=1.330540\n",
      "[05/22/2025 15:39:46 INFO 140473021069120] processed a total of 1291 examples\n",
      "#metrics {\"StartTime\": 1747928381.876189, \"EndTime\": 1747928386.9513302, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5074.880123138428, \"count\": 1, \"min\": 5074.880123138428, \"max\": 5074.880123138428}}}\n",
      "[05/22/2025 15:39:46 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=254.38594302814766 records/second\n",
      "[05/22/2025 15:39:46 INFO 140473021069120] #progress_metric: host=algo-1, completed 12.0 % of epochs\n",
      "[05/22/2025 15:39:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=47, train loss <loss>=1.2610970247875561\n",
      "[05/22/2025 15:39:46 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:48 INFO 140473021069120] Epoch[48] Batch[0] avg_epoch_loss=0.941543\n",
      "[05/22/2025 15:39:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=0.941543459892273\n",
      "[05/22/2025 15:39:50 INFO 140473021069120] Epoch[48] Batch[5] avg_epoch_loss=1.154266\n",
      "[05/22/2025 15:39:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=1.1542660593986511\n",
      "[05/22/2025 15:39:50 INFO 140473021069120] Epoch[48] Batch [5]#011Speed: 367.38 samples/sec#011loss=1.154266\n",
      "[05/22/2025 15:39:52 INFO 140473021069120] Epoch[48] Batch[10] avg_epoch_loss=1.146320\n",
      "[05/22/2025 15:39:52 INFO 140473021069120] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=1.1367855429649354\n",
      "[05/22/2025 15:39:52 INFO 140473021069120] Epoch[48] Batch [10]#011Speed: 352.78 samples/sec#011loss=1.136786\n",
      "[05/22/2025 15:39:52 INFO 140473021069120] processed a total of 1309 examples\n",
      "#metrics {\"StartTime\": 1747928386.951389, \"EndTime\": 1747928392.0416152, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5089.976072311401, \"count\": 1, \"min\": 5089.976072311401, \"max\": 5089.976072311401}}}\n",
      "[05/22/2025 15:39:52 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=257.16790419601466 records/second\n",
      "[05/22/2025 15:39:52 INFO 140473021069120] #progress_metric: host=algo-1, completed 12.25 % of epochs\n",
      "[05/22/2025 15:39:52 INFO 140473021069120] #quality_metric: host=algo-1, epoch=48, train loss <loss>=1.1463203701105984\n",
      "[05/22/2025 15:39:52 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:53 INFO 140473021069120] Epoch[49] Batch[0] avg_epoch_loss=1.085177\n",
      "[05/22/2025 15:39:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=1.085176944732666\n",
      "[05/22/2025 15:39:55 INFO 140473021069120] Epoch[49] Batch[5] avg_epoch_loss=1.131994\n",
      "[05/22/2025 15:39:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=1.1319943070411682\n",
      "[05/22/2025 15:39:55 INFO 140473021069120] Epoch[49] Batch [5]#011Speed: 364.49 samples/sec#011loss=1.131994\n",
      "[05/22/2025 15:39:57 INFO 140473021069120] Epoch[49] Batch[10] avg_epoch_loss=1.144571\n",
      "[05/22/2025 15:39:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=1.1596637487411499\n",
      "[05/22/2025 15:39:57 INFO 140473021069120] Epoch[49] Batch [10]#011Speed: 349.72 samples/sec#011loss=1.159664\n",
      "[05/22/2025 15:39:57 INFO 140473021069120] processed a total of 1339 examples\n",
      "#metrics {\"StartTime\": 1747928392.0416749, \"EndTime\": 1747928397.1404061, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5098.48952293396, \"count\": 1, \"min\": 5098.48952293396, \"max\": 5098.48952293396}}}\n",
      "[05/22/2025 15:39:57 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.62201276081066 records/second\n",
      "[05/22/2025 15:39:57 INFO 140473021069120] #progress_metric: host=algo-1, completed 12.5 % of epochs\n",
      "[05/22/2025 15:39:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=49, train loss <loss>=1.1445713259957053\n",
      "[05/22/2025 15:39:57 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:39:58 INFO 140473021069120] Epoch[50] Batch[0] avg_epoch_loss=1.213834\n",
      "[05/22/2025 15:39:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=1.2138336896896362\n",
      "[05/22/2025 15:40:00 INFO 140473021069120] Epoch[50] Batch[5] avg_epoch_loss=1.156766\n",
      "[05/22/2025 15:40:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=1.1567663550376892\n",
      "[05/22/2025 15:40:00 INFO 140473021069120] Epoch[50] Batch [5]#011Speed: 367.50 samples/sec#011loss=1.156766\n",
      "[05/22/2025 15:40:02 INFO 140473021069120] Epoch[50] Batch[10] avg_epoch_loss=1.193471\n",
      "[05/22/2025 15:40:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=1.2375170469284058\n",
      "[05/22/2025 15:40:02 INFO 140473021069120] Epoch[50] Batch [10]#011Speed: 346.74 samples/sec#011loss=1.237517\n",
      "[05/22/2025 15:40:02 INFO 140473021069120] processed a total of 1308 examples\n",
      "#metrics {\"StartTime\": 1747928397.1404693, \"EndTime\": 1747928402.2634118, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5122.6255893707275, \"count\": 1, \"min\": 5122.6255893707275, \"max\": 5122.6255893707275}}}\n",
      "[05/22/2025 15:40:02 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=255.33349629055135 records/second\n",
      "[05/22/2025 15:40:02 INFO 140473021069120] #progress_metric: host=algo-1, completed 12.75 % of epochs\n",
      "[05/22/2025 15:40:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=50, train loss <loss>=1.193471214988015\n",
      "[05/22/2025 15:40:02 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:03 INFO 140473021069120] Epoch[51] Batch[0] avg_epoch_loss=0.887713\n",
      "[05/22/2025 15:40:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.8877127766609192\n",
      "[05/22/2025 15:40:05 INFO 140473021069120] Epoch[51] Batch[5] avg_epoch_loss=1.173602\n",
      "[05/22/2025 15:40:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=1.1736018359661102\n",
      "[05/22/2025 15:40:05 INFO 140473021069120] Epoch[51] Batch [5]#011Speed: 361.67 samples/sec#011loss=1.173602\n",
      "[05/22/2025 15:40:07 INFO 140473021069120] processed a total of 1268 examples\n",
      "#metrics {\"StartTime\": 1747928402.2634702, \"EndTime\": 1747928407.0125313, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4748.753070831299, \"count\": 1, \"min\": 4748.753070831299, \"max\": 4748.753070831299}}}\n",
      "[05/22/2025 15:40:07 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=267.0116186493224 records/second\n",
      "[05/22/2025 15:40:07 INFO 140473021069120] #progress_metric: host=algo-1, completed 13.0 % of epochs\n",
      "[05/22/2025 15:40:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=51, train loss <loss>=1.186758691072464\n",
      "[05/22/2025 15:40:07 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:08 INFO 140473021069120] Epoch[52] Batch[0] avg_epoch_loss=1.341136\n",
      "[05/22/2025 15:40:08 INFO 140473021069120] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=1.3411355018615723\n",
      "[05/22/2025 15:40:10 INFO 140473021069120] Epoch[52] Batch[5] avg_epoch_loss=1.187064\n",
      "[05/22/2025 15:40:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=1.1870640118916829\n",
      "[05/22/2025 15:40:10 INFO 140473021069120] Epoch[52] Batch [5]#011Speed: 361.30 samples/sec#011loss=1.187064\n",
      "[05/22/2025 15:40:12 INFO 140473021069120] Epoch[52] Batch[10] avg_epoch_loss=1.013263\n",
      "[05/22/2025 15:40:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=0.8047026515007019\n",
      "[05/22/2025 15:40:12 INFO 140473021069120] Epoch[52] Batch [10]#011Speed: 346.38 samples/sec#011loss=0.804703\n",
      "[05/22/2025 15:40:12 INFO 140473021069120] processed a total of 1329 examples\n",
      "#metrics {\"StartTime\": 1747928407.0126028, \"EndTime\": 1747928412.1557987, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5142.807483673096, \"count\": 1, \"min\": 5142.807483673096, \"max\": 5142.807483673096}}}\n",
      "[05/22/2025 15:40:12 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=258.41477726622355 records/second\n",
      "[05/22/2025 15:40:12 INFO 140473021069120] #progress_metric: host=algo-1, completed 13.25 % of epochs\n",
      "[05/22/2025 15:40:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=52, train loss <loss>=1.013263393532146\n",
      "[05/22/2025 15:40:12 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:40:12 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_8eab8889-6faf-45fe-9509-9a86c6b628aa-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928412.1558573, \"EndTime\": 1747928412.1924915, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 36.34071350097656, \"count\": 1, \"min\": 36.34071350097656, \"max\": 36.34071350097656}}}\n",
      "[05/22/2025 15:40:13 INFO 140473021069120] Epoch[53] Batch[0] avg_epoch_loss=1.278192\n",
      "[05/22/2025 15:40:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=1.2781920433044434\n",
      "[05/22/2025 15:40:15 INFO 140473021069120] Epoch[53] Batch[5] avg_epoch_loss=1.213656\n",
      "[05/22/2025 15:40:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=1.2136558492978413\n",
      "[05/22/2025 15:40:15 INFO 140473021069120] Epoch[53] Batch [5]#011Speed: 362.79 samples/sec#011loss=1.213656\n",
      "[05/22/2025 15:40:16 INFO 140473021069120] processed a total of 1235 examples\n",
      "#metrics {\"StartTime\": 1747928412.1925502, \"EndTime\": 1747928416.9241633, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4731.558561325073, \"count\": 1, \"min\": 4731.558561325073, \"max\": 4731.558561325073}}}\n",
      "[05/22/2025 15:40:16 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.0085993837435 records/second\n",
      "[05/22/2025 15:40:16 INFO 140473021069120] #progress_metric: host=algo-1, completed 13.5 % of epochs\n",
      "[05/22/2025 15:40:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=53, train loss <loss>=1.233583188056946\n",
      "[05/22/2025 15:40:16 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:18 INFO 140473021069120] Epoch[54] Batch[0] avg_epoch_loss=1.249720\n",
      "[05/22/2025 15:40:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=1.2497203350067139\n",
      "[05/22/2025 15:40:20 INFO 140473021069120] Epoch[54] Batch[5] avg_epoch_loss=1.153875\n",
      "[05/22/2025 15:40:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=1.1538745264212291\n",
      "[05/22/2025 15:40:20 INFO 140473021069120] Epoch[54] Batch [5]#011Speed: 371.07 samples/sec#011loss=1.153875\n",
      "[05/22/2025 15:40:21 INFO 140473021069120] Epoch[54] Batch[10] avg_epoch_loss=1.158367\n",
      "[05/22/2025 15:40:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=1.1637582540512086\n",
      "[05/22/2025 15:40:21 INFO 140473021069120] Epoch[54] Batch [10]#011Speed: 358.11 samples/sec#011loss=1.163758\n",
      "[05/22/2025 15:40:21 INFO 140473021069120] processed a total of 1311 examples\n",
      "#metrics {\"StartTime\": 1747928416.9242203, \"EndTime\": 1747928421.9834325, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5058.822393417358, \"count\": 1, \"min\": 5058.822393417358, \"max\": 5058.822393417358}}}\n",
      "[05/22/2025 15:40:21 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.1468119480079 records/second\n",
      "[05/22/2025 15:40:21 INFO 140473021069120] #progress_metric: host=algo-1, completed 13.75 % of epochs\n",
      "[05/22/2025 15:40:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=54, train loss <loss>=1.1583671298894016\n",
      "[05/22/2025 15:40:21 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:23 INFO 140473021069120] Epoch[55] Batch[0] avg_epoch_loss=1.370382\n",
      "[05/22/2025 15:40:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=1.3703820705413818\n",
      "[05/22/2025 15:40:25 INFO 140473021069120] Epoch[55] Batch[5] avg_epoch_loss=1.204919\n",
      "[05/22/2025 15:40:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=1.2049190004666646\n",
      "[05/22/2025 15:40:25 INFO 140473021069120] Epoch[55] Batch [5]#011Speed: 365.66 samples/sec#011loss=1.204919\n",
      "[05/22/2025 15:40:26 INFO 140473021069120] processed a total of 1228 examples\n",
      "#metrics {\"StartTime\": 1747928421.9834902, \"EndTime\": 1747928426.718153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4734.404563903809, \"count\": 1, \"min\": 4734.404563903809, \"max\": 4734.404563903809}}}\n",
      "[05/22/2025 15:40:26 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.3727719601735 records/second\n",
      "[05/22/2025 15:40:26 INFO 140473021069120] #progress_metric: host=algo-1, completed 14.0 % of epochs\n",
      "[05/22/2025 15:40:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=55, train loss <loss>=1.1754234731197357\n",
      "[05/22/2025 15:40:26 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:28 INFO 140473021069120] Epoch[56] Batch[0] avg_epoch_loss=1.130870\n",
      "[05/22/2025 15:40:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=1.1308704614639282\n",
      "[05/22/2025 15:40:30 INFO 140473021069120] Epoch[56] Batch[5] avg_epoch_loss=1.248215\n",
      "[05/22/2025 15:40:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=1.2482154766718547\n",
      "[05/22/2025 15:40:30 INFO 140473021069120] Epoch[56] Batch [5]#011Speed: 363.67 samples/sec#011loss=1.248215\n",
      "[05/22/2025 15:40:31 INFO 140473021069120] Epoch[56] Batch[10] avg_epoch_loss=1.308934\n",
      "[05/22/2025 15:40:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=1.3817955255508423\n",
      "[05/22/2025 15:40:31 INFO 140473021069120] Epoch[56] Batch [10]#011Speed: 356.62 samples/sec#011loss=1.381796\n",
      "[05/22/2025 15:40:31 INFO 140473021069120] processed a total of 1306 examples\n",
      "#metrics {\"StartTime\": 1747928426.7182167, \"EndTime\": 1747928431.8086708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5090.162992477417, \"count\": 1, \"min\": 5090.162992477417, \"max\": 5090.162992477417}}}\n",
      "[05/22/2025 15:40:31 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.56900213348445 records/second\n",
      "[05/22/2025 15:40:31 INFO 140473021069120] #progress_metric: host=algo-1, completed 14.25 % of epochs\n",
      "[05/22/2025 15:40:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=56, train loss <loss>=1.308933680707758\n",
      "[05/22/2025 15:40:31 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:33 INFO 140473021069120] Epoch[57] Batch[0] avg_epoch_loss=1.040512\n",
      "[05/22/2025 15:40:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=1.0405116081237793\n",
      "[05/22/2025 15:40:35 INFO 140473021069120] Epoch[57] Batch[5] avg_epoch_loss=1.106106\n",
      "[05/22/2025 15:40:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=1.1061058640480042\n",
      "[05/22/2025 15:40:35 INFO 140473021069120] Epoch[57] Batch [5]#011Speed: 364.33 samples/sec#011loss=1.106106\n",
      "[05/22/2025 15:40:36 INFO 140473021069120] Epoch[57] Batch[10] avg_epoch_loss=1.107003\n",
      "[05/22/2025 15:40:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=1.108079981803894\n",
      "[05/22/2025 15:40:36 INFO 140473021069120] Epoch[57] Batch [10]#011Speed: 348.42 samples/sec#011loss=1.108080\n",
      "[05/22/2025 15:40:36 INFO 140473021069120] processed a total of 1339 examples\n",
      "#metrics {\"StartTime\": 1747928431.8087294, \"EndTime\": 1747928436.9195807, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5110.5797290802, \"count\": 1, \"min\": 5110.5797290802, \"max\": 5110.5797290802}}}\n",
      "[05/22/2025 15:40:36 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.00081919384235 records/second\n",
      "[05/22/2025 15:40:36 INFO 140473021069120] #progress_metric: host=algo-1, completed 14.5 % of epochs\n",
      "[05/22/2025 15:40:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=57, train loss <loss>=1.1070031903006814\n",
      "[05/22/2025 15:40:36 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:38 INFO 140473021069120] Epoch[58] Batch[0] avg_epoch_loss=1.034258\n",
      "[05/22/2025 15:40:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=1.0342581272125244\n",
      "[05/22/2025 15:40:40 INFO 140473021069120] Epoch[58] Batch[5] avg_epoch_loss=1.057185\n",
      "[05/22/2025 15:40:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=1.0571852028369904\n",
      "[05/22/2025 15:40:40 INFO 140473021069120] Epoch[58] Batch [5]#011Speed: 365.06 samples/sec#011loss=1.057185\n",
      "[05/22/2025 15:40:42 INFO 140473021069120] Epoch[58] Batch[10] avg_epoch_loss=1.152053\n",
      "[05/22/2025 15:40:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=1.2658948183059693\n",
      "[05/22/2025 15:40:42 INFO 140473021069120] Epoch[58] Batch [10]#011Speed: 359.78 samples/sec#011loss=1.265895\n",
      "[05/22/2025 15:40:42 INFO 140473021069120] processed a total of 1289 examples\n",
      "#metrics {\"StartTime\": 1747928436.9196424, \"EndTime\": 1747928442.0026062, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5082.687854766846, \"count\": 1, \"min\": 5082.687854766846, \"max\": 5082.687854766846}}}\n",
      "[05/22/2025 15:40:42 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.60184532957825 records/second\n",
      "[05/22/2025 15:40:42 INFO 140473021069120] #progress_metric: host=algo-1, completed 14.75 % of epochs\n",
      "[05/22/2025 15:40:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=58, train loss <loss>=1.1520532098683445\n",
      "[05/22/2025 15:40:42 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:43 INFO 140473021069120] Epoch[59] Batch[0] avg_epoch_loss=1.179569\n",
      "[05/22/2025 15:40:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=1.1795690059661865\n",
      "[05/22/2025 15:40:45 INFO 140473021069120] Epoch[59] Batch[5] avg_epoch_loss=1.192276\n",
      "[05/22/2025 15:40:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=1.1922758022944133\n",
      "[05/22/2025 15:40:45 INFO 140473021069120] Epoch[59] Batch [5]#011Speed: 363.34 samples/sec#011loss=1.192276\n",
      "[05/22/2025 15:40:46 INFO 140473021069120] processed a total of 1278 examples\n",
      "#metrics {\"StartTime\": 1747928442.0026622, \"EndTime\": 1747928446.7402606, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4737.344264984131, \"count\": 1, \"min\": 4737.344264984131, \"max\": 4737.344264984131}}}\n",
      "[05/22/2025 15:40:46 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.7659976266038 records/second\n",
      "[05/22/2025 15:40:46 INFO 140473021069120] #progress_metric: host=algo-1, completed 15.0 % of epochs\n",
      "[05/22/2025 15:40:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=59, train loss <loss>=1.1356324195861816\n",
      "[05/22/2025 15:40:46 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:48 INFO 140473021069120] Epoch[60] Batch[0] avg_epoch_loss=1.198143\n",
      "[05/22/2025 15:40:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=1.1981432437896729\n",
      "[05/22/2025 15:40:50 INFO 140473021069120] Epoch[60] Batch[5] avg_epoch_loss=1.140287\n",
      "[05/22/2025 15:40:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=1.1402873198191326\n",
      "[05/22/2025 15:40:50 INFO 140473021069120] Epoch[60] Batch [5]#011Speed: 364.23 samples/sec#011loss=1.140287\n",
      "[05/22/2025 15:40:51 INFO 140473021069120] processed a total of 1227 examples\n",
      "#metrics {\"StartTime\": 1747928446.7403245, \"EndTime\": 1747928451.4609451, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4720.314502716064, \"count\": 1, \"min\": 4720.314502716064, \"max\": 4720.314502716064}}}\n",
      "[05/22/2025 15:40:51 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.9350945517945 records/second\n",
      "[05/22/2025 15:40:51 INFO 140473021069120] #progress_metric: host=algo-1, completed 15.25 % of epochs\n",
      "[05/22/2025 15:40:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=60, train loss <loss>=1.1396251976490022\n",
      "[05/22/2025 15:40:51 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:53 INFO 140473021069120] Epoch[61] Batch[0] avg_epoch_loss=1.115897\n",
      "[05/22/2025 15:40:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=1.1158971786499023\n",
      "[05/22/2025 15:40:54 INFO 140473021069120] Epoch[61] Batch[5] avg_epoch_loss=1.195431\n",
      "[05/22/2025 15:40:54 INFO 140473021069120] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=1.1954314510027568\n",
      "[05/22/2025 15:40:54 INFO 140473021069120] Epoch[61] Batch [5]#011Speed: 367.47 samples/sec#011loss=1.195431\n",
      "[05/22/2025 15:40:56 INFO 140473021069120] processed a total of 1270 examples\n",
      "#metrics {\"StartTime\": 1747928451.4610088, \"EndTime\": 1747928456.1573877, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4696.0625648498535, \"count\": 1, \"min\": 4696.0625648498535, \"max\": 4696.0625648498535}}}\n",
      "[05/22/2025 15:40:56 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=270.43321729798697 records/second\n",
      "[05/22/2025 15:40:56 INFO 140473021069120] #progress_metric: host=algo-1, completed 15.5 % of epochs\n",
      "[05/22/2025 15:40:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=61, train loss <loss>=1.1509206891059875\n",
      "[05/22/2025 15:40:56 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:40:57 INFO 140473021069120] Epoch[62] Batch[0] avg_epoch_loss=1.380199\n",
      "[05/22/2025 15:40:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=1.3801991939544678\n",
      "[05/22/2025 15:40:59 INFO 140473021069120] Epoch[62] Batch[5] avg_epoch_loss=1.123009\n",
      "[05/22/2025 15:40:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=1.1230091949303944\n",
      "[05/22/2025 15:40:59 INFO 140473021069120] Epoch[62] Batch [5]#011Speed: 365.68 samples/sec#011loss=1.123009\n",
      "[05/22/2025 15:41:01 INFO 140473021069120] Epoch[62] Batch[10] avg_epoch_loss=0.969387\n",
      "[05/22/2025 15:41:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=0.7850394129753113\n",
      "[05/22/2025 15:41:01 INFO 140473021069120] Epoch[62] Batch [10]#011Speed: 342.37 samples/sec#011loss=0.785039\n",
      "[05/22/2025 15:41:01 INFO 140473021069120] processed a total of 1307 examples\n",
      "#metrics {\"StartTime\": 1747928456.1574643, \"EndTime\": 1747928461.3186543, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5160.869121551514, \"count\": 1, \"min\": 5160.869121551514, \"max\": 5160.869121551514}}}\n",
      "[05/22/2025 15:41:01 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.24756514017474 records/second\n",
      "[05/22/2025 15:41:01 INFO 140473021069120] #progress_metric: host=algo-1, completed 15.75 % of epochs\n",
      "[05/22/2025 15:41:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.969386566768993\n",
      "[05/22/2025 15:41:01 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:41:01 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_a2c18d6a-a987-49e2-bbbd-1e1a0f29abf1-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928461.3187141, \"EndTime\": 1747928461.3570094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 37.49442100524902, \"count\": 1, \"min\": 37.49442100524902, \"max\": 37.49442100524902}}}\n",
      "[05/22/2025 15:41:02 INFO 140473021069120] Epoch[63] Batch[0] avg_epoch_loss=1.295915\n",
      "[05/22/2025 15:41:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=1.295914649963379\n",
      "[05/22/2025 15:41:04 INFO 140473021069120] Epoch[63] Batch[5] avg_epoch_loss=1.191923\n",
      "[05/22/2025 15:41:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=1.1919226050376892\n",
      "[05/22/2025 15:41:04 INFO 140473021069120] Epoch[63] Batch [5]#011Speed: 367.24 samples/sec#011loss=1.191923\n",
      "[05/22/2025 15:41:06 INFO 140473021069120] processed a total of 1253 examples\n",
      "#metrics {\"StartTime\": 1747928461.3570716, \"EndTime\": 1747928466.0890894, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4731.959581375122, \"count\": 1, \"min\": 4731.959581375122, \"max\": 4731.959581375122}}}\n",
      "[05/22/2025 15:41:06 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=264.79043831477924 records/second\n",
      "[05/22/2025 15:41:06 INFO 140473021069120] #progress_metric: host=algo-1, completed 16.0 % of epochs\n",
      "[05/22/2025 15:41:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=63, train loss <loss>=1.215284776687622\n",
      "[05/22/2025 15:41:06 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:07 INFO 140473021069120] Epoch[64] Batch[0] avg_epoch_loss=1.045707\n",
      "[05/22/2025 15:41:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=1.0457067489624023\n",
      "[05/22/2025 15:41:09 INFO 140473021069120] Epoch[64] Batch[5] avg_epoch_loss=1.156032\n",
      "[05/22/2025 15:41:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=1.1560320655504863\n",
      "[05/22/2025 15:41:09 INFO 140473021069120] Epoch[64] Batch [5]#011Speed: 364.69 samples/sec#011loss=1.156032\n",
      "[05/22/2025 15:41:10 INFO 140473021069120] processed a total of 1265 examples\n",
      "#metrics {\"StartTime\": 1747928466.0891438, \"EndTime\": 1747928470.80794, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4718.522310256958, \"count\": 1, \"min\": 4718.522310256958, \"max\": 4718.522310256958}}}\n",
      "[05/22/2025 15:41:10 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.0869742985283 records/second\n",
      "[05/22/2025 15:41:10 INFO 140473021069120] #progress_metric: host=algo-1, completed 16.25 % of epochs\n",
      "[05/22/2025 15:41:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=64, train loss <loss>=1.1669796347618102\n",
      "[05/22/2025 15:41:10 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:12 INFO 140473021069120] Epoch[65] Batch[0] avg_epoch_loss=1.113003\n",
      "[05/22/2025 15:41:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=1.1130033731460571\n",
      "[05/22/2025 15:41:14 INFO 140473021069120] Epoch[65] Batch[5] avg_epoch_loss=1.088128\n",
      "[05/22/2025 15:41:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=1.0881282190481822\n",
      "[05/22/2025 15:41:14 INFO 140473021069120] Epoch[65] Batch [5]#011Speed: 363.55 samples/sec#011loss=1.088128\n",
      "[05/22/2025 15:41:15 INFO 140473021069120] processed a total of 1253 examples\n",
      "#metrics {\"StartTime\": 1747928470.8080044, \"EndTime\": 1747928475.5384912, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4730.163335800171, \"count\": 1, \"min\": 4730.163335800171, \"max\": 4730.163335800171}}}\n",
      "[05/22/2025 15:41:15 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=264.89054819701283 records/second\n",
      "[05/22/2025 15:41:15 INFO 140473021069120] #progress_metric: host=algo-1, completed 16.5 % of epochs\n",
      "[05/22/2025 15:41:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=65, train loss <loss>=1.1200810015201568\n",
      "[05/22/2025 15:41:15 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:17 INFO 140473021069120] Epoch[66] Batch[0] avg_epoch_loss=1.069035\n",
      "[05/22/2025 15:41:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=1.0690348148345947\n",
      "[05/22/2025 15:41:18 INFO 140473021069120] Epoch[66] Batch[5] avg_epoch_loss=1.059936\n",
      "[05/22/2025 15:41:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=1.0599364936351776\n",
      "[05/22/2025 15:41:18 INFO 140473021069120] Epoch[66] Batch [5]#011Speed: 365.29 samples/sec#011loss=1.059936\n",
      "[05/22/2025 15:41:20 INFO 140473021069120] Epoch[66] Batch[10] avg_epoch_loss=1.079552\n",
      "[05/22/2025 15:41:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=1.1030898094177246\n",
      "[05/22/2025 15:41:20 INFO 140473021069120] Epoch[66] Batch [10]#011Speed: 350.45 samples/sec#011loss=1.103090\n",
      "[05/22/2025 15:41:20 INFO 140473021069120] processed a total of 1329 examples\n",
      "#metrics {\"StartTime\": 1747928475.5385544, \"EndTime\": 1747928480.639856, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5101.000785827637, \"count\": 1, \"min\": 5101.000785827637, \"max\": 5101.000785827637}}}\n",
      "[05/22/2025 15:41:20 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.53244558408016 records/second\n",
      "[05/22/2025 15:41:20 INFO 140473021069120] #progress_metric: host=algo-1, completed 16.75 % of epochs\n",
      "[05/22/2025 15:41:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=66, train loss <loss>=1.079551637172699\n",
      "[05/22/2025 15:41:20 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:22 INFO 140473021069120] Epoch[67] Batch[0] avg_epoch_loss=1.134887\n",
      "[05/22/2025 15:41:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=1.1348868608474731\n",
      "[05/22/2025 15:41:23 INFO 140473021069120] Epoch[67] Batch[5] avg_epoch_loss=1.094378\n",
      "[05/22/2025 15:41:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=1.0943778256575267\n",
      "[05/22/2025 15:41:23 INFO 140473021069120] Epoch[67] Batch [5]#011Speed: 365.31 samples/sec#011loss=1.094378\n",
      "[05/22/2025 15:41:25 INFO 140473021069120] processed a total of 1244 examples\n",
      "#metrics {\"StartTime\": 1747928480.6399162, \"EndTime\": 1747928485.3599546, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4719.648361206055, \"count\": 1, \"min\": 4719.648361206055, \"max\": 4719.648361206055}}}\n",
      "[05/22/2025 15:41:25 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.5735261906821 records/second\n",
      "[05/22/2025 15:41:25 INFO 140473021069120] #progress_metric: host=algo-1, completed 17.0 % of epochs\n",
      "[05/22/2025 15:41:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=67, train loss <loss>=1.1198411643505097\n",
      "[05/22/2025 15:41:25 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:26 INFO 140473021069120] Epoch[68] Batch[0] avg_epoch_loss=1.161335\n",
      "[05/22/2025 15:41:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=1.1613351106643677\n",
      "[05/22/2025 15:41:28 INFO 140473021069120] Epoch[68] Batch[5] avg_epoch_loss=1.092699\n",
      "[05/22/2025 15:41:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=1.0926988224188487\n",
      "[05/22/2025 15:41:28 INFO 140473021069120] Epoch[68] Batch [5]#011Speed: 365.46 samples/sec#011loss=1.092699\n",
      "[05/22/2025 15:41:30 INFO 140473021069120] Epoch[68] Batch[10] avg_epoch_loss=1.082533\n",
      "[05/22/2025 15:41:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=1.0703335762023927\n",
      "[05/22/2025 15:41:30 INFO 140473021069120] Epoch[68] Batch [10]#011Speed: 356.51 samples/sec#011loss=1.070334\n",
      "[05/22/2025 15:41:30 INFO 140473021069120] processed a total of 1290 examples\n",
      "#metrics {\"StartTime\": 1747928485.3600202, \"EndTime\": 1747928490.4573414, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5096.956491470337, \"count\": 1, \"min\": 5096.956491470337, \"max\": 5096.956491470337}}}\n",
      "[05/22/2025 15:41:30 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.08799884893924 records/second\n",
      "[05/22/2025 15:41:30 INFO 140473021069120] #progress_metric: host=algo-1, completed 17.25 % of epochs\n",
      "[05/22/2025 15:41:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=68, train loss <loss>=1.0825328014113687\n",
      "[05/22/2025 15:41:30 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:31 INFO 140473021069120] Epoch[69] Batch[0] avg_epoch_loss=1.186863\n",
      "[05/22/2025 15:41:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=1.1868633031845093\n",
      "[05/22/2025 15:41:33 INFO 140473021069120] Epoch[69] Batch[5] avg_epoch_loss=1.122080\n",
      "[05/22/2025 15:41:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=1.1220804353555043\n",
      "[05/22/2025 15:41:33 INFO 140473021069120] Epoch[69] Batch [5]#011Speed: 367.80 samples/sec#011loss=1.122080\n",
      "[05/22/2025 15:41:35 INFO 140473021069120] Epoch[69] Batch[10] avg_epoch_loss=1.025550\n",
      "[05/22/2025 15:41:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=0.9097144469618798\n",
      "[05/22/2025 15:41:35 INFO 140473021069120] Epoch[69] Batch [10]#011Speed: 354.12 samples/sec#011loss=0.909714\n",
      "[05/22/2025 15:41:35 INFO 140473021069120] processed a total of 1314 examples\n",
      "#metrics {\"StartTime\": 1747928490.457399, \"EndTime\": 1747928495.5474634, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5089.811563491821, \"count\": 1, \"min\": 5089.811563491821, \"max\": 5089.811563491821}}}\n",
      "[05/22/2025 15:41:35 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=258.15803454379306 records/second\n",
      "[05/22/2025 15:41:35 INFO 140473021069120] #progress_metric: host=algo-1, completed 17.5 % of epochs\n",
      "[05/22/2025 15:41:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=69, train loss <loss>=1.0255504406311295\n",
      "[05/22/2025 15:41:35 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:37 INFO 140473021069120] Epoch[70] Batch[0] avg_epoch_loss=1.250405\n",
      "[05/22/2025 15:41:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=1.250405192375183\n",
      "[05/22/2025 15:41:38 INFO 140473021069120] Epoch[70] Batch[5] avg_epoch_loss=1.006167\n",
      "[05/22/2025 15:41:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=1.0061672429243724\n",
      "[05/22/2025 15:41:38 INFO 140473021069120] Epoch[70] Batch [5]#011Speed: 364.50 samples/sec#011loss=1.006167\n",
      "[05/22/2025 15:41:40 INFO 140473021069120] processed a total of 1278 examples\n",
      "#metrics {\"StartTime\": 1747928495.5475266, \"EndTime\": 1747928500.2903516, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4742.496728897095, \"count\": 1, \"min\": 4742.496728897095, \"max\": 4742.496728897095}}}\n",
      "[05/22/2025 15:41:40 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.47286326703744 records/second\n",
      "[05/22/2025 15:41:40 INFO 140473021069120] #progress_metric: host=algo-1, completed 17.75 % of epochs\n",
      "[05/22/2025 15:41:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=70, train loss <loss>=1.0144180417060853\n",
      "[05/22/2025 15:41:40 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:41 INFO 140473021069120] Epoch[71] Batch[0] avg_epoch_loss=1.144726\n",
      "[05/22/2025 15:41:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=1.1447259187698364\n",
      "[05/22/2025 15:41:43 INFO 140473021069120] Epoch[71] Batch[5] avg_epoch_loss=1.168977\n",
      "[05/22/2025 15:41:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=1.1689772009849548\n",
      "[05/22/2025 15:41:43 INFO 140473021069120] Epoch[71] Batch [5]#011Speed: 366.77 samples/sec#011loss=1.168977\n",
      "[05/22/2025 15:41:45 INFO 140473021069120] processed a total of 1271 examples\n",
      "#metrics {\"StartTime\": 1747928500.2904162, \"EndTime\": 1747928505.0277011, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4736.989498138428, \"count\": 1, \"min\": 4736.989498138428, \"max\": 4736.989498138428}}}\n",
      "[05/22/2025 15:41:45 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.3085931108451 records/second\n",
      "[05/22/2025 15:41:45 INFO 140473021069120] #progress_metric: host=algo-1, completed 18.0 % of epochs\n",
      "[05/22/2025 15:41:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=71, train loss <loss>=1.0791034519672393\n",
      "[05/22/2025 15:41:45 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:46 INFO 140473021069120] Epoch[72] Batch[0] avg_epoch_loss=0.889915\n",
      "[05/22/2025 15:41:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.889915406703949\n",
      "[05/22/2025 15:41:48 INFO 140473021069120] Epoch[72] Batch[5] avg_epoch_loss=1.030381\n",
      "[05/22/2025 15:41:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=1.0303809940814972\n",
      "[05/22/2025 15:41:48 INFO 140473021069120] Epoch[72] Batch [5]#011Speed: 366.96 samples/sec#011loss=1.030381\n",
      "[05/22/2025 15:41:49 INFO 140473021069120] processed a total of 1279 examples\n",
      "#metrics {\"StartTime\": 1747928505.027764, \"EndTime\": 1747928509.7572346, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4729.085206985474, \"count\": 1, \"min\": 4729.085206985474, \"max\": 4729.085206985474}}}\n",
      "[05/22/2025 15:41:49 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=270.44844841816956 records/second\n",
      "[05/22/2025 15:41:49 INFO 140473021069120] #progress_metric: host=algo-1, completed 18.25 % of epochs\n",
      "[05/22/2025 15:41:49 INFO 140473021069120] #quality_metric: host=algo-1, epoch=72, train loss <loss>=1.0322664201259613\n",
      "[05/22/2025 15:41:49 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:51 INFO 140473021069120] Epoch[73] Batch[0] avg_epoch_loss=1.168695\n",
      "[05/22/2025 15:41:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=1.1686952114105225\n",
      "[05/22/2025 15:41:53 INFO 140473021069120] Epoch[73] Batch[5] avg_epoch_loss=1.185527\n",
      "[05/22/2025 15:41:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=1.1855274339516957\n",
      "[05/22/2025 15:41:53 INFO 140473021069120] Epoch[73] Batch [5]#011Speed: 365.41 samples/sec#011loss=1.185527\n",
      "[05/22/2025 15:41:54 INFO 140473021069120] Epoch[73] Batch[10] avg_epoch_loss=1.290788\n",
      "[05/22/2025 15:41:54 INFO 140473021069120] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=1.4171001315116882\n",
      "[05/22/2025 15:41:54 INFO 140473021069120] Epoch[73] Batch [10]#011Speed: 356.10 samples/sec#011loss=1.417100\n",
      "[05/22/2025 15:41:54 INFO 140473021069120] processed a total of 1285 examples\n",
      "#metrics {\"StartTime\": 1747928509.7572997, \"EndTime\": 1747928514.8496478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5091.994524002075, \"count\": 1, \"min\": 5091.994524002075, \"max\": 5091.994524002075}}}\n",
      "[05/22/2025 15:41:54 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=252.350103467332 records/second\n",
      "[05/22/2025 15:41:54 INFO 140473021069120] #progress_metric: host=algo-1, completed 18.5 % of epochs\n",
      "[05/22/2025 15:41:54 INFO 140473021069120] #quality_metric: host=algo-1, epoch=73, train loss <loss>=1.2907877510244197\n",
      "[05/22/2025 15:41:54 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:41:56 INFO 140473021069120] Epoch[74] Batch[0] avg_epoch_loss=1.083031\n",
      "[05/22/2025 15:41:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=1.0830307006835938\n",
      "[05/22/2025 15:41:58 INFO 140473021069120] Epoch[74] Batch[5] avg_epoch_loss=1.067661\n",
      "[05/22/2025 15:41:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=1.0676607290903728\n",
      "[05/22/2025 15:41:58 INFO 140473021069120] Epoch[74] Batch [5]#011Speed: 367.06 samples/sec#011loss=1.067661\n",
      "[05/22/2025 15:41:59 INFO 140473021069120] processed a total of 1244 examples\n",
      "#metrics {\"StartTime\": 1747928514.8497562, \"EndTime\": 1747928519.5652237, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4715.195417404175, \"count\": 1, \"min\": 4715.195417404175, \"max\": 4715.195417404175}}}\n",
      "[05/22/2025 15:41:59 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.8225017426486 records/second\n",
      "[05/22/2025 15:41:59 INFO 140473021069120] #progress_metric: host=algo-1, completed 18.75 % of epochs\n",
      "[05/22/2025 15:41:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=74, train loss <loss>=1.0463603734970093\n",
      "[05/22/2025 15:41:59 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:01 INFO 140473021069120] Epoch[75] Batch[0] avg_epoch_loss=1.043250\n",
      "[05/22/2025 15:42:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=1.0432502031326294\n",
      "[05/22/2025 15:42:02 INFO 140473021069120] Epoch[75] Batch[5] avg_epoch_loss=1.160783\n",
      "[05/22/2025 15:42:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=1.1607833504676819\n",
      "[05/22/2025 15:42:02 INFO 140473021069120] Epoch[75] Batch [5]#011Speed: 359.69 samples/sec#011loss=1.160783\n",
      "[05/22/2025 15:42:04 INFO 140473021069120] Epoch[75] Batch[10] avg_epoch_loss=1.223396\n",
      "[05/22/2025 15:42:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=1.2985320925712585\n",
      "[05/22/2025 15:42:04 INFO 140473021069120] Epoch[75] Batch [10]#011Speed: 351.78 samples/sec#011loss=1.298532\n",
      "[05/22/2025 15:42:04 INFO 140473021069120] processed a total of 1311 examples\n",
      "#metrics {\"StartTime\": 1747928519.5652888, \"EndTime\": 1747928524.6972623, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5131.664276123047, \"count\": 1, \"min\": 5131.664276123047, \"max\": 5131.664276123047}}}\n",
      "[05/22/2025 15:42:04 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=255.46833093246155 records/second\n",
      "[05/22/2025 15:42:04 INFO 140473021069120] #progress_metric: host=algo-1, completed 19.0 % of epochs\n",
      "[05/22/2025 15:42:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=75, train loss <loss>=1.2233964150602168\n",
      "[05/22/2025 15:42:04 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:06 INFO 140473021069120] Epoch[76] Batch[0] avg_epoch_loss=1.111946\n",
      "[05/22/2025 15:42:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=1.1119457483291626\n",
      "[05/22/2025 15:42:07 INFO 140473021069120] Epoch[76] Batch[5] avg_epoch_loss=1.084122\n",
      "[05/22/2025 15:42:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=1.0841215749581654\n",
      "[05/22/2025 15:42:07 INFO 140473021069120] Epoch[76] Batch [5]#011Speed: 364.56 samples/sec#011loss=1.084122\n",
      "[05/22/2025 15:42:09 INFO 140473021069120] Epoch[76] Batch[10] avg_epoch_loss=1.133437\n",
      "[05/22/2025 15:42:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=1.1926145553588867\n",
      "[05/22/2025 15:42:09 INFO 140473021069120] Epoch[76] Batch [10]#011Speed: 344.04 samples/sec#011loss=1.192615\n",
      "[05/22/2025 15:42:09 INFO 140473021069120] processed a total of 1356 examples\n",
      "#metrics {\"StartTime\": 1747928524.6973214, \"EndTime\": 1747928529.8137567, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5116.122722625732, \"count\": 1, \"min\": 5116.122722625732, \"max\": 5116.122722625732}}}\n",
      "[05/22/2025 15:42:09 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=265.03999191758624 records/second\n",
      "[05/22/2025 15:42:09 INFO 140473021069120] #progress_metric: host=algo-1, completed 19.25 % of epochs\n",
      "[05/22/2025 15:42:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=76, train loss <loss>=1.1334365660494023\n",
      "[05/22/2025 15:42:09 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:11 INFO 140473021069120] Epoch[77] Batch[0] avg_epoch_loss=1.254322\n",
      "[05/22/2025 15:42:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=1.2543222904205322\n",
      "[05/22/2025 15:42:13 INFO 140473021069120] Epoch[77] Batch[5] avg_epoch_loss=1.187768\n",
      "[05/22/2025 15:42:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=1.187767853339513\n",
      "[05/22/2025 15:42:13 INFO 140473021069120] Epoch[77] Batch [5]#011Speed: 376.53 samples/sec#011loss=1.187768\n",
      "[05/22/2025 15:42:14 INFO 140473021069120] Epoch[77] Batch[10] avg_epoch_loss=1.187382\n",
      "[05/22/2025 15:42:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=1.186919641494751\n",
      "[05/22/2025 15:42:14 INFO 140473021069120] Epoch[77] Batch [10]#011Speed: 371.09 samples/sec#011loss=1.186920\n",
      "[05/22/2025 15:42:14 INFO 140473021069120] processed a total of 1302 examples\n",
      "#metrics {\"StartTime\": 1747928529.8138146, \"EndTime\": 1747928534.780599, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4966.531038284302, \"count\": 1, \"min\": 4966.531038284302, \"max\": 4966.531038284302}}}\n",
      "[05/22/2025 15:42:14 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.15019133516404 records/second\n",
      "[05/22/2025 15:42:14 INFO 140473021069120] #progress_metric: host=algo-1, completed 19.5 % of epochs\n",
      "[05/22/2025 15:42:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=77, train loss <loss>=1.1873823025009849\n",
      "[05/22/2025 15:42:14 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:16 INFO 140473021069120] Epoch[78] Batch[0] avg_epoch_loss=1.262038\n",
      "[05/22/2025 15:42:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=1.2620383501052856\n",
      "[05/22/2025 15:42:17 INFO 140473021069120] Epoch[78] Batch[5] avg_epoch_loss=1.123750\n",
      "[05/22/2025 15:42:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=1.1237501204013824\n",
      "[05/22/2025 15:42:17 INFO 140473021069120] Epoch[78] Batch [5]#011Speed: 383.05 samples/sec#011loss=1.123750\n",
      "[05/22/2025 15:42:19 INFO 140473021069120] processed a total of 1272 examples\n",
      "#metrics {\"StartTime\": 1747928534.780659, \"EndTime\": 1747928539.362713, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4581.787109375, \"count\": 1, \"min\": 4581.787109375, \"max\": 4581.787109375}}}\n",
      "[05/22/2025 15:42:19 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=277.615253066675 records/second\n",
      "[05/22/2025 15:42:19 INFO 140473021069120] #progress_metric: host=algo-1, completed 19.75 % of epochs\n",
      "[05/22/2025 15:42:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=78, train loss <loss>=1.1269802510738374\n",
      "[05/22/2025 15:42:19 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:20 INFO 140473021069120] Epoch[79] Batch[0] avg_epoch_loss=1.027770\n",
      "[05/22/2025 15:42:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=1.0277695655822754\n",
      "[05/22/2025 15:42:22 INFO 140473021069120] Epoch[79] Batch[5] avg_epoch_loss=1.078108\n",
      "[05/22/2025 15:42:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=1.0781078437964122\n",
      "[05/22/2025 15:42:22 INFO 140473021069120] Epoch[79] Batch [5]#011Speed: 386.83 samples/sec#011loss=1.078108\n",
      "[05/22/2025 15:42:23 INFO 140473021069120] processed a total of 1273 examples\n",
      "#metrics {\"StartTime\": 1747928539.3627768, \"EndTime\": 1747928543.9019148, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4538.846492767334, \"count\": 1, \"min\": 4538.846492767334, \"max\": 4538.846492767334}}}\n",
      "[05/22/2025 15:42:23 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=280.46283017822327 records/second\n",
      "[05/22/2025 15:42:23 INFO 140473021069120] #progress_metric: host=algo-1, completed 20.0 % of epochs\n",
      "[05/22/2025 15:42:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=79, train loss <loss>=1.0570194959640502\n",
      "[05/22/2025 15:42:23 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:25 INFO 140473021069120] Epoch[80] Batch[0] avg_epoch_loss=1.021299\n",
      "[05/22/2025 15:42:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=1.021299123764038\n",
      "[05/22/2025 15:42:27 INFO 140473021069120] Epoch[80] Batch[5] avg_epoch_loss=1.088795\n",
      "[05/22/2025 15:42:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=1.0887948870658875\n",
      "[05/22/2025 15:42:27 INFO 140473021069120] Epoch[80] Batch [5]#011Speed: 388.06 samples/sec#011loss=1.088795\n",
      "[05/22/2025 15:42:28 INFO 140473021069120] Epoch[80] Batch[10] avg_epoch_loss=1.116169\n",
      "[05/22/2025 15:42:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=1.1490181922912597\n",
      "[05/22/2025 15:42:28 INFO 140473021069120] Epoch[80] Batch [10]#011Speed: 364.83 samples/sec#011loss=1.149018\n",
      "[05/22/2025 15:42:28 INFO 140473021069120] processed a total of 1318 examples\n",
      "#metrics {\"StartTime\": 1747928543.9019675, \"EndTime\": 1747928548.8081682, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4905.771970748901, \"count\": 1, \"min\": 4905.771970748901, \"max\": 4905.771970748901}}}\n",
      "[05/22/2025 15:42:28 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.6581185440879 records/second\n",
      "[05/22/2025 15:42:28 INFO 140473021069120] #progress_metric: host=algo-1, completed 20.25 % of epochs\n",
      "[05/22/2025 15:42:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=80, train loss <loss>=1.116169116713784\n",
      "[05/22/2025 15:42:28 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:30 INFO 140473021069120] Epoch[81] Batch[0] avg_epoch_loss=1.055828\n",
      "[05/22/2025 15:42:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=1.0558280944824219\n",
      "[05/22/2025 15:42:32 INFO 140473021069120] Epoch[81] Batch[5] avg_epoch_loss=1.009344\n",
      "[05/22/2025 15:42:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=1.0093436042467754\n",
      "[05/22/2025 15:42:32 INFO 140473021069120] Epoch[81] Batch [5]#011Speed: 382.03 samples/sec#011loss=1.009344\n",
      "[05/22/2025 15:42:33 INFO 140473021069120] processed a total of 1280 examples\n",
      "#metrics {\"StartTime\": 1747928548.8082314, \"EndTime\": 1747928553.4092648, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4600.722551345825, \"count\": 1, \"min\": 4600.722551345825, \"max\": 4600.722551345825}}}\n",
      "[05/22/2025 15:42:33 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=278.2113723984545 records/second\n",
      "[05/22/2025 15:42:33 INFO 140473021069120] #progress_metric: host=algo-1, completed 20.5 % of epochs\n",
      "[05/22/2025 15:42:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.9899244427680969\n",
      "[05/22/2025 15:42:33 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:34 INFO 140473021069120] Epoch[82] Batch[0] avg_epoch_loss=0.932140\n",
      "[05/22/2025 15:42:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=0.93213951587677\n",
      "[05/22/2025 15:42:36 INFO 140473021069120] Epoch[82] Batch[5] avg_epoch_loss=1.013089\n",
      "[05/22/2025 15:42:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=1.0130885144074757\n",
      "[05/22/2025 15:42:36 INFO 140473021069120] Epoch[82] Batch [5]#011Speed: 379.46 samples/sec#011loss=1.013089\n",
      "[05/22/2025 15:42:38 INFO 140473021069120] Epoch[82] Batch[10] avg_epoch_loss=1.055726\n",
      "[05/22/2025 15:42:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=1.1068899989128114\n",
      "[05/22/2025 15:42:38 INFO 140473021069120] Epoch[82] Batch [10]#011Speed: 366.78 samples/sec#011loss=1.106890\n",
      "[05/22/2025 15:42:38 INFO 140473021069120] processed a total of 1341 examples\n",
      "#metrics {\"StartTime\": 1747928553.40933, \"EndTime\": 1747928558.337629, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4928.0102252960205, \"count\": 1, \"min\": 4928.0102252960205, \"max\": 4928.0102252960205}}}\n",
      "[05/22/2025 15:42:38 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=272.11278122628653 records/second\n",
      "[05/22/2025 15:42:38 INFO 140473021069120] #progress_metric: host=algo-1, completed 20.75 % of epochs\n",
      "[05/22/2025 15:42:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=82, train loss <loss>=1.055725552818992\n",
      "[05/22/2025 15:42:38 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:39 INFO 140473021069120] Epoch[83] Batch[0] avg_epoch_loss=1.221895\n",
      "[05/22/2025 15:42:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=1.2218945026397705\n",
      "[05/22/2025 15:42:41 INFO 140473021069120] Epoch[83] Batch[5] avg_epoch_loss=1.085027\n",
      "[05/22/2025 15:42:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=1.0850266913572948\n",
      "[05/22/2025 15:42:41 INFO 140473021069120] Epoch[83] Batch [5]#011Speed: 383.01 samples/sec#011loss=1.085027\n",
      "[05/22/2025 15:42:42 INFO 140473021069120] processed a total of 1278 examples\n",
      "#metrics {\"StartTime\": 1747928558.3376946, \"EndTime\": 1747928562.8955817, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4557.622671127319, \"count\": 1, \"min\": 4557.622671127319, \"max\": 4557.622671127319}}}\n",
      "[05/22/2025 15:42:42 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=280.40368560235663 records/second\n",
      "[05/22/2025 15:42:42 INFO 140473021069120] #progress_metric: host=algo-1, completed 21.0 % of epochs\n",
      "[05/22/2025 15:42:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=83, train loss <loss>=1.0729526996612548\n",
      "[05/22/2025 15:42:42 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:44 INFO 140473021069120] Epoch[84] Batch[0] avg_epoch_loss=1.093407\n",
      "[05/22/2025 15:42:44 INFO 140473021069120] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=1.0934066772460938\n",
      "[05/22/2025 15:42:46 INFO 140473021069120] Epoch[84] Batch[5] avg_epoch_loss=1.025222\n",
      "[05/22/2025 15:42:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=1.02522212266922\n",
      "[05/22/2025 15:42:46 INFO 140473021069120] Epoch[84] Batch [5]#011Speed: 387.49 samples/sec#011loss=1.025222\n",
      "[05/22/2025 15:42:47 INFO 140473021069120] processed a total of 1278 examples\n",
      "#metrics {\"StartTime\": 1747928562.8956442, \"EndTime\": 1747928567.4402742, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4544.333696365356, \"count\": 1, \"min\": 4544.333696365356, \"max\": 4544.333696365356}}}\n",
      "[05/22/2025 15:42:47 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=281.2234751949232 records/second\n",
      "[05/22/2025 15:42:47 INFO 140473021069120] #progress_metric: host=algo-1, completed 21.25 % of epochs\n",
      "[05/22/2025 15:42:47 INFO 140473021069120] #quality_metric: host=algo-1, epoch=84, train loss <loss>=1.043020385503769\n",
      "[05/22/2025 15:42:47 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:48 INFO 140473021069120] Epoch[85] Batch[0] avg_epoch_loss=0.998295\n",
      "[05/22/2025 15:42:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.9982950687408447\n",
      "[05/22/2025 15:42:50 INFO 140473021069120] Epoch[85] Batch[5] avg_epoch_loss=1.163457\n",
      "[05/22/2025 15:42:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=1.1634569366772969\n",
      "[05/22/2025 15:42:50 INFO 140473021069120] Epoch[85] Batch [5]#011Speed: 382.98 samples/sec#011loss=1.163457\n",
      "[05/22/2025 15:42:51 INFO 140473021069120] processed a total of 1216 examples\n",
      "#metrics {\"StartTime\": 1747928567.4403381, \"EndTime\": 1747928571.994643, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4554.007053375244, \"count\": 1, \"min\": 4554.007053375244, \"max\": 4554.007053375244}}}\n",
      "[05/22/2025 15:42:51 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=267.01191709714357 records/second\n",
      "[05/22/2025 15:42:51 INFO 140473021069120] #progress_metric: host=algo-1, completed 21.5 % of epochs\n",
      "[05/22/2025 15:42:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=85, train loss <loss>=1.192380166053772\n",
      "[05/22/2025 15:42:51 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:53 INFO 140473021069120] Epoch[86] Batch[0] avg_epoch_loss=0.773920\n",
      "[05/22/2025 15:42:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.7739198207855225\n",
      "[05/22/2025 15:42:55 INFO 140473021069120] Epoch[86] Batch[5] avg_epoch_loss=0.983667\n",
      "[05/22/2025 15:42:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.9836667080720266\n",
      "[05/22/2025 15:42:55 INFO 140473021069120] Epoch[86] Batch [5]#011Speed: 383.46 samples/sec#011loss=0.983667\n",
      "[05/22/2025 15:42:56 INFO 140473021069120] processed a total of 1241 examples\n",
      "#metrics {\"StartTime\": 1747928571.9947097, \"EndTime\": 1747928576.573354, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4578.356027603149, \"count\": 1, \"min\": 4578.356027603149, \"max\": 4578.356027603149}}}\n",
      "[05/22/2025 15:42:56 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=271.05255954127296 records/second\n",
      "[05/22/2025 15:42:56 INFO 140473021069120] #progress_metric: host=algo-1, completed 21.75 % of epochs\n",
      "[05/22/2025 15:42:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=86, train loss <loss>=1.0098801910877229\n",
      "[05/22/2025 15:42:56 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:42:58 INFO 140473021069120] Epoch[87] Batch[0] avg_epoch_loss=1.087547\n",
      "[05/22/2025 15:42:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=1.087546706199646\n",
      "[05/22/2025 15:42:59 INFO 140473021069120] Epoch[87] Batch[5] avg_epoch_loss=1.077011\n",
      "[05/22/2025 15:42:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=1.0770113170146942\n",
      "[05/22/2025 15:42:59 INFO 140473021069120] Epoch[87] Batch [5]#011Speed: 384.72 samples/sec#011loss=1.077011\n",
      "[05/22/2025 15:43:01 INFO 140473021069120] processed a total of 1230 examples\n",
      "#metrics {\"StartTime\": 1747928576.573417, \"EndTime\": 1747928581.1336935, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4559.914350509644, \"count\": 1, \"min\": 4559.914350509644, \"max\": 4559.914350509644}}}\n",
      "[05/22/2025 15:43:01 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.7361262322639 records/second\n",
      "[05/22/2025 15:43:01 INFO 140473021069120] #progress_metric: host=algo-1, completed 22.0 % of epochs\n",
      "[05/22/2025 15:43:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=87, train loss <loss>=1.0676725506782532\n",
      "[05/22/2025 15:43:01 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:02 INFO 140473021069120] Epoch[88] Batch[0] avg_epoch_loss=0.977155\n",
      "[05/22/2025 15:43:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.9771552681922913\n",
      "[05/22/2025 15:43:04 INFO 140473021069120] Epoch[88] Batch[5] avg_epoch_loss=1.011018\n",
      "[05/22/2025 15:43:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=1.011017640431722\n",
      "[05/22/2025 15:43:04 INFO 140473021069120] Epoch[88] Batch [5]#011Speed: 380.38 samples/sec#011loss=1.011018\n",
      "[05/22/2025 15:43:05 INFO 140473021069120] processed a total of 1270 examples\n",
      "#metrics {\"StartTime\": 1747928581.1337593, \"EndTime\": 1747928585.7294962, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4595.433235168457, \"count\": 1, \"min\": 4595.433235168457, \"max\": 4595.433235168457}}}\n",
      "[05/22/2025 15:43:05 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=276.35548564826104 records/second\n",
      "[05/22/2025 15:43:05 INFO 140473021069120] #progress_metric: host=algo-1, completed 22.25 % of epochs\n",
      "[05/22/2025 15:43:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=88, train loss <loss>=1.0678709208965302\n",
      "[05/22/2025 15:43:05 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:07 INFO 140473021069120] Epoch[89] Batch[0] avg_epoch_loss=0.822166\n",
      "[05/22/2025 15:43:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.8221655488014221\n",
      "[05/22/2025 15:43:09 INFO 140473021069120] Epoch[89] Batch[5] avg_epoch_loss=0.933798\n",
      "[05/22/2025 15:43:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.9337982734044393\n",
      "[05/22/2025 15:43:09 INFO 140473021069120] Epoch[89] Batch [5]#011Speed: 367.06 samples/sec#011loss=0.933798\n",
      "[05/22/2025 15:43:10 INFO 140473021069120] processed a total of 1256 examples\n",
      "#metrics {\"StartTime\": 1747928585.729563, \"EndTime\": 1747928590.4656081, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4735.696792602539, \"count\": 1, \"min\": 4735.696792602539, \"max\": 4735.696792602539}}}\n",
      "[05/22/2025 15:43:10 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=265.2142348525124 records/second\n",
      "[05/22/2025 15:43:10 INFO 140473021069120] #progress_metric: host=algo-1, completed 22.5 % of epochs\n",
      "[05/22/2025 15:43:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.9953184008598328\n",
      "[05/22/2025 15:43:10 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:12 INFO 140473021069120] Epoch[90] Batch[0] avg_epoch_loss=1.058350\n",
      "[05/22/2025 15:43:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=1.0583502054214478\n",
      "[05/22/2025 15:43:13 INFO 140473021069120] Epoch[90] Batch[5] avg_epoch_loss=0.997224\n",
      "[05/22/2025 15:43:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.997224489847819\n",
      "[05/22/2025 15:43:13 INFO 140473021069120] Epoch[90] Batch [5]#011Speed: 360.58 samples/sec#011loss=0.997224\n",
      "[05/22/2025 15:43:15 INFO 140473021069120] processed a total of 1279 examples\n",
      "#metrics {\"StartTime\": 1747928590.4656756, \"EndTime\": 1747928595.2130256, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4747.054576873779, \"count\": 1, \"min\": 4747.054576873779, \"max\": 4747.054576873779}}}\n",
      "[05/22/2025 15:43:15 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.4251136358614 records/second\n",
      "[05/22/2025 15:43:15 INFO 140473021069120] #progress_metric: host=algo-1, completed 22.75 % of epochs\n",
      "[05/22/2025 15:43:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=90, train loss <loss>=1.037149393558502\n",
      "[05/22/2025 15:43:15 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:16 INFO 140473021069120] Epoch[91] Batch[0] avg_epoch_loss=1.236675\n",
      "[05/22/2025 15:43:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=1.2366749048233032\n",
      "[05/22/2025 15:43:18 INFO 140473021069120] Epoch[91] Batch[5] avg_epoch_loss=1.143716\n",
      "[05/22/2025 15:43:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=1.1437163750330608\n",
      "[05/22/2025 15:43:18 INFO 140473021069120] Epoch[91] Batch [5]#011Speed: 366.35 samples/sec#011loss=1.143716\n",
      "[05/22/2025 15:43:20 INFO 140473021069120] Epoch[91] Batch[10] avg_epoch_loss=0.894720\n",
      "[05/22/2025 15:43:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=0.5959249854087829\n",
      "[05/22/2025 15:43:20 INFO 140473021069120] Epoch[91] Batch [10]#011Speed: 365.41 samples/sec#011loss=0.595925\n",
      "[05/22/2025 15:43:20 INFO 140473021069120] processed a total of 1282 examples\n",
      "#metrics {\"StartTime\": 1747928595.2130873, \"EndTime\": 1747928600.2636905, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5050.313949584961, \"count\": 1, \"min\": 5050.313949584961, \"max\": 5050.313949584961}}}\n",
      "[05/22/2025 15:43:20 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.84132689235457 records/second\n",
      "[05/22/2025 15:43:20 INFO 140473021069120] #progress_metric: host=algo-1, completed 23.0 % of epochs\n",
      "[05/22/2025 15:43:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.8947202888402072\n",
      "[05/22/2025 15:43:20 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:43:20 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_bd3e27bc-b54d-4506-b394-5c70f7c7500b-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928600.2637482, \"EndTime\": 1747928600.299176, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 34.6066951751709, \"count\": 1, \"min\": 34.6066951751709, \"max\": 34.6066951751709}}}\n",
      "[05/22/2025 15:43:21 INFO 140473021069120] Epoch[92] Batch[0] avg_epoch_loss=1.327308\n",
      "[05/22/2025 15:43:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=1.3273075819015503\n",
      "[05/22/2025 15:43:23 INFO 140473021069120] Epoch[92] Batch[5] avg_epoch_loss=1.066934\n",
      "[05/22/2025 15:43:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=1.0669344663619995\n",
      "[05/22/2025 15:43:23 INFO 140473021069120] Epoch[92] Batch [5]#011Speed: 354.75 samples/sec#011loss=1.066934\n",
      "[05/22/2025 15:43:25 INFO 140473021069120] Epoch[92] Batch[10] avg_epoch_loss=1.070310\n",
      "[05/22/2025 15:43:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=1.0743607759475708\n",
      "[05/22/2025 15:43:25 INFO 140473021069120] Epoch[92] Batch [10]#011Speed: 353.28 samples/sec#011loss=1.074361\n",
      "[05/22/2025 15:43:25 INFO 140473021069120] processed a total of 1283 examples\n",
      "#metrics {\"StartTime\": 1747928600.299238, \"EndTime\": 1747928605.5135984, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5214.303731918335, \"count\": 1, \"min\": 5214.303731918335, \"max\": 5214.303731918335}}}\n",
      "[05/22/2025 15:43:25 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=246.0498154109266 records/second\n",
      "[05/22/2025 15:43:25 INFO 140473021069120] #progress_metric: host=algo-1, completed 23.25 % of epochs\n",
      "[05/22/2025 15:43:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=92, train loss <loss>=1.0703100616281682\n",
      "[05/22/2025 15:43:25 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:27 INFO 140473021069120] Epoch[93] Batch[0] avg_epoch_loss=1.127075\n",
      "[05/22/2025 15:43:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=1.1270753145217896\n",
      "[05/22/2025 15:43:28 INFO 140473021069120] Epoch[93] Batch[5] avg_epoch_loss=1.217709\n",
      "[05/22/2025 15:43:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=1.2177087664604187\n",
      "[05/22/2025 15:43:28 INFO 140473021069120] Epoch[93] Batch [5]#011Speed: 363.18 samples/sec#011loss=1.217709\n",
      "[05/22/2025 15:43:30 INFO 140473021069120] Epoch[93] Batch[10] avg_epoch_loss=1.036681\n",
      "[05/22/2025 15:43:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=0.8194473475217819\n",
      "[05/22/2025 15:43:30 INFO 140473021069120] Epoch[93] Batch [10]#011Speed: 350.65 samples/sec#011loss=0.819447\n",
      "[05/22/2025 15:43:30 INFO 140473021069120] processed a total of 1359 examples\n",
      "#metrics {\"StartTime\": 1747928605.5136561, \"EndTime\": 1747928610.5920177, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5077.532052993774, \"count\": 1, \"min\": 5077.532052993774, \"max\": 5077.532052993774}}}\n",
      "[05/22/2025 15:43:30 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=267.6448500500068 records/second\n",
      "[05/22/2025 15:43:30 INFO 140473021069120] #progress_metric: host=algo-1, completed 23.5 % of epochs\n",
      "[05/22/2025 15:43:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=93, train loss <loss>=1.0366808487610384\n",
      "[05/22/2025 15:43:30 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:32 INFO 140473021069120] Epoch[94] Batch[0] avg_epoch_loss=1.233449\n",
      "[05/22/2025 15:43:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=1.23344886302948\n",
      "[05/22/2025 15:43:33 INFO 140473021069120] Epoch[94] Batch[5] avg_epoch_loss=1.079572\n",
      "[05/22/2025 15:43:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=1.0795723895231883\n",
      "[05/22/2025 15:43:33 INFO 140473021069120] Epoch[94] Batch [5]#011Speed: 365.42 samples/sec#011loss=1.079572\n",
      "[05/22/2025 15:43:35 INFO 140473021069120] processed a total of 1236 examples\n",
      "#metrics {\"StartTime\": 1747928610.5920792, \"EndTime\": 1747928615.3000581, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4707.680940628052, \"count\": 1, \"min\": 4707.680940628052, \"max\": 4707.680940628052}}}\n",
      "[05/22/2025 15:43:35 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.5432048022048 records/second\n",
      "[05/22/2025 15:43:35 INFO 140473021069120] #progress_metric: host=algo-1, completed 23.75 % of epochs\n",
      "[05/22/2025 15:43:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=94, train loss <loss>=1.1207936942577361\n",
      "[05/22/2025 15:43:35 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:36 INFO 140473021069120] Epoch[95] Batch[0] avg_epoch_loss=0.994891\n",
      "[05/22/2025 15:43:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.9948914647102356\n",
      "[05/22/2025 15:43:38 INFO 140473021069120] Epoch[95] Batch[5] avg_epoch_loss=0.991529\n",
      "[05/22/2025 15:43:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.9915285408496857\n",
      "[05/22/2025 15:43:38 INFO 140473021069120] Epoch[95] Batch [5]#011Speed: 362.48 samples/sec#011loss=0.991529\n",
      "[05/22/2025 15:43:40 INFO 140473021069120] Epoch[95] Batch[10] avg_epoch_loss=1.044408\n",
      "[05/22/2025 15:43:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=1.1078625798225403\n",
      "[05/22/2025 15:43:40 INFO 140473021069120] Epoch[95] Batch [10]#011Speed: 357.28 samples/sec#011loss=1.107863\n",
      "[05/22/2025 15:43:40 INFO 140473021069120] processed a total of 1332 examples\n",
      "#metrics {\"StartTime\": 1747928615.3001437, \"EndTime\": 1747928620.3949273, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5094.475746154785, \"count\": 1, \"min\": 5094.475746154785, \"max\": 5094.475746154785}}}\n",
      "[05/22/2025 15:43:40 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.4552876130205 records/second\n",
      "[05/22/2025 15:43:40 INFO 140473021069120] #progress_metric: host=algo-1, completed 24.0 % of epochs\n",
      "[05/22/2025 15:43:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=95, train loss <loss>=1.0444076494737105\n",
      "[05/22/2025 15:43:40 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:41 INFO 140473021069120] Epoch[96] Batch[0] avg_epoch_loss=1.195832\n",
      "[05/22/2025 15:43:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=1.195831537246704\n",
      "[05/22/2025 15:43:43 INFO 140473021069120] Epoch[96] Batch[5] avg_epoch_loss=1.075604\n",
      "[05/22/2025 15:43:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=1.075603683789571\n",
      "[05/22/2025 15:43:43 INFO 140473021069120] Epoch[96] Batch [5]#011Speed: 371.47 samples/sec#011loss=1.075604\n",
      "[05/22/2025 15:43:45 INFO 140473021069120] Epoch[96] Batch[10] avg_epoch_loss=1.080641\n",
      "[05/22/2025 15:43:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=1.0866860270500183\n",
      "[05/22/2025 15:43:45 INFO 140473021069120] Epoch[96] Batch [10]#011Speed: 355.21 samples/sec#011loss=1.086686\n",
      "[05/22/2025 15:43:45 INFO 140473021069120] processed a total of 1306 examples\n",
      "#metrics {\"StartTime\": 1747928620.394985, \"EndTime\": 1747928625.4552054, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5059.966087341309, \"count\": 1, \"min\": 5059.966087341309, \"max\": 5059.966087341309}}}\n",
      "[05/22/2025 15:43:45 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=258.0999847950897 records/second\n",
      "[05/22/2025 15:43:45 INFO 140473021069120] #progress_metric: host=algo-1, completed 24.25 % of epochs\n",
      "[05/22/2025 15:43:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=96, train loss <loss>=1.0806411125443198\n",
      "[05/22/2025 15:43:45 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:46 INFO 140473021069120] Epoch[97] Batch[0] avg_epoch_loss=1.048962\n",
      "[05/22/2025 15:43:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=1.0489623546600342\n",
      "[05/22/2025 15:43:48 INFO 140473021069120] Epoch[97] Batch[5] avg_epoch_loss=1.166506\n",
      "[05/22/2025 15:43:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=1.1665059328079224\n",
      "[05/22/2025 15:43:48 INFO 140473021069120] Epoch[97] Batch [5]#011Speed: 368.96 samples/sec#011loss=1.166506\n",
      "[05/22/2025 15:43:50 INFO 140473021069120] processed a total of 1265 examples\n",
      "#metrics {\"StartTime\": 1747928625.4552631, \"EndTime\": 1747928630.12425, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4668.715715408325, \"count\": 1, \"min\": 4668.715715408325, \"max\": 4668.715715408325}}}\n",
      "[05/22/2025 15:43:50 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=270.9470620127673 records/second\n",
      "[05/22/2025 15:43:50 INFO 140473021069120] #progress_metric: host=algo-1, completed 24.5 % of epochs\n",
      "[05/22/2025 15:43:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=97, train loss <loss>=1.1376230835914611\n",
      "[05/22/2025 15:43:50 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:51 INFO 140473021069120] Epoch[98] Batch[0] avg_epoch_loss=1.114192\n",
      "[05/22/2025 15:43:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=1.1141921281814575\n",
      "[05/22/2025 15:43:53 INFO 140473021069120] Epoch[98] Batch[5] avg_epoch_loss=1.004713\n",
      "[05/22/2025 15:43:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=1.0047130088011424\n",
      "[05/22/2025 15:43:53 INFO 140473021069120] Epoch[98] Batch [5]#011Speed: 365.29 samples/sec#011loss=1.004713\n",
      "[05/22/2025 15:43:55 INFO 140473021069120] Epoch[98] Batch[10] avg_epoch_loss=1.040482\n",
      "[05/22/2025 15:43:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=1.0834053039550782\n",
      "[05/22/2025 15:43:55 INFO 140473021069120] Epoch[98] Batch [10]#011Speed: 363.52 samples/sec#011loss=1.083405\n",
      "[05/22/2025 15:43:55 INFO 140473021069120] processed a total of 1295 examples\n",
      "#metrics {\"StartTime\": 1747928630.1243126, \"EndTime\": 1747928635.1646767, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5040.063381195068, \"count\": 1, \"min\": 5040.063381195068, \"max\": 5040.063381195068}}}\n",
      "[05/22/2025 15:43:55 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.93674045973484 records/second\n",
      "[05/22/2025 15:43:55 INFO 140473021069120] #progress_metric: host=algo-1, completed 24.75 % of epochs\n",
      "[05/22/2025 15:43:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=98, train loss <loss>=1.0404822338711133\n",
      "[05/22/2025 15:43:55 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:43:56 INFO 140473021069120] Epoch[99] Batch[0] avg_epoch_loss=1.054221\n",
      "[05/22/2025 15:43:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=1.0542209148406982\n",
      "[05/22/2025 15:43:58 INFO 140473021069120] Epoch[99] Batch[5] avg_epoch_loss=1.047099\n",
      "[05/22/2025 15:43:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=1.0470986068248749\n",
      "[05/22/2025 15:43:58 INFO 140473021069120] Epoch[99] Batch [5]#011Speed: 367.61 samples/sec#011loss=1.047099\n",
      "[05/22/2025 15:43:59 INFO 140473021069120] processed a total of 1260 examples\n",
      "#metrics {\"StartTime\": 1747928635.164736, \"EndTime\": 1747928639.8651545, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4699.979543685913, \"count\": 1, \"min\": 4699.979543685913, \"max\": 4699.979543685913}}}\n",
      "[05/22/2025 15:43:59 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.0814455151491 records/second\n",
      "[05/22/2025 15:43:59 INFO 140473021069120] #progress_metric: host=algo-1, completed 25.0 % of epochs\n",
      "[05/22/2025 15:43:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=99, train loss <loss>=1.0850393712520598\n",
      "[05/22/2025 15:43:59 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:01 INFO 140473021069120] Epoch[100] Batch[0] avg_epoch_loss=1.030696\n",
      "[05/22/2025 15:44:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=1.0306962728500366\n",
      "[05/22/2025 15:44:03 INFO 140473021069120] Epoch[100] Batch[5] avg_epoch_loss=1.083163\n",
      "[05/22/2025 15:44:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=1.0831634998321533\n",
      "[05/22/2025 15:44:03 INFO 140473021069120] Epoch[100] Batch [5]#011Speed: 364.31 samples/sec#011loss=1.083163\n",
      "[05/22/2025 15:44:04 INFO 140473021069120] processed a total of 1231 examples\n",
      "#metrics {\"StartTime\": 1747928639.8652105, \"EndTime\": 1747928644.5965548, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4730.921030044556, \"count\": 1, \"min\": 4730.921030044556, \"max\": 4730.921030044556}}}\n",
      "[05/22/2025 15:44:04 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.1983908402217 records/second\n",
      "[05/22/2025 15:44:04 INFO 140473021069120] #progress_metric: host=algo-1, completed 25.25 % of epochs\n",
      "[05/22/2025 15:44:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=100, train loss <loss>=0.9815716594457626\n",
      "[05/22/2025 15:44:04 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:06 INFO 140473021069120] Epoch[101] Batch[0] avg_epoch_loss=1.093094\n",
      "[05/22/2025 15:44:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=1.0930944681167603\n",
      "[05/22/2025 15:44:07 INFO 140473021069120] Epoch[101] Batch[5] avg_epoch_loss=1.036106\n",
      "[05/22/2025 15:44:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=1.0361062586307526\n",
      "[05/22/2025 15:44:07 INFO 140473021069120] Epoch[101] Batch [5]#011Speed: 366.31 samples/sec#011loss=1.036106\n",
      "[05/22/2025 15:44:09 INFO 140473021069120] Epoch[101] Batch[10] avg_epoch_loss=1.059730\n",
      "[05/22/2025 15:44:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=1.0880792737007141\n",
      "[05/22/2025 15:44:09 INFO 140473021069120] Epoch[101] Batch [10]#011Speed: 360.92 samples/sec#011loss=1.088079\n",
      "[05/22/2025 15:44:09 INFO 140473021069120] processed a total of 1297 examples\n",
      "#metrics {\"StartTime\": 1747928644.5966103, \"EndTime\": 1747928649.6517844, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5054.86273765564, \"count\": 1, \"min\": 5054.86273765564, \"max\": 5054.86273765564}}}\n",
      "[05/22/2025 15:44:09 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.5787921505178 records/second\n",
      "[05/22/2025 15:44:09 INFO 140473021069120] #progress_metric: host=algo-1, completed 25.5 % of epochs\n",
      "[05/22/2025 15:44:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=101, train loss <loss>=1.059730356389826\n",
      "[05/22/2025 15:44:09 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:11 INFO 140473021069120] Epoch[102] Batch[0] avg_epoch_loss=1.008718\n",
      "[05/22/2025 15:44:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=1.0087180137634277\n",
      "[05/22/2025 15:44:12 INFO 140473021069120] Epoch[102] Batch[5] avg_epoch_loss=0.988430\n",
      "[05/22/2025 15:44:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=0.988429586092631\n",
      "[05/22/2025 15:44:12 INFO 140473021069120] Epoch[102] Batch [5]#011Speed: 366.06 samples/sec#011loss=0.988430\n",
      "[05/22/2025 15:44:14 INFO 140473021069120] processed a total of 1237 examples\n",
      "#metrics {\"StartTime\": 1747928649.6518707, \"EndTime\": 1747928654.3633423, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4711.182355880737, \"count\": 1, \"min\": 4711.182355880737, \"max\": 4711.182355880737}}}\n",
      "[05/22/2025 15:44:14 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.5615620314522 records/second\n",
      "[05/22/2025 15:44:14 INFO 140473021069120] #progress_metric: host=algo-1, completed 25.75 % of epochs\n",
      "[05/22/2025 15:44:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=102, train loss <loss>=0.9430611044168472\n",
      "[05/22/2025 15:44:14 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:15 INFO 140473021069120] Epoch[103] Batch[0] avg_epoch_loss=1.054017\n",
      "[05/22/2025 15:44:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=1.0540167093276978\n",
      "[05/22/2025 15:44:17 INFO 140473021069120] Epoch[103] Batch[5] avg_epoch_loss=1.005569\n",
      "[05/22/2025 15:44:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=1.0055692891279857\n",
      "[05/22/2025 15:44:17 INFO 140473021069120] Epoch[103] Batch [5]#011Speed: 370.64 samples/sec#011loss=1.005569\n",
      "[05/22/2025 15:44:19 INFO 140473021069120] Epoch[103] Batch[10] avg_epoch_loss=1.028782\n",
      "[05/22/2025 15:44:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=1.0566366553306579\n",
      "[05/22/2025 15:44:19 INFO 140473021069120] Epoch[103] Batch [10]#011Speed: 361.35 samples/sec#011loss=1.056637\n",
      "[05/22/2025 15:44:19 INFO 140473021069120] processed a total of 1288 examples\n",
      "#metrics {\"StartTime\": 1747928654.3634055, \"EndTime\": 1747928659.404119, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5040.402173995972, \"count\": 1, \"min\": 5040.402173995972, \"max\": 5040.402173995972}}}\n",
      "[05/22/2025 15:44:19 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=255.529761899694 records/second\n",
      "[05/22/2025 15:44:19 INFO 140473021069120] #progress_metric: host=algo-1, completed 26.0 % of epochs\n",
      "[05/22/2025 15:44:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=103, train loss <loss>=1.0287817283110186\n",
      "[05/22/2025 15:44:19 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:20 INFO 140473021069120] Epoch[104] Batch[0] avg_epoch_loss=0.996965\n",
      "[05/22/2025 15:44:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=0.9969654083251953\n",
      "[05/22/2025 15:44:22 INFO 140473021069120] Epoch[104] Batch[5] avg_epoch_loss=1.072699\n",
      "[05/22/2025 15:44:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=1.0726987024148305\n",
      "[05/22/2025 15:44:22 INFO 140473021069120] Epoch[104] Batch [5]#011Speed: 370.09 samples/sec#011loss=1.072699\n",
      "[05/22/2025 15:44:24 INFO 140473021069120] Epoch[104] Batch[10] avg_epoch_loss=1.146497\n",
      "[05/22/2025 15:44:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=1.2350540280342102\n",
      "[05/22/2025 15:44:24 INFO 140473021069120] Epoch[104] Batch [10]#011Speed: 363.00 samples/sec#011loss=1.235054\n",
      "[05/22/2025 15:44:24 INFO 140473021069120] processed a total of 1310 examples\n",
      "#metrics {\"StartTime\": 1747928659.404178, \"EndTime\": 1747928664.4449153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5040.480613708496, \"count\": 1, \"min\": 5040.480613708496, \"max\": 5040.480613708496}}}\n",
      "[05/22/2025 15:44:24 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.8914995424674 records/second\n",
      "[05/22/2025 15:44:24 INFO 140473021069120] #progress_metric: host=algo-1, completed 26.25 % of epochs\n",
      "[05/22/2025 15:44:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=104, train loss <loss>=1.1464965776963667\n",
      "[05/22/2025 15:44:24 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:25 INFO 140473021069120] Epoch[105] Batch[0] avg_epoch_loss=0.978644\n",
      "[05/22/2025 15:44:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=0.9786438345909119\n",
      "[05/22/2025 15:44:27 INFO 140473021069120] Epoch[105] Batch[5] avg_epoch_loss=1.079613\n",
      "[05/22/2025 15:44:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=1.0796126921971638\n",
      "[05/22/2025 15:44:27 INFO 140473021069120] Epoch[105] Batch [5]#011Speed: 369.42 samples/sec#011loss=1.079613\n",
      "[05/22/2025 15:44:29 INFO 140473021069120] processed a total of 1267 examples\n",
      "#metrics {\"StartTime\": 1747928664.4449716, \"EndTime\": 1747928669.1189141, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4673.685550689697, \"count\": 1, \"min\": 4673.685550689697, \"max\": 4673.685550689697}}}\n",
      "[05/22/2025 15:44:29 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=271.0869404618409 records/second\n",
      "[05/22/2025 15:44:29 INFO 140473021069120] #progress_metric: host=algo-1, completed 26.5 % of epochs\n",
      "[05/22/2025 15:44:29 INFO 140473021069120] #quality_metric: host=algo-1, epoch=105, train loss <loss>=1.031531298160553\n",
      "[05/22/2025 15:44:29 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:30 INFO 140473021069120] Epoch[106] Batch[0] avg_epoch_loss=1.213460\n",
      "[05/22/2025 15:44:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=1.213459849357605\n",
      "[05/22/2025 15:44:32 INFO 140473021069120] Epoch[106] Batch[5] avg_epoch_loss=1.066689\n",
      "[05/22/2025 15:44:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=1.066688597202301\n",
      "[05/22/2025 15:44:32 INFO 140473021069120] Epoch[106] Batch [5]#011Speed: 375.15 samples/sec#011loss=1.066689\n",
      "[05/22/2025 15:44:34 INFO 140473021069120] Epoch[106] Batch[10] avg_epoch_loss=0.898752\n",
      "[05/22/2025 15:44:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=0.6972283244132995\n",
      "[05/22/2025 15:44:34 INFO 140473021069120] Epoch[106] Batch [10]#011Speed: 358.00 samples/sec#011loss=0.697228\n",
      "[05/22/2025 15:44:34 INFO 140473021069120] processed a total of 1322 examples\n",
      "#metrics {\"StartTime\": 1747928669.1189754, \"EndTime\": 1747928674.1325333, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5013.263940811157, \"count\": 1, \"min\": 5013.263940811157, \"max\": 5013.263940811157}}}\n",
      "[05/22/2025 15:44:34 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.6958435566418 records/second\n",
      "[05/22/2025 15:44:34 INFO 140473021069120] #progress_metric: host=algo-1, completed 26.75 % of epochs\n",
      "[05/22/2025 15:44:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=106, train loss <loss>=0.8987521095709368\n",
      "[05/22/2025 15:44:34 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:35 INFO 140473021069120] Epoch[107] Batch[0] avg_epoch_loss=1.041586\n",
      "[05/22/2025 15:44:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=1.0415856838226318\n",
      "[05/22/2025 15:44:37 INFO 140473021069120] Epoch[107] Batch[5] avg_epoch_loss=1.115863\n",
      "[05/22/2025 15:44:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=1.1158631245295207\n",
      "[05/22/2025 15:44:37 INFO 140473021069120] Epoch[107] Batch [5]#011Speed: 375.12 samples/sec#011loss=1.115863\n",
      "[05/22/2025 15:44:38 INFO 140473021069120] processed a total of 1272 examples\n",
      "#metrics {\"StartTime\": 1747928674.132592, \"EndTime\": 1747928678.7977116, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4664.7820472717285, \"count\": 1, \"min\": 4664.7820472717285, \"max\": 4664.7820472717285}}}\n",
      "[05/22/2025 15:44:38 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=272.67631004774125 records/second\n",
      "[05/22/2025 15:44:38 INFO 140473021069120] #progress_metric: host=algo-1, completed 27.0 % of epochs\n",
      "[05/22/2025 15:44:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=107, train loss <loss>=1.1238124132156373\n",
      "[05/22/2025 15:44:38 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:40 INFO 140473021069120] Epoch[108] Batch[0] avg_epoch_loss=0.924013\n",
      "[05/22/2025 15:44:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=0.9240131378173828\n",
      "[05/22/2025 15:44:42 INFO 140473021069120] Epoch[108] Batch[5] avg_epoch_loss=0.976477\n",
      "[05/22/2025 15:44:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=0.9764770964781443\n",
      "[05/22/2025 15:44:42 INFO 140473021069120] Epoch[108] Batch [5]#011Speed: 369.09 samples/sec#011loss=0.976477\n",
      "[05/22/2025 15:44:43 INFO 140473021069120] processed a total of 1280 examples\n",
      "#metrics {\"StartTime\": 1747928678.7977693, \"EndTime\": 1747928683.5157416, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4717.629432678223, \"count\": 1, \"min\": 4717.629432678223, \"max\": 4717.629432678223}}}\n",
      "[05/22/2025 15:44:43 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=271.3177458090533 records/second\n",
      "[05/22/2025 15:44:43 INFO 140473021069120] #progress_metric: host=algo-1, completed 27.25 % of epochs\n",
      "[05/22/2025 15:44:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=108, train loss <loss>=0.9342579483985901\n",
      "[05/22/2025 15:44:43 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:45 INFO 140473021069120] Epoch[109] Batch[0] avg_epoch_loss=0.932489\n",
      "[05/22/2025 15:44:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=0.9324891567230225\n",
      "[05/22/2025 15:44:46 INFO 140473021069120] Epoch[109] Batch[5] avg_epoch_loss=0.905916\n",
      "[05/22/2025 15:44:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=0.9059155583381653\n",
      "[05/22/2025 15:44:46 INFO 140473021069120] Epoch[109] Batch [5]#011Speed: 368.96 samples/sec#011loss=0.905916\n",
      "[05/22/2025 15:44:48 INFO 140473021069120] Epoch[109] Batch[10] avg_epoch_loss=0.964851\n",
      "[05/22/2025 15:44:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=1.0355738401412964\n",
      "[05/22/2025 15:44:48 INFO 140473021069120] Epoch[109] Batch [10]#011Speed: 364.47 samples/sec#011loss=1.035574\n",
      "[05/22/2025 15:44:48 INFO 140473021069120] processed a total of 1284 examples\n",
      "#metrics {\"StartTime\": 1747928683.5157976, \"EndTime\": 1747928688.552359, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5036.181688308716, \"count\": 1, \"min\": 5036.181688308716, \"max\": 5036.181688308716}}}\n",
      "[05/22/2025 15:44:48 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=254.95047263833152 records/second\n",
      "[05/22/2025 15:44:48 INFO 140473021069120] #progress_metric: host=algo-1, completed 27.5 % of epochs\n",
      "[05/22/2025 15:44:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=109, train loss <loss>=0.9648511409759521\n",
      "[05/22/2025 15:44:48 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:50 INFO 140473021069120] Epoch[110] Batch[0] avg_epoch_loss=1.046557\n",
      "[05/22/2025 15:44:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=1.0465574264526367\n",
      "[05/22/2025 15:44:51 INFO 140473021069120] Epoch[110] Batch[5] avg_epoch_loss=1.013820\n",
      "[05/22/2025 15:44:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=1.0138203501701355\n",
      "[05/22/2025 15:44:51 INFO 140473021069120] Epoch[110] Batch [5]#011Speed: 379.28 samples/sec#011loss=1.013820\n",
      "[05/22/2025 15:44:53 INFO 140473021069120] Epoch[110] Batch[10] avg_epoch_loss=0.844865\n",
      "[05/22/2025 15:44:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=0.642117977142334\n",
      "[05/22/2025 15:44:53 INFO 140473021069120] Epoch[110] Batch [10]#011Speed: 361.29 samples/sec#011loss=0.642118\n",
      "[05/22/2025 15:44:53 INFO 140473021069120] processed a total of 1334 examples\n",
      "#metrics {\"StartTime\": 1747928688.55242, \"EndTime\": 1747928693.5117247, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4959.05065536499, \"count\": 1, \"min\": 4959.05065536499, \"max\": 4959.05065536499}}}\n",
      "[05/22/2025 15:44:53 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.9985477967 records/second\n",
      "[05/22/2025 15:44:53 INFO 140473021069120] #progress_metric: host=algo-1, completed 27.75 % of epochs\n",
      "[05/22/2025 15:44:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=110, train loss <loss>=0.8448647260665894\n",
      "[05/22/2025 15:44:53 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:44:53 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_6c9f293f-10a9-4a91-8116-9ddce904158f-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928693.5117812, \"EndTime\": 1747928693.548464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 36.40580177307129, \"count\": 1, \"min\": 36.40580177307129, \"max\": 36.40580177307129}}}\n",
      "[05/22/2025 15:44:55 INFO 140473021069120] Epoch[111] Batch[0] avg_epoch_loss=1.266438\n",
      "[05/22/2025 15:44:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=1.2664376497268677\n",
      "[05/22/2025 15:44:56 INFO 140473021069120] Epoch[111] Batch[5] avg_epoch_loss=1.127783\n",
      "[05/22/2025 15:44:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=1.1277832289536793\n",
      "[05/22/2025 15:44:56 INFO 140473021069120] Epoch[111] Batch [5]#011Speed: 381.05 samples/sec#011loss=1.127783\n",
      "[05/22/2025 15:44:58 INFO 140473021069120] processed a total of 1269 examples\n",
      "#metrics {\"StartTime\": 1747928693.548523, \"EndTime\": 1747928698.1487057, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4600.128650665283, \"count\": 1, \"min\": 4600.128650665283, \"max\": 4600.128650665283}}}\n",
      "[05/22/2025 15:44:58 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=275.85628840459884 records/second\n",
      "[05/22/2025 15:44:58 INFO 140473021069120] #progress_metric: host=algo-1, completed 28.0 % of epochs\n",
      "[05/22/2025 15:44:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=111, train loss <loss>=1.161208838224411\n",
      "[05/22/2025 15:44:58 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:44:59 INFO 140473021069120] Epoch[112] Batch[0] avg_epoch_loss=1.162858\n",
      "[05/22/2025 15:44:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=1.162858486175537\n",
      "[05/22/2025 15:45:01 INFO 140473021069120] Epoch[112] Batch[5] avg_epoch_loss=1.116682\n",
      "[05/22/2025 15:45:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=1.1166815360387166\n",
      "[05/22/2025 15:45:01 INFO 140473021069120] Epoch[112] Batch [5]#011Speed: 370.45 samples/sec#011loss=1.116682\n",
      "[05/22/2025 15:45:02 INFO 140473021069120] processed a total of 1255 examples\n",
      "#metrics {\"StartTime\": 1747928698.1487687, \"EndTime\": 1747928702.7692995, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4620.168924331665, \"count\": 1, \"min\": 4620.168924331665, \"max\": 4620.168924331665}}}\n",
      "[05/22/2025 15:45:02 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=271.6294547726007 records/second\n",
      "[05/22/2025 15:45:02 INFO 140473021069120] #progress_metric: host=algo-1, completed 28.25 % of epochs\n",
      "[05/22/2025 15:45:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=112, train loss <loss>=1.100429767370224\n",
      "[05/22/2025 15:45:02 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:04 INFO 140473021069120] Epoch[113] Batch[0] avg_epoch_loss=1.057027\n",
      "[05/22/2025 15:45:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=1.0570274591445923\n",
      "[05/22/2025 15:45:06 INFO 140473021069120] Epoch[113] Batch[5] avg_epoch_loss=1.024586\n",
      "[05/22/2025 15:45:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=1.0245860815048218\n",
      "[05/22/2025 15:45:06 INFO 140473021069120] Epoch[113] Batch [5]#011Speed: 374.11 samples/sec#011loss=1.024586\n",
      "[05/22/2025 15:45:07 INFO 140473021069120] Epoch[113] Batch[10] avg_epoch_loss=1.026469\n",
      "[05/22/2025 15:45:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=1.0287292838096618\n",
      "[05/22/2025 15:45:07 INFO 140473021069120] Epoch[113] Batch [10]#011Speed: 365.09 samples/sec#011loss=1.028729\n",
      "[05/22/2025 15:45:07 INFO 140473021069120] processed a total of 1306 examples\n",
      "#metrics {\"StartTime\": 1747928702.7693639, \"EndTime\": 1747928707.76072, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4990.995168685913, \"count\": 1, \"min\": 4990.995168685913, \"max\": 4990.995168685913}}}\n",
      "[05/22/2025 15:45:07 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.6666737046194 records/second\n",
      "[05/22/2025 15:45:07 INFO 140473021069120] #progress_metric: host=algo-1, completed 28.5 % of epochs\n",
      "[05/22/2025 15:45:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=113, train loss <loss>=1.026469355279749\n",
      "[05/22/2025 15:45:07 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:09 INFO 140473021069120] Epoch[114] Batch[0] avg_epoch_loss=0.991377\n",
      "[05/22/2025 15:45:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=0.991377055644989\n",
      "[05/22/2025 15:45:10 INFO 140473021069120] Epoch[114] Batch[5] avg_epoch_loss=1.073754\n",
      "[05/22/2025 15:45:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=1.07375364502271\n",
      "[05/22/2025 15:45:10 INFO 140473021069120] Epoch[114] Batch [5]#011Speed: 375.87 samples/sec#011loss=1.073754\n",
      "[05/22/2025 15:45:12 INFO 140473021069120] processed a total of 1225 examples\n",
      "#metrics {\"StartTime\": 1747928707.7607787, \"EndTime\": 1747928712.3579767, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4596.931457519531, \"count\": 1, \"min\": 4596.931457519531, \"max\": 4596.931457519531}}}\n",
      "[05/22/2025 15:45:12 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=266.47495213792575 records/second\n",
      "[05/22/2025 15:45:12 INFO 140473021069120] #progress_metric: host=algo-1, completed 28.75 % of epochs\n",
      "[05/22/2025 15:45:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=114, train loss <loss>=1.095088404417038\n",
      "[05/22/2025 15:45:12 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:13 INFO 140473021069120] Epoch[115] Batch[0] avg_epoch_loss=0.977654\n",
      "[05/22/2025 15:45:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=0.9776535630226135\n",
      "[05/22/2025 15:45:15 INFO 140473021069120] Epoch[115] Batch[5] avg_epoch_loss=1.072839\n",
      "[05/22/2025 15:45:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=1.0728385051091511\n",
      "[05/22/2025 15:45:15 INFO 140473021069120] Epoch[115] Batch [5]#011Speed: 381.65 samples/sec#011loss=1.072839\n",
      "[05/22/2025 15:45:16 INFO 140473021069120] processed a total of 1233 examples\n",
      "#metrics {\"StartTime\": 1747928712.3580701, \"EndTime\": 1747928716.9280105, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4569.629907608032, \"count\": 1, \"min\": 4569.629907608032, \"max\": 4569.629907608032}}}\n",
      "[05/22/2025 15:45:16 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.8193399184309 records/second\n",
      "[05/22/2025 15:45:16 INFO 140473021069120] #progress_metric: host=algo-1, completed 29.0 % of epochs\n",
      "[05/22/2025 15:45:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=115, train loss <loss>=1.1151397943496704\n",
      "[05/22/2025 15:45:16 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:18 INFO 140473021069120] Epoch[116] Batch[0] avg_epoch_loss=0.920783\n",
      "[05/22/2025 15:45:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=0.9207825064659119\n",
      "[05/22/2025 15:45:20 INFO 140473021069120] Epoch[116] Batch[5] avg_epoch_loss=0.961273\n",
      "[05/22/2025 15:45:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=0.9612733423709869\n",
      "[05/22/2025 15:45:20 INFO 140473021069120] Epoch[116] Batch [5]#011Speed: 374.77 samples/sec#011loss=0.961273\n",
      "[05/22/2025 15:45:21 INFO 140473021069120] Epoch[116] Batch[10] avg_epoch_loss=1.019379\n",
      "[05/22/2025 15:45:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=1.0891067504882812\n",
      "[05/22/2025 15:45:21 INFO 140473021069120] Epoch[116] Batch [10]#011Speed: 379.30 samples/sec#011loss=1.089107\n",
      "[05/22/2025 15:45:21 INFO 140473021069120] processed a total of 1291 examples\n",
      "#metrics {\"StartTime\": 1747928716.928075, \"EndTime\": 1747928721.8664508, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4938.054323196411, \"count\": 1, \"min\": 4938.054323196411, \"max\": 4938.054323196411}}}\n",
      "[05/22/2025 15:45:21 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.4336512550016 records/second\n",
      "[05/22/2025 15:45:21 INFO 140473021069120] #progress_metric: host=algo-1, completed 29.25 % of epochs\n",
      "[05/22/2025 15:45:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=116, train loss <loss>=1.019379436969757\n",
      "[05/22/2025 15:45:21 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:23 INFO 140473021069120] Epoch[117] Batch[0] avg_epoch_loss=1.139837\n",
      "[05/22/2025 15:45:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=1.1398369073867798\n",
      "[05/22/2025 15:45:25 INFO 140473021069120] Epoch[117] Batch[5] avg_epoch_loss=1.065159\n",
      "[05/22/2025 15:45:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=1.065159281094869\n",
      "[05/22/2025 15:45:25 INFO 140473021069120] Epoch[117] Batch [5]#011Speed: 378.05 samples/sec#011loss=1.065159\n",
      "[05/22/2025 15:45:26 INFO 140473021069120] processed a total of 1239 examples\n",
      "#metrics {\"StartTime\": 1747928721.866523, \"EndTime\": 1747928726.4487834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4581.991672515869, \"count\": 1, \"min\": 4581.991672515869, \"max\": 4581.991672515869}}}\n",
      "[05/22/2025 15:45:26 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=270.40091228013944 records/second\n",
      "[05/22/2025 15:45:26 INFO 140473021069120] #progress_metric: host=algo-1, completed 29.5 % of epochs\n",
      "[05/22/2025 15:45:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=117, train loss <loss>=1.0439858555793762\n",
      "[05/22/2025 15:45:26 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:27 INFO 140473021069120] Epoch[118] Batch[0] avg_epoch_loss=1.092296\n",
      "[05/22/2025 15:45:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=1.0922961235046387\n",
      "[05/22/2025 15:45:29 INFO 140473021069120] Epoch[118] Batch[5] avg_epoch_loss=0.915073\n",
      "[05/22/2025 15:45:29 INFO 140473021069120] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=0.9150726795196533\n",
      "[05/22/2025 15:45:29 INFO 140473021069120] Epoch[118] Batch [5]#011Speed: 378.66 samples/sec#011loss=0.915073\n",
      "[05/22/2025 15:45:31 INFO 140473021069120] Epoch[118] Batch[10] avg_epoch_loss=1.005237\n",
      "[05/22/2025 15:45:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=1.1134350419044494\n",
      "[05/22/2025 15:45:31 INFO 140473021069120] Epoch[118] Batch [10]#011Speed: 379.27 samples/sec#011loss=1.113435\n",
      "[05/22/2025 15:45:31 INFO 140473021069120] processed a total of 1283 examples\n",
      "#metrics {\"StartTime\": 1747928726.4488475, \"EndTime\": 1747928731.3585103, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4909.317255020142, \"count\": 1, \"min\": 4909.317255020142, \"max\": 4909.317255020142}}}\n",
      "[05/22/2025 15:45:31 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.3347127975186 records/second\n",
      "[05/22/2025 15:45:31 INFO 140473021069120] #progress_metric: host=algo-1, completed 29.75 % of epochs\n",
      "[05/22/2025 15:45:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=118, train loss <loss>=1.0052373896945606\n",
      "[05/22/2025 15:45:31 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:32 INFO 140473021069120] Epoch[119] Batch[0] avg_epoch_loss=1.188796\n",
      "[05/22/2025 15:45:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=1.1887962818145752\n",
      "[05/22/2025 15:45:34 INFO 140473021069120] Epoch[119] Batch[5] avg_epoch_loss=1.028535\n",
      "[05/22/2025 15:45:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=1.0285354256629944\n",
      "[05/22/2025 15:45:34 INFO 140473021069120] Epoch[119] Batch [5]#011Speed: 381.82 samples/sec#011loss=1.028535\n",
      "[05/22/2025 15:45:36 INFO 140473021069120] Epoch[119] Batch[10] avg_epoch_loss=1.063400\n",
      "[05/22/2025 15:45:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=1.1052367687225342\n",
      "[05/22/2025 15:45:36 INFO 140473021069120] Epoch[119] Batch [10]#011Speed: 367.14 samples/sec#011loss=1.105237\n",
      "[05/22/2025 15:45:36 INFO 140473021069120] processed a total of 1377 examples\n",
      "#metrics {\"StartTime\": 1747928731.3585727, \"EndTime\": 1747928736.2381117, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4879.175424575806, \"count\": 1, \"min\": 4879.175424575806, \"max\": 4879.175424575806}}}\n",
      "[05/22/2025 15:45:36 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=282.21468794925534 records/second\n",
      "[05/22/2025 15:45:36 INFO 140473021069120] #progress_metric: host=algo-1, completed 30.0 % of epochs\n",
      "[05/22/2025 15:45:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=119, train loss <loss>=1.0633996725082397\n",
      "[05/22/2025 15:45:36 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:37 INFO 140473021069120] Epoch[120] Batch[0] avg_epoch_loss=0.812420\n",
      "[05/22/2025 15:45:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=0.8124202489852905\n",
      "[05/22/2025 15:45:39 INFO 140473021069120] Epoch[120] Batch[5] avg_epoch_loss=0.938153\n",
      "[05/22/2025 15:45:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=0.9381527801354727\n",
      "[05/22/2025 15:45:39 INFO 140473021069120] Epoch[120] Batch [5]#011Speed: 385.06 samples/sec#011loss=0.938153\n",
      "[05/22/2025 15:45:40 INFO 140473021069120] processed a total of 1270 examples\n",
      "#metrics {\"StartTime\": 1747928736.2381704, \"EndTime\": 1747928740.7645445, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4526.111364364624, \"count\": 1, \"min\": 4526.111364364624, \"max\": 4526.111364364624}}}\n",
      "[05/22/2025 15:45:40 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=280.5884219192217 records/second\n",
      "[05/22/2025 15:45:40 INFO 140473021069120] #progress_metric: host=algo-1, completed 30.25 % of epochs\n",
      "[05/22/2025 15:45:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=120, train loss <loss>=0.9708568632602692\n",
      "[05/22/2025 15:45:40 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:42 INFO 140473021069120] Epoch[121] Batch[0] avg_epoch_loss=0.961998\n",
      "[05/22/2025 15:45:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=0.9619984030723572\n",
      "[05/22/2025 15:45:43 INFO 140473021069120] Epoch[121] Batch[5] avg_epoch_loss=0.970122\n",
      "[05/22/2025 15:45:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=0.9701222280661265\n",
      "[05/22/2025 15:45:43 INFO 140473021069120] Epoch[121] Batch [5]#011Speed: 400.57 samples/sec#011loss=0.970122\n",
      "[05/22/2025 15:45:45 INFO 140473021069120] Epoch[121] Batch[10] avg_epoch_loss=1.055008\n",
      "[05/22/2025 15:45:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=1.1568707585334779\n",
      "[05/22/2025 15:45:45 INFO 140473021069120] Epoch[121] Batch [10]#011Speed: 388.60 samples/sec#011loss=1.156871\n",
      "[05/22/2025 15:45:45 INFO 140473021069120] processed a total of 1303 examples\n",
      "#metrics {\"StartTime\": 1747928740.7646067, \"EndTime\": 1747928745.4948208, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4729.922294616699, \"count\": 1, \"min\": 4729.922294616699, \"max\": 4729.922294616699}}}\n",
      "[05/22/2025 15:45:45 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=275.47506114565766 records/second\n",
      "[05/22/2025 15:45:45 INFO 140473021069120] #progress_metric: host=algo-1, completed 30.5 % of epochs\n",
      "[05/22/2025 15:45:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=121, train loss <loss>=1.0550079237331043\n",
      "[05/22/2025 15:45:45 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:46 INFO 140473021069120] Epoch[122] Batch[0] avg_epoch_loss=1.250586\n",
      "[05/22/2025 15:45:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=1.2505863904953003\n",
      "[05/22/2025 15:45:48 INFO 140473021069120] Epoch[122] Batch[5] avg_epoch_loss=1.068612\n",
      "[05/22/2025 15:45:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=1.0686119596163433\n",
      "[05/22/2025 15:45:48 INFO 140473021069120] Epoch[122] Batch [5]#011Speed: 407.40 samples/sec#011loss=1.068612\n",
      "[05/22/2025 15:45:50 INFO 140473021069120] Epoch[122] Batch[10] avg_epoch_loss=1.019367\n",
      "[05/22/2025 15:45:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=0.960274076461792\n",
      "[05/22/2025 15:45:50 INFO 140473021069120] Epoch[122] Batch [10]#011Speed: 383.34 samples/sec#011loss=0.960274\n",
      "[05/22/2025 15:45:50 INFO 140473021069120] processed a total of 1325 examples\n",
      "#metrics {\"StartTime\": 1747928745.4948804, \"EndTime\": 1747928750.2177024, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4722.556591033936, \"count\": 1, \"min\": 4722.556591033936, \"max\": 4722.556591033936}}}\n",
      "[05/22/2025 15:45:50 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=280.5630148621158 records/second\n",
      "[05/22/2025 15:45:50 INFO 140473021069120] #progress_metric: host=algo-1, completed 30.75 % of epochs\n",
      "[05/22/2025 15:45:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=122, train loss <loss>=1.0193674672733655\n",
      "[05/22/2025 15:45:50 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:51 INFO 140473021069120] Epoch[123] Batch[0] avg_epoch_loss=0.853446\n",
      "[05/22/2025 15:45:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=0.8534463047981262\n",
      "[05/22/2025 15:45:53 INFO 140473021069120] Epoch[123] Batch[5] avg_epoch_loss=0.942929\n",
      "[05/22/2025 15:45:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=0.9429286519686381\n",
      "[05/22/2025 15:45:53 INFO 140473021069120] Epoch[123] Batch [5]#011Speed: 403.49 samples/sec#011loss=0.942929\n",
      "[05/22/2025 15:45:54 INFO 140473021069120] processed a total of 1266 examples\n",
      "#metrics {\"StartTime\": 1747928750.2177637, \"EndTime\": 1747928754.597054, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4379.019737243652, \"count\": 1, \"min\": 4379.019737243652, \"max\": 4379.019737243652}}}\n",
      "[05/22/2025 15:45:54 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=289.0997390202438 records/second\n",
      "[05/22/2025 15:45:54 INFO 140473021069120] #progress_metric: host=algo-1, completed 31.0 % of epochs\n",
      "[05/22/2025 15:45:54 INFO 140473021069120] #quality_metric: host=algo-1, epoch=123, train loss <loss>=0.9551256418228149\n",
      "[05/22/2025 15:45:54 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:45:56 INFO 140473021069120] Epoch[124] Batch[0] avg_epoch_loss=1.137486\n",
      "[05/22/2025 15:45:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=1.137486457824707\n",
      "[05/22/2025 15:45:57 INFO 140473021069120] Epoch[124] Batch[5] avg_epoch_loss=1.000258\n",
      "[05/22/2025 15:45:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=1.0002577006816864\n",
      "[05/22/2025 15:45:57 INFO 140473021069120] Epoch[124] Batch [5]#011Speed: 404.23 samples/sec#011loss=1.000258\n",
      "[05/22/2025 15:45:58 INFO 140473021069120] processed a total of 1255 examples\n",
      "#metrics {\"StartTime\": 1747928754.5971167, \"EndTime\": 1747928758.9777133, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4380.25689125061, \"count\": 1, \"min\": 4380.25689125061, \"max\": 4380.25689125061}}}\n",
      "[05/22/2025 15:45:58 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=286.50670063042503 records/second\n",
      "[05/22/2025 15:45:58 INFO 140473021069120] #progress_metric: host=algo-1, completed 31.25 % of epochs\n",
      "[05/22/2025 15:45:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=124, train loss <loss>=1.0041990280151367\n",
      "[05/22/2025 15:45:58 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:00 INFO 140473021069120] Epoch[125] Batch[0] avg_epoch_loss=1.033393\n",
      "[05/22/2025 15:46:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=1.0333927869796753\n",
      "[05/22/2025 15:46:02 INFO 140473021069120] Epoch[125] Batch[5] avg_epoch_loss=1.082532\n",
      "[05/22/2025 15:46:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=1.082532246907552\n",
      "[05/22/2025 15:46:02 INFO 140473021069120] Epoch[125] Batch [5]#011Speed: 399.24 samples/sec#011loss=1.082532\n",
      "[05/22/2025 15:46:03 INFO 140473021069120] Epoch[125] Batch[10] avg_epoch_loss=1.036180\n",
      "[05/22/2025 15:46:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=0.9805572867393494\n",
      "[05/22/2025 15:46:03 INFO 140473021069120] Epoch[125] Batch [10]#011Speed: 394.92 samples/sec#011loss=0.980557\n",
      "[05/22/2025 15:46:03 INFO 140473021069120] processed a total of 1294 examples\n",
      "#metrics {\"StartTime\": 1747928758.977777, \"EndTime\": 1747928763.6960225, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4717.952251434326, \"count\": 1, \"min\": 4717.952251434326, \"max\": 4717.952251434326}}}\n",
      "[05/22/2025 15:46:03 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=274.26668275651434 records/second\n",
      "[05/22/2025 15:46:03 INFO 140473021069120] #progress_metric: host=algo-1, completed 31.5 % of epochs\n",
      "[05/22/2025 15:46:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=125, train loss <loss>=1.0361799922856418\n",
      "[05/22/2025 15:46:03 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:05 INFO 140473021069120] Epoch[126] Batch[0] avg_epoch_loss=1.106478\n",
      "[05/22/2025 15:46:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=1.1064780950546265\n",
      "[05/22/2025 15:46:06 INFO 140473021069120] Epoch[126] Batch[5] avg_epoch_loss=0.920196\n",
      "[05/22/2025 15:46:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=0.9201962848504385\n",
      "[05/22/2025 15:46:06 INFO 140473021069120] Epoch[126] Batch [5]#011Speed: 410.08 samples/sec#011loss=0.920196\n",
      "[05/22/2025 15:46:08 INFO 140473021069120] processed a total of 1228 examples\n",
      "#metrics {\"StartTime\": 1747928763.6960785, \"EndTime\": 1747928768.0476675, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4351.304292678833, \"count\": 1, \"min\": 4351.304292678833, \"max\": 4351.304292678833}}}\n",
      "[05/22/2025 15:46:08 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=282.2080625093556 records/second\n",
      "[05/22/2025 15:46:08 INFO 140473021069120] #progress_metric: host=algo-1, completed 31.75 % of epochs\n",
      "[05/22/2025 15:46:08 INFO 140473021069120] #quality_metric: host=algo-1, epoch=126, train loss <loss>=0.9497721791267395\n",
      "[05/22/2025 15:46:08 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:09 INFO 140473021069120] Epoch[127] Batch[0] avg_epoch_loss=0.952488\n",
      "[05/22/2025 15:46:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=0.952487587928772\n",
      "[05/22/2025 15:46:11 INFO 140473021069120] Epoch[127] Batch[5] avg_epoch_loss=0.958743\n",
      "[05/22/2025 15:46:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=0.9587430854638418\n",
      "[05/22/2025 15:46:11 INFO 140473021069120] Epoch[127] Batch [5]#011Speed: 398.33 samples/sec#011loss=0.958743\n",
      "[05/22/2025 15:46:12 INFO 140473021069120] processed a total of 1249 examples\n",
      "#metrics {\"StartTime\": 1747928768.0477326, \"EndTime\": 1747928772.451364, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4403.298616409302, \"count\": 1, \"min\": 4403.298616409302, \"max\": 4403.298616409302}}}\n",
      "[05/22/2025 15:46:12 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=283.6449208756994 records/second\n",
      "[05/22/2025 15:46:12 INFO 140473021069120] #progress_metric: host=algo-1, completed 32.0 % of epochs\n",
      "[05/22/2025 15:46:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=127, train loss <loss>=0.9663097858428955\n",
      "[05/22/2025 15:46:12 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:13 INFO 140473021069120] Epoch[128] Batch[0] avg_epoch_loss=0.889794\n",
      "[05/22/2025 15:46:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=0.8897942304611206\n",
      "[05/22/2025 15:46:15 INFO 140473021069120] Epoch[128] Batch[5] avg_epoch_loss=1.052202\n",
      "[05/22/2025 15:46:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=1.0522024234135945\n",
      "[05/22/2025 15:46:15 INFO 140473021069120] Epoch[128] Batch [5]#011Speed: 408.36 samples/sec#011loss=1.052202\n",
      "[05/22/2025 15:46:17 INFO 140473021069120] Epoch[128] Batch[10] avg_epoch_loss=0.973185\n",
      "[05/22/2025 15:46:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=0.878365159034729\n",
      "[05/22/2025 15:46:17 INFO 140473021069120] Epoch[128] Batch [10]#011Speed: 368.04 samples/sec#011loss=0.878365\n",
      "[05/22/2025 15:46:17 INFO 140473021069120] processed a total of 1398 examples\n",
      "#metrics {\"StartTime\": 1747928772.4514277, \"EndTime\": 1747928777.2015555, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4749.822616577148, \"count\": 1, \"min\": 4749.822616577148, \"max\": 4749.822616577148}}}\n",
      "[05/22/2025 15:46:17 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=294.3212702495807 records/second\n",
      "[05/22/2025 15:46:17 INFO 140473021069120] #progress_metric: host=algo-1, completed 32.25 % of epochs\n",
      "[05/22/2025 15:46:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=128, train loss <loss>=0.9731854850595648\n",
      "[05/22/2025 15:46:17 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:18 INFO 140473021069120] Epoch[129] Batch[0] avg_epoch_loss=0.857965\n",
      "[05/22/2025 15:46:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=0.8579646348953247\n",
      "[05/22/2025 15:46:20 INFO 140473021069120] Epoch[129] Batch[5] avg_epoch_loss=1.010578\n",
      "[05/22/2025 15:46:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=1.0105780760447185\n",
      "[05/22/2025 15:46:20 INFO 140473021069120] Epoch[129] Batch [5]#011Speed: 404.72 samples/sec#011loss=1.010578\n",
      "[05/22/2025 15:46:21 INFO 140473021069120] Epoch[129] Batch[10] avg_epoch_loss=0.973357\n",
      "[05/22/2025 15:46:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=129, batch=10 train loss <loss>=0.928691565990448\n",
      "[05/22/2025 15:46:21 INFO 140473021069120] Epoch[129] Batch [10]#011Speed: 383.96 samples/sec#011loss=0.928692\n",
      "[05/22/2025 15:46:21 INFO 140473021069120] processed a total of 1352 examples\n",
      "#metrics {\"StartTime\": 1747928777.2016146, \"EndTime\": 1747928781.9253132, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4723.423957824707, \"count\": 1, \"min\": 4723.423957824707, \"max\": 4723.423957824707}}}\n",
      "[05/22/2025 15:46:21 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=286.22768023597195 records/second\n",
      "[05/22/2025 15:46:21 INFO 140473021069120] #progress_metric: host=algo-1, completed 32.5 % of epochs\n",
      "[05/22/2025 15:46:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=129, train loss <loss>=0.9733569351109591\n",
      "[05/22/2025 15:46:21 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:23 INFO 140473021069120] Epoch[130] Batch[0] avg_epoch_loss=0.905479\n",
      "[05/22/2025 15:46:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=0.9054790139198303\n",
      "[05/22/2025 15:46:25 INFO 140473021069120] Epoch[130] Batch[5] avg_epoch_loss=0.824622\n",
      "[05/22/2025 15:46:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=0.8246216277281443\n",
      "[05/22/2025 15:46:25 INFO 140473021069120] Epoch[130] Batch [5]#011Speed: 387.93 samples/sec#011loss=0.824622\n",
      "[05/22/2025 15:46:26 INFO 140473021069120] Epoch[130] Batch[10] avg_epoch_loss=0.932315\n",
      "[05/22/2025 15:46:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=1.0615479528903962\n",
      "[05/22/2025 15:46:26 INFO 140473021069120] Epoch[130] Batch [10]#011Speed: 373.35 samples/sec#011loss=1.061548\n",
      "[05/22/2025 15:46:26 INFO 140473021069120] processed a total of 1321 examples\n",
      "#metrics {\"StartTime\": 1747928781.925373, \"EndTime\": 1747928786.78729, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4861.621141433716, \"count\": 1, \"min\": 4861.621141433716, \"max\": 4861.621141433716}}}\n",
      "[05/22/2025 15:46:26 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=271.71491890519303 records/second\n",
      "[05/22/2025 15:46:26 INFO 140473021069120] #progress_metric: host=algo-1, completed 32.75 % of epochs\n",
      "[05/22/2025 15:46:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=130, train loss <loss>=0.9323154118928042\n",
      "[05/22/2025 15:46:26 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:28 INFO 140473021069120] Epoch[131] Batch[0] avg_epoch_loss=1.125729\n",
      "[05/22/2025 15:46:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=1.125728726387024\n",
      "[05/22/2025 15:46:29 INFO 140473021069120] Epoch[131] Batch[5] avg_epoch_loss=0.918254\n",
      "[05/22/2025 15:46:29 INFO 140473021069120] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=0.9182535608609518\n",
      "[05/22/2025 15:46:29 INFO 140473021069120] Epoch[131] Batch [5]#011Speed: 390.05 samples/sec#011loss=0.918254\n",
      "[05/22/2025 15:46:31 INFO 140473021069120] processed a total of 1219 examples\n",
      "#metrics {\"StartTime\": 1747928786.787349, \"EndTime\": 1747928791.266231, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4478.60860824585, \"count\": 1, \"min\": 4478.60860824585, \"max\": 4478.60860824585}}}\n",
      "[05/22/2025 15:46:31 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=272.17714102498513 records/second\n",
      "[05/22/2025 15:46:31 INFO 140473021069120] #progress_metric: host=algo-1, completed 33.0 % of epochs\n",
      "[05/22/2025 15:46:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=131, train loss <loss>=0.8104404211044312\n",
      "[05/22/2025 15:46:31 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:46:31 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_344e5c79-6ccd-42b3-ad34-4321cf7f9738-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928791.2662935, \"EndTime\": 1747928791.3010416, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 34.3780517578125, \"count\": 1, \"min\": 34.3780517578125, \"max\": 34.3780517578125}}}\n",
      "[05/22/2025 15:46:32 INFO 140473021069120] Epoch[132] Batch[0] avg_epoch_loss=1.311468\n",
      "[05/22/2025 15:46:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=1.3114681243896484\n",
      "[05/22/2025 15:46:34 INFO 140473021069120] Epoch[132] Batch[5] avg_epoch_loss=1.041118\n",
      "[05/22/2025 15:46:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=1.041117916504542\n",
      "[05/22/2025 15:46:34 INFO 140473021069120] Epoch[132] Batch [5]#011Speed: 383.06 samples/sec#011loss=1.041118\n",
      "[05/22/2025 15:46:36 INFO 140473021069120] Epoch[132] Batch[10] avg_epoch_loss=1.013894\n",
      "[05/22/2025 15:46:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=0.9812259793281555\n",
      "[05/22/2025 15:46:36 INFO 140473021069120] Epoch[132] Batch [10]#011Speed: 372.52 samples/sec#011loss=0.981226\n",
      "[05/22/2025 15:46:36 INFO 140473021069120] processed a total of 1331 examples\n",
      "#metrics {\"StartTime\": 1747928791.301099, \"EndTime\": 1747928796.1818583, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4880.7053565979, \"count\": 1, \"min\": 4880.7053565979, \"max\": 4880.7053565979}}}\n",
      "[05/22/2025 15:46:36 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=272.70183544341205 records/second\n",
      "[05/22/2025 15:46:36 INFO 140473021069120] #progress_metric: host=algo-1, completed 33.25 % of epochs\n",
      "[05/22/2025 15:46:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=132, train loss <loss>=1.0138943086970935\n",
      "[05/22/2025 15:46:36 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:37 INFO 140473021069120] Epoch[133] Batch[0] avg_epoch_loss=1.138878\n",
      "[05/22/2025 15:46:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=1.1388781070709229\n",
      "[05/22/2025 15:46:39 INFO 140473021069120] Epoch[133] Batch[5] avg_epoch_loss=1.021480\n",
      "[05/22/2025 15:46:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=1.0214802622795105\n",
      "[05/22/2025 15:46:39 INFO 140473021069120] Epoch[133] Batch [5]#011Speed: 368.25 samples/sec#011loss=1.021480\n",
      "[05/22/2025 15:46:41 INFO 140473021069120] Epoch[133] Batch[10] avg_epoch_loss=0.991260\n",
      "[05/22/2025 15:46:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=0.9549953460693359\n",
      "[05/22/2025 15:46:41 INFO 140473021069120] Epoch[133] Batch [10]#011Speed: 367.56 samples/sec#011loss=0.954995\n",
      "[05/22/2025 15:46:41 INFO 140473021069120] processed a total of 1334 examples\n",
      "#metrics {\"StartTime\": 1747928796.1819112, \"EndTime\": 1747928801.1584764, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4976.256608963013, \"count\": 1, \"min\": 4976.256608963013, \"max\": 4976.256608963013}}}\n",
      "[05/22/2025 15:46:41 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.0683045058096 records/second\n",
      "[05/22/2025 15:46:41 INFO 140473021069120] #progress_metric: host=algo-1, completed 33.5 % of epochs\n",
      "[05/22/2025 15:46:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=133, train loss <loss>=0.9912598458203402\n",
      "[05/22/2025 15:46:41 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:42 INFO 140473021069120] Epoch[134] Batch[0] avg_epoch_loss=1.098221\n",
      "[05/22/2025 15:46:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=1.0982205867767334\n",
      "[05/22/2025 15:46:44 INFO 140473021069120] Epoch[134] Batch[5] avg_epoch_loss=0.985439\n",
      "[05/22/2025 15:46:44 INFO 140473021069120] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=0.9854385455449423\n",
      "[05/22/2025 15:46:44 INFO 140473021069120] Epoch[134] Batch [5]#011Speed: 377.51 samples/sec#011loss=0.985439\n",
      "[05/22/2025 15:46:46 INFO 140473021069120] Epoch[134] Batch[10] avg_epoch_loss=0.788255\n",
      "[05/22/2025 15:46:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=0.5516354441642761\n",
      "[05/22/2025 15:46:46 INFO 140473021069120] Epoch[134] Batch [10]#011Speed: 365.87 samples/sec#011loss=0.551635\n",
      "[05/22/2025 15:46:46 INFO 140473021069120] processed a total of 1301 examples\n",
      "#metrics {\"StartTime\": 1747928801.1585355, \"EndTime\": 1747928806.1227582, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4963.909387588501, \"count\": 1, \"min\": 4963.909387588501, \"max\": 4963.909387588501}}}\n",
      "[05/22/2025 15:46:46 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.087291642892 records/second\n",
      "[05/22/2025 15:46:46 INFO 140473021069120] #progress_metric: host=algo-1, completed 33.75 % of epochs\n",
      "[05/22/2025 15:46:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=134, train loss <loss>=0.7882553176446394\n",
      "[05/22/2025 15:46:46 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:46:46 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_24a23558-8190-4d08-baac-6745148889f9-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928806.1228151, \"EndTime\": 1747928806.1588175, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 35.58206558227539, \"count\": 1, \"min\": 35.58206558227539, \"max\": 35.58206558227539}}}\n",
      "[05/22/2025 15:46:47 INFO 140473021069120] Epoch[135] Batch[0] avg_epoch_loss=1.269989\n",
      "[05/22/2025 15:46:47 INFO 140473021069120] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=1.2699886560440063\n",
      "[05/22/2025 15:46:49 INFO 140473021069120] Epoch[135] Batch[5] avg_epoch_loss=1.336197\n",
      "[05/22/2025 15:46:49 INFO 140473021069120] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=1.3361974755922954\n",
      "[05/22/2025 15:46:49 INFO 140473021069120] Epoch[135] Batch [5]#011Speed: 379.30 samples/sec#011loss=1.336197\n",
      "[05/22/2025 15:46:50 INFO 140473021069120] processed a total of 1275 examples\n",
      "#metrics {\"StartTime\": 1747928806.1588757, \"EndTime\": 1747928810.7375476, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4578.616380691528, \"count\": 1, \"min\": 4578.616380691528, \"max\": 4578.616380691528}}}\n",
      "[05/22/2025 15:46:50 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=278.4633587921804 records/second\n",
      "[05/22/2025 15:46:50 INFO 140473021069120] #progress_metric: host=algo-1, completed 34.0 % of epochs\n",
      "[05/22/2025 15:46:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=135, train loss <loss>=1.3268482685089111\n",
      "[05/22/2025 15:46:50 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:52 INFO 140473021069120] Epoch[136] Batch[0] avg_epoch_loss=1.199377\n",
      "[05/22/2025 15:46:52 INFO 140473021069120] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=1.1993772983551025\n",
      "[05/22/2025 15:46:53 INFO 140473021069120] Epoch[136] Batch[5] avg_epoch_loss=1.241057\n",
      "[05/22/2025 15:46:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=1.2410573959350586\n",
      "[05/22/2025 15:46:53 INFO 140473021069120] Epoch[136] Batch [5]#011Speed: 376.85 samples/sec#011loss=1.241057\n",
      "[05/22/2025 15:46:55 INFO 140473021069120] processed a total of 1279 examples\n",
      "#metrics {\"StartTime\": 1747928810.7376027, \"EndTime\": 1747928815.3627248, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4624.785423278809, \"count\": 1, \"min\": 4624.785423278809, \"max\": 4624.785423278809}}}\n",
      "[05/22/2025 15:46:55 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=276.54752598776605 records/second\n",
      "[05/22/2025 15:46:55 INFO 140473021069120] #progress_metric: host=algo-1, completed 34.25 % of epochs\n",
      "[05/22/2025 15:46:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=136, train loss <loss>=1.2121171712875367\n",
      "[05/22/2025 15:46:55 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:46:56 INFO 140473021069120] Epoch[137] Batch[0] avg_epoch_loss=1.218561\n",
      "[05/22/2025 15:46:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=1.2185605764389038\n",
      "[05/22/2025 15:46:58 INFO 140473021069120] Epoch[137] Batch[5] avg_epoch_loss=1.132387\n",
      "[05/22/2025 15:46:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=1.1323865056037903\n",
      "[05/22/2025 15:46:58 INFO 140473021069120] Epoch[137] Batch [5]#011Speed: 378.29 samples/sec#011loss=1.132387\n",
      "[05/22/2025 15:47:00 INFO 140473021069120] processed a total of 1277 examples\n",
      "#metrics {\"StartTime\": 1747928815.3627915, \"EndTime\": 1747928820.0031972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4640.108585357666, \"count\": 1, \"min\": 4640.108585357666, \"max\": 4640.108585357666}}}\n",
      "[05/22/2025 15:47:00 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=275.20359026377713 records/second\n",
      "[05/22/2025 15:47:00 INFO 140473021069120] #progress_metric: host=algo-1, completed 34.5 % of epochs\n",
      "[05/22/2025 15:47:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=137, train loss <loss>=1.1188019275665284\n",
      "[05/22/2025 15:47:00 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:01 INFO 140473021069120] Epoch[138] Batch[0] avg_epoch_loss=0.855229\n",
      "[05/22/2025 15:47:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=0.8552286028862\n",
      "[05/22/2025 15:47:03 INFO 140473021069120] Epoch[138] Batch[5] avg_epoch_loss=1.082742\n",
      "[05/22/2025 15:47:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=1.0827424228191376\n",
      "[05/22/2025 15:47:03 INFO 140473021069120] Epoch[138] Batch [5]#011Speed: 373.86 samples/sec#011loss=1.082742\n",
      "[05/22/2025 15:47:04 INFO 140473021069120] processed a total of 1228 examples\n",
      "#metrics {\"StartTime\": 1747928820.0032604, \"EndTime\": 1747928824.6511753, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4647.622108459473, \"count\": 1, \"min\": 4647.622108459473, \"max\": 4647.622108459473}}}\n",
      "[05/22/2025 15:47:04 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=264.21585139579344 records/second\n",
      "[05/22/2025 15:47:04 INFO 140473021069120] #progress_metric: host=algo-1, completed 34.75 % of epochs\n",
      "[05/22/2025 15:47:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=138, train loss <loss>=1.0349555909633636\n",
      "[05/22/2025 15:47:04 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:06 INFO 140473021069120] Epoch[139] Batch[0] avg_epoch_loss=1.012142\n",
      "[05/22/2025 15:47:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=1.0121424198150635\n",
      "[05/22/2025 15:47:07 INFO 140473021069120] Epoch[139] Batch[5] avg_epoch_loss=0.951697\n",
      "[05/22/2025 15:47:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=0.9516971409320831\n",
      "[05/22/2025 15:47:07 INFO 140473021069120] Epoch[139] Batch [5]#011Speed: 381.16 samples/sec#011loss=0.951697\n",
      "[05/22/2025 15:47:09 INFO 140473021069120] processed a total of 1279 examples\n",
      "#metrics {\"StartTime\": 1747928824.651239, \"EndTime\": 1747928829.258389, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4606.8596839904785, \"count\": 1, \"min\": 4606.8596839904785, \"max\": 4606.8596839904785}}}\n",
      "[05/22/2025 15:47:09 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=277.62318856480255 records/second\n",
      "[05/22/2025 15:47:09 INFO 140473021069120] #progress_metric: host=algo-1, completed 35.0 % of epochs\n",
      "[05/22/2025 15:47:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=139, train loss <loss>=0.9367577135562897\n",
      "[05/22/2025 15:47:09 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:10 INFO 140473021069120] Epoch[140] Batch[0] avg_epoch_loss=0.932171\n",
      "[05/22/2025 15:47:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=0.932171106338501\n",
      "[05/22/2025 15:47:12 INFO 140473021069120] Epoch[140] Batch[5] avg_epoch_loss=1.017336\n",
      "[05/22/2025 15:47:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=1.0173361599445343\n",
      "[05/22/2025 15:47:12 INFO 140473021069120] Epoch[140] Batch [5]#011Speed: 382.45 samples/sec#011loss=1.017336\n",
      "[05/22/2025 15:47:14 INFO 140473021069120] Epoch[140] Batch[10] avg_epoch_loss=1.138380\n",
      "[05/22/2025 15:47:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=1.28363196849823\n",
      "[05/22/2025 15:47:14 INFO 140473021069120] Epoch[140] Batch [10]#011Speed: 359.32 samples/sec#011loss=1.283632\n",
      "[05/22/2025 15:47:14 INFO 140473021069120] processed a total of 1298 examples\n",
      "#metrics {\"StartTime\": 1747928829.258461, \"EndTime\": 1747928834.2429569, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4984.050273895264, \"count\": 1, \"min\": 4984.050273895264, \"max\": 4984.050273895264}}}\n",
      "[05/22/2025 15:47:14 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.4259885132771 records/second\n",
      "[05/22/2025 15:47:14 INFO 140473021069120] #progress_metric: host=algo-1, completed 35.25 % of epochs\n",
      "[05/22/2025 15:47:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=140, train loss <loss>=1.1383797092871233\n",
      "[05/22/2025 15:47:14 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:15 INFO 140473021069120] Epoch[141] Batch[0] avg_epoch_loss=1.070809\n",
      "[05/22/2025 15:47:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=1.070809245109558\n",
      "[05/22/2025 15:47:17 INFO 140473021069120] Epoch[141] Batch[5] avg_epoch_loss=0.939269\n",
      "[05/22/2025 15:47:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=0.9392691155274709\n",
      "[05/22/2025 15:47:17 INFO 140473021069120] Epoch[141] Batch [5]#011Speed: 367.66 samples/sec#011loss=0.939269\n",
      "[05/22/2025 15:47:19 INFO 140473021069120] Epoch[141] Batch[10] avg_epoch_loss=0.915800\n",
      "[05/22/2025 15:47:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=0.887636935710907\n",
      "[05/22/2025 15:47:19 INFO 140473021069120] Epoch[141] Batch [10]#011Speed: 357.06 samples/sec#011loss=0.887637\n",
      "[05/22/2025 15:47:19 INFO 140473021069120] processed a total of 1289 examples\n",
      "#metrics {\"StartTime\": 1747928834.2430174, \"EndTime\": 1747928839.3186133, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5075.331687927246, \"count\": 1, \"min\": 5075.331687927246, \"max\": 5075.331687927246}}}\n",
      "[05/22/2025 15:47:19 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.96903907024577 records/second\n",
      "[05/22/2025 15:47:19 INFO 140473021069120] #progress_metric: host=algo-1, completed 35.5 % of epochs\n",
      "[05/22/2025 15:47:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=141, train loss <loss>=0.9157999428835782\n",
      "[05/22/2025 15:47:19 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:20 INFO 140473021069120] Epoch[142] Batch[0] avg_epoch_loss=1.100899\n",
      "[05/22/2025 15:47:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=1.1008985042572021\n",
      "[05/22/2025 15:47:22 INFO 140473021069120] Epoch[142] Batch[5] avg_epoch_loss=1.000882\n",
      "[05/22/2025 15:47:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=1.0008817116419475\n",
      "[05/22/2025 15:47:22 INFO 140473021069120] Epoch[142] Batch [5]#011Speed: 370.21 samples/sec#011loss=1.000882\n",
      "[05/22/2025 15:47:24 INFO 140473021069120] Epoch[142] Batch[10] avg_epoch_loss=1.020973\n",
      "[05/22/2025 15:47:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=142, batch=10 train loss <loss>=1.0450826406478881\n",
      "[05/22/2025 15:47:24 INFO 140473021069120] Epoch[142] Batch [10]#011Speed: 366.08 samples/sec#011loss=1.045083\n",
      "[05/22/2025 15:47:24 INFO 140473021069120] processed a total of 1282 examples\n",
      "#metrics {\"StartTime\": 1747928839.3186738, \"EndTime\": 1747928844.3483918, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5029.44540977478, \"count\": 1, \"min\": 5029.44540977478, \"max\": 5029.44540977478}}}\n",
      "[05/22/2025 15:47:24 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=254.89461425100933 records/second\n",
      "[05/22/2025 15:47:24 INFO 140473021069120] #progress_metric: host=algo-1, completed 35.75 % of epochs\n",
      "[05/22/2025 15:47:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=142, train loss <loss>=1.0209730430082842\n",
      "[05/22/2025 15:47:24 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:25 INFO 140473021069120] Epoch[143] Batch[0] avg_epoch_loss=0.948298\n",
      "[05/22/2025 15:47:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=0.948297917842865\n",
      "[05/22/2025 15:47:27 INFO 140473021069120] Epoch[143] Batch[5] avg_epoch_loss=1.020002\n",
      "[05/22/2025 15:47:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=1.0200021366278331\n",
      "[05/22/2025 15:47:27 INFO 140473021069120] Epoch[143] Batch [5]#011Speed: 364.96 samples/sec#011loss=1.020002\n",
      "[05/22/2025 15:47:29 INFO 140473021069120] Epoch[143] Batch[10] avg_epoch_loss=1.070052\n",
      "[05/22/2025 15:47:29 INFO 140473021069120] #quality_metric: host=algo-1, epoch=143, batch=10 train loss <loss>=1.1301121830940246\n",
      "[05/22/2025 15:47:29 INFO 140473021069120] Epoch[143] Batch [10]#011Speed: 361.81 samples/sec#011loss=1.130112\n",
      "[05/22/2025 15:47:29 INFO 140473021069120] processed a total of 1309 examples\n",
      "#metrics {\"StartTime\": 1747928844.3484485, \"EndTime\": 1747928849.4651408, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5116.441249847412, \"count\": 1, \"min\": 5116.441249847412, \"max\": 5116.441249847412}}}\n",
      "[05/22/2025 15:47:29 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=255.83768176559624 records/second\n",
      "[05/22/2025 15:47:29 INFO 140473021069120] #progress_metric: host=algo-1, completed 36.0 % of epochs\n",
      "[05/22/2025 15:47:29 INFO 140473021069120] #quality_metric: host=algo-1, epoch=143, train loss <loss>=1.0700521577488293\n",
      "[05/22/2025 15:47:29 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:31 INFO 140473021069120] Epoch[144] Batch[0] avg_epoch_loss=1.067840\n",
      "[05/22/2025 15:47:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=1.067840337753296\n",
      "[05/22/2025 15:47:32 INFO 140473021069120] Epoch[144] Batch[5] avg_epoch_loss=1.025502\n",
      "[05/22/2025 15:47:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=1.0255024532477062\n",
      "[05/22/2025 15:47:32 INFO 140473021069120] Epoch[144] Batch [5]#011Speed: 365.93 samples/sec#011loss=1.025502\n",
      "[05/22/2025 15:47:34 INFO 140473021069120] Epoch[144] Batch[10] avg_epoch_loss=1.134054\n",
      "[05/22/2025 15:47:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=1.2643155217170716\n",
      "[05/22/2025 15:47:34 INFO 140473021069120] Epoch[144] Batch [10]#011Speed: 361.53 samples/sec#011loss=1.264316\n",
      "[05/22/2025 15:47:34 INFO 140473021069120] processed a total of 1300 examples\n",
      "#metrics {\"StartTime\": 1747928849.465197, \"EndTime\": 1747928854.5329661, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5067.516326904297, \"count\": 1, \"min\": 5067.516326904297, \"max\": 5067.516326904297}}}\n",
      "[05/22/2025 15:47:34 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.53114781864116 records/second\n",
      "[05/22/2025 15:47:34 INFO 140473021069120] #progress_metric: host=algo-1, completed 36.25 % of epochs\n",
      "[05/22/2025 15:47:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=144, train loss <loss>=1.1340538480065085\n",
      "[05/22/2025 15:47:34 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:36 INFO 140473021069120] Epoch[145] Batch[0] avg_epoch_loss=0.797184\n",
      "[05/22/2025 15:47:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=0.7971840500831604\n",
      "[05/22/2025 15:47:37 INFO 140473021069120] Epoch[145] Batch[5] avg_epoch_loss=0.870301\n",
      "[05/22/2025 15:47:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=0.8703014850616455\n",
      "[05/22/2025 15:47:37 INFO 140473021069120] Epoch[145] Batch [5]#011Speed: 367.09 samples/sec#011loss=0.870301\n",
      "[05/22/2025 15:47:39 INFO 140473021069120] processed a total of 1214 examples\n",
      "#metrics {\"StartTime\": 1747928854.533028, \"EndTime\": 1747928859.2425542, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4709.148406982422, \"count\": 1, \"min\": 4709.148406982422, \"max\": 4709.148406982422}}}\n",
      "[05/22/2025 15:47:39 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=257.7902982491482 records/second\n",
      "[05/22/2025 15:47:39 INFO 140473021069120] #progress_metric: host=algo-1, completed 36.5 % of epochs\n",
      "[05/22/2025 15:47:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=145, train loss <loss>=0.871818745136261\n",
      "[05/22/2025 15:47:39 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:40 INFO 140473021069120] Epoch[146] Batch[0] avg_epoch_loss=1.118982\n",
      "[05/22/2025 15:47:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=1.1189823150634766\n",
      "[05/22/2025 15:47:42 INFO 140473021069120] Epoch[146] Batch[5] avg_epoch_loss=0.935679\n",
      "[05/22/2025 15:47:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=0.935679296652476\n",
      "[05/22/2025 15:47:42 INFO 140473021069120] Epoch[146] Batch [5]#011Speed: 370.86 samples/sec#011loss=0.935679\n",
      "[05/22/2025 15:47:43 INFO 140473021069120] processed a total of 1267 examples\n",
      "#metrics {\"StartTime\": 1747928859.242626, \"EndTime\": 1747928863.9448168, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4701.8139362335205, \"count\": 1, \"min\": 4701.8139362335205, \"max\": 4701.8139362335205}}}\n",
      "[05/22/2025 15:47:43 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.4649063720321 records/second\n",
      "[05/22/2025 15:47:43 INFO 140473021069120] #progress_metric: host=algo-1, completed 36.75 % of epochs\n",
      "[05/22/2025 15:47:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=146, train loss <loss>=0.9241920113563538\n",
      "[05/22/2025 15:47:43 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:45 INFO 140473021069120] Epoch[147] Batch[0] avg_epoch_loss=1.046063\n",
      "[05/22/2025 15:47:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=1.046062707901001\n",
      "[05/22/2025 15:47:47 INFO 140473021069120] Epoch[147] Batch[5] avg_epoch_loss=0.929870\n",
      "[05/22/2025 15:47:47 INFO 140473021069120] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=0.9298695425192515\n",
      "[05/22/2025 15:47:47 INFO 140473021069120] Epoch[147] Batch [5]#011Speed: 368.09 samples/sec#011loss=0.929870\n",
      "[05/22/2025 15:47:48 INFO 140473021069120] processed a total of 1236 examples\n",
      "#metrics {\"StartTime\": 1747928863.9448824, \"EndTime\": 1747928868.6779764, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4732.797145843506, \"count\": 1, \"min\": 4732.797145843506, \"max\": 4732.797145843506}}}\n",
      "[05/22/2025 15:47:48 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.1505161417429 records/second\n",
      "[05/22/2025 15:47:48 INFO 140473021069120] #progress_metric: host=algo-1, completed 37.0 % of epochs\n",
      "[05/22/2025 15:47:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=147, train loss <loss>=0.9301648199558258\n",
      "[05/22/2025 15:47:48 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:50 INFO 140473021069120] Epoch[148] Batch[0] avg_epoch_loss=0.796711\n",
      "[05/22/2025 15:47:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=0.7967113852500916\n",
      "[05/22/2025 15:47:51 INFO 140473021069120] Epoch[148] Batch[5] avg_epoch_loss=0.844466\n",
      "[05/22/2025 15:47:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=0.8444655338923136\n",
      "[05/22/2025 15:47:51 INFO 140473021069120] Epoch[148] Batch [5]#011Speed: 367.55 samples/sec#011loss=0.844466\n",
      "[05/22/2025 15:47:53 INFO 140473021069120] Epoch[148] Batch[10] avg_epoch_loss=0.953685\n",
      "[05/22/2025 15:47:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=1.0847484111785888\n",
      "[05/22/2025 15:47:53 INFO 140473021069120] Epoch[148] Batch [10]#011Speed: 350.71 samples/sec#011loss=1.084748\n",
      "[05/22/2025 15:47:53 INFO 140473021069120] processed a total of 1334 examples\n",
      "#metrics {\"StartTime\": 1747928868.6780522, \"EndTime\": 1747928873.754757, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5076.38955116272, \"count\": 1, \"min\": 5076.38955116272, \"max\": 5076.38955116272}}}\n",
      "[05/22/2025 15:47:53 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.7798227683212 records/second\n",
      "[05/22/2025 15:47:53 INFO 140473021069120] #progress_metric: host=algo-1, completed 37.25 % of epochs\n",
      "[05/22/2025 15:47:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=148, train loss <loss>=0.9536850235678933\n",
      "[05/22/2025 15:47:53 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:47:55 INFO 140473021069120] Epoch[149] Batch[0] avg_epoch_loss=1.050461\n",
      "[05/22/2025 15:47:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=1.050460934638977\n",
      "[05/22/2025 15:47:57 INFO 140473021069120] Epoch[149] Batch[5] avg_epoch_loss=0.993618\n",
      "[05/22/2025 15:47:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=0.9936176339785258\n",
      "[05/22/2025 15:47:57 INFO 140473021069120] Epoch[149] Batch [5]#011Speed: 368.80 samples/sec#011loss=0.993618\n",
      "[05/22/2025 15:47:58 INFO 140473021069120] Epoch[149] Batch[10] avg_epoch_loss=1.034184\n",
      "[05/22/2025 15:47:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=1.0828631281852723\n",
      "[05/22/2025 15:47:58 INFO 140473021069120] Epoch[149] Batch [10]#011Speed: 361.64 samples/sec#011loss=1.082863\n",
      "[05/22/2025 15:47:58 INFO 140473021069120] processed a total of 1306 examples\n",
      "#metrics {\"StartTime\": 1747928873.754817, \"EndTime\": 1747928878.798423, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5043.352127075195, \"count\": 1, \"min\": 5043.352127075195, \"max\": 5043.352127075195}}}\n",
      "[05/22/2025 15:47:58 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=258.95035741010173 records/second\n",
      "[05/22/2025 15:47:58 INFO 140473021069120] #progress_metric: host=algo-1, completed 37.5 % of epochs\n",
      "[05/22/2025 15:47:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=149, train loss <loss>=1.034183767708865\n",
      "[05/22/2025 15:47:58 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:00 INFO 140473021069120] Epoch[150] Batch[0] avg_epoch_loss=1.206737\n",
      "[05/22/2025 15:48:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=1.2067372798919678\n",
      "[05/22/2025 15:48:02 INFO 140473021069120] Epoch[150] Batch[5] avg_epoch_loss=1.023881\n",
      "[05/22/2025 15:48:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=1.0238811671733856\n",
      "[05/22/2025 15:48:02 INFO 140473021069120] Epoch[150] Batch [5]#011Speed: 358.71 samples/sec#011loss=1.023881\n",
      "[05/22/2025 15:48:03 INFO 140473021069120] Epoch[150] Batch[10] avg_epoch_loss=0.992057\n",
      "[05/22/2025 15:48:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=0.9538679957389832\n",
      "[05/22/2025 15:48:03 INFO 140473021069120] Epoch[150] Batch [10]#011Speed: 348.38 samples/sec#011loss=0.953868\n",
      "[05/22/2025 15:48:03 INFO 140473021069120] processed a total of 1337 examples\n",
      "#metrics {\"StartTime\": 1747928878.798482, \"EndTime\": 1747928883.929146, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5130.387306213379, \"count\": 1, \"min\": 5130.387306213379, \"max\": 5130.387306213379}}}\n",
      "[05/22/2025 15:48:03 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.59975882442683 records/second\n",
      "[05/22/2025 15:48:03 INFO 140473021069120] #progress_metric: host=algo-1, completed 37.75 % of epochs\n",
      "[05/22/2025 15:48:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=150, train loss <loss>=0.9920569983395663\n",
      "[05/22/2025 15:48:03 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:05 INFO 140473021069120] Epoch[151] Batch[0] avg_epoch_loss=0.992564\n",
      "[05/22/2025 15:48:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=0.9925637245178223\n",
      "[05/22/2025 15:48:07 INFO 140473021069120] Epoch[151] Batch[5] avg_epoch_loss=0.967451\n",
      "[05/22/2025 15:48:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=0.9674513638019562\n",
      "[05/22/2025 15:48:07 INFO 140473021069120] Epoch[151] Batch [5]#011Speed: 369.75 samples/sec#011loss=0.967451\n",
      "[05/22/2025 15:48:09 INFO 140473021069120] Epoch[151] Batch[10] avg_epoch_loss=0.922905\n",
      "[05/22/2025 15:48:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=0.8694502592086792\n",
      "[05/22/2025 15:48:09 INFO 140473021069120] Epoch[151] Batch [10]#011Speed: 351.13 samples/sec#011loss=0.869450\n",
      "[05/22/2025 15:48:09 INFO 140473021069120] processed a total of 1283 examples\n",
      "#metrics {\"StartTime\": 1747928883.9292042, \"EndTime\": 1747928889.0373104, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5107.848167419434, \"count\": 1, \"min\": 5107.848167419434, \"max\": 5107.848167419434}}}\n",
      "[05/22/2025 15:48:09 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=251.17576326963155 records/second\n",
      "[05/22/2025 15:48:09 INFO 140473021069120] #progress_metric: host=algo-1, completed 38.0 % of epochs\n",
      "[05/22/2025 15:48:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=151, train loss <loss>=0.9229054071686484\n",
      "[05/22/2025 15:48:09 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:10 INFO 140473021069120] Epoch[152] Batch[0] avg_epoch_loss=1.047453\n",
      "[05/22/2025 15:48:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=1.0474529266357422\n",
      "[05/22/2025 15:48:12 INFO 140473021069120] Epoch[152] Batch[5] avg_epoch_loss=0.936958\n",
      "[05/22/2025 15:48:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=0.9369580646355947\n",
      "[05/22/2025 15:48:12 INFO 140473021069120] Epoch[152] Batch [5]#011Speed: 368.77 samples/sec#011loss=0.936958\n",
      "[05/22/2025 15:48:14 INFO 140473021069120] Epoch[152] Batch[10] avg_epoch_loss=1.035071\n",
      "[05/22/2025 15:48:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=1.152806544303894\n",
      "[05/22/2025 15:48:14 INFO 140473021069120] Epoch[152] Batch [10]#011Speed: 361.20 samples/sec#011loss=1.152807\n",
      "[05/22/2025 15:48:14 INFO 140473021069120] processed a total of 1283 examples\n",
      "#metrics {\"StartTime\": 1747928889.037411, \"EndTime\": 1747928894.0973294, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5059.631586074829, \"count\": 1, \"min\": 5059.631586074829, \"max\": 5059.631586074829}}}\n",
      "[05/22/2025 15:48:14 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.5714494325345 records/second\n",
      "[05/22/2025 15:48:14 INFO 140473021069120] #progress_metric: host=algo-1, completed 38.25 % of epochs\n",
      "[05/22/2025 15:48:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=152, train loss <loss>=1.0350710099393672\n",
      "[05/22/2025 15:48:14 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:15 INFO 140473021069120] Epoch[153] Batch[0] avg_epoch_loss=0.805924\n",
      "[05/22/2025 15:48:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=0.8059238791465759\n",
      "[05/22/2025 15:48:17 INFO 140473021069120] Epoch[153] Batch[5] avg_epoch_loss=0.928888\n",
      "[05/22/2025 15:48:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=0.9288875361283621\n",
      "[05/22/2025 15:48:17 INFO 140473021069120] Epoch[153] Batch [5]#011Speed: 361.28 samples/sec#011loss=0.928888\n",
      "[05/22/2025 15:48:18 INFO 140473021069120] processed a total of 1263 examples\n",
      "#metrics {\"StartTime\": 1747928894.0973883, \"EndTime\": 1747928898.833343, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4735.703706741333, \"count\": 1, \"min\": 4735.703706741333, \"max\": 4735.703706741333}}}\n",
      "[05/22/2025 15:48:18 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=266.69196372743664 records/second\n",
      "[05/22/2025 15:48:18 INFO 140473021069120] #progress_metric: host=algo-1, completed 38.5 % of epochs\n",
      "[05/22/2025 15:48:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=153, train loss <loss>=0.9174858152866363\n",
      "[05/22/2025 15:48:18 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:20 INFO 140473021069120] Epoch[154] Batch[0] avg_epoch_loss=0.859225\n",
      "[05/22/2025 15:48:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=0.8592250347137451\n",
      "[05/22/2025 15:48:22 INFO 140473021069120] Epoch[154] Batch[5] avg_epoch_loss=0.897647\n",
      "[05/22/2025 15:48:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=0.8976467251777649\n",
      "[05/22/2025 15:48:22 INFO 140473021069120] Epoch[154] Batch [5]#011Speed: 372.92 samples/sec#011loss=0.897647\n",
      "[05/22/2025 15:48:23 INFO 140473021069120] Epoch[154] Batch[10] avg_epoch_loss=0.916789\n",
      "[05/22/2025 15:48:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=0.9397598028182983\n",
      "[05/22/2025 15:48:23 INFO 140473021069120] Epoch[154] Batch [10]#011Speed: 356.63 samples/sec#011loss=0.939760\n",
      "[05/22/2025 15:48:23 INFO 140473021069120] processed a total of 1307 examples\n",
      "#metrics {\"StartTime\": 1747928898.8334095, \"EndTime\": 1747928903.8780305, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5044.2774295806885, \"count\": 1, \"min\": 5044.2774295806885, \"max\": 5044.2774295806885}}}\n",
      "[05/22/2025 15:48:23 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.100743320811 records/second\n",
      "[05/22/2025 15:48:23 INFO 140473021069120] #progress_metric: host=algo-1, completed 38.75 % of epochs\n",
      "[05/22/2025 15:48:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=154, train loss <loss>=0.9167890331961892\n",
      "[05/22/2025 15:48:23 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:25 INFO 140473021069120] Epoch[155] Batch[0] avg_epoch_loss=0.629179\n",
      "[05/22/2025 15:48:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=0.629178524017334\n",
      "[05/22/2025 15:48:27 INFO 140473021069120] Epoch[155] Batch[5] avg_epoch_loss=0.849107\n",
      "[05/22/2025 15:48:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=0.8491072456041971\n",
      "[05/22/2025 15:48:27 INFO 140473021069120] Epoch[155] Batch [5]#011Speed: 372.46 samples/sec#011loss=0.849107\n",
      "[05/22/2025 15:48:28 INFO 140473021069120] Epoch[155] Batch[10] avg_epoch_loss=0.780685\n",
      "[05/22/2025 15:48:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=0.6985784858465195\n",
      "[05/22/2025 15:48:28 INFO 140473021069120] Epoch[155] Batch [10]#011Speed: 339.71 samples/sec#011loss=0.698578\n",
      "[05/22/2025 15:48:28 INFO 140473021069120] processed a total of 1354 examples\n",
      "#metrics {\"StartTime\": 1747928903.8780916, \"EndTime\": 1747928908.9790823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5100.444793701172, \"count\": 1, \"min\": 5100.444793701172, \"max\": 5100.444793701172}}}\n",
      "[05/22/2025 15:48:28 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=265.46318432083353 records/second\n",
      "[05/22/2025 15:48:28 INFO 140473021069120] #progress_metric: host=algo-1, completed 39.0 % of epochs\n",
      "[05/22/2025 15:48:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=155, train loss <loss>=0.78068508207798\n",
      "[05/22/2025 15:48:28 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:48:29 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_65f088e6-66f2-44d5-ac1b-9c15a0238dd1-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928908.9791312, \"EndTime\": 1747928909.014177, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 34.81459617614746, \"count\": 1, \"min\": 34.81459617614746, \"max\": 34.81459617614746}}}\n",
      "[05/22/2025 15:48:30 INFO 140473021069120] Epoch[156] Batch[0] avg_epoch_loss=1.254939\n",
      "[05/22/2025 15:48:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=1.2549388408660889\n",
      "[05/22/2025 15:48:32 INFO 140473021069120] Epoch[156] Batch[5] avg_epoch_loss=1.072210\n",
      "[05/22/2025 15:48:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=1.0722095767656963\n",
      "[05/22/2025 15:48:32 INFO 140473021069120] Epoch[156] Batch [5]#011Speed: 376.28 samples/sec#011loss=1.072210\n",
      "[05/22/2025 15:48:34 INFO 140473021069120] Epoch[156] Batch[10] avg_epoch_loss=1.063463\n",
      "[05/22/2025 15:48:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=156, batch=10 train loss <loss>=1.0529678463935852\n",
      "[05/22/2025 15:48:34 INFO 140473021069120] Epoch[156] Batch [10]#011Speed: 364.09 samples/sec#011loss=1.052968\n",
      "[05/22/2025 15:48:34 INFO 140473021069120] processed a total of 1298 examples\n",
      "#metrics {\"StartTime\": 1747928909.0142355, \"EndTime\": 1747928914.0205863, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5006.297588348389, \"count\": 1, \"min\": 5006.297588348389, \"max\": 5006.297588348389}}}\n",
      "[05/22/2025 15:48:34 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.26900781887366 records/second\n",
      "[05/22/2025 15:48:34 INFO 140473021069120] #progress_metric: host=algo-1, completed 39.25 % of epochs\n",
      "[05/22/2025 15:48:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=156, train loss <loss>=1.0634633356874639\n",
      "[05/22/2025 15:48:34 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:35 INFO 140473021069120] Epoch[157] Batch[0] avg_epoch_loss=1.159177\n",
      "[05/22/2025 15:48:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=1.1591769456863403\n",
      "[05/22/2025 15:48:37 INFO 140473021069120] Epoch[157] Batch[5] avg_epoch_loss=1.030395\n",
      "[05/22/2025 15:48:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=1.0303950011730194\n",
      "[05/22/2025 15:48:37 INFO 140473021069120] Epoch[157] Batch [5]#011Speed: 373.20 samples/sec#011loss=1.030395\n",
      "[05/22/2025 15:48:39 INFO 140473021069120] Epoch[157] Batch[10] avg_epoch_loss=1.117726\n",
      "[05/22/2025 15:48:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=1.222523033618927\n",
      "[05/22/2025 15:48:39 INFO 140473021069120] Epoch[157] Batch [10]#011Speed: 356.48 samples/sec#011loss=1.222523\n",
      "[05/22/2025 15:48:39 INFO 140473021069120] processed a total of 1299 examples\n",
      "#metrics {\"StartTime\": 1747928914.020643, \"EndTime\": 1747928919.0652866, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5044.388294219971, \"count\": 1, \"min\": 5044.388294219971, \"max\": 5044.388294219971}}}\n",
      "[05/22/2025 15:48:39 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=257.50951025417885 records/second\n",
      "[05/22/2025 15:48:39 INFO 140473021069120] #progress_metric: host=algo-1, completed 39.5 % of epochs\n",
      "[05/22/2025 15:48:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=157, train loss <loss>=1.1177259250120684\n",
      "[05/22/2025 15:48:39 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:40 INFO 140473021069120] Epoch[158] Batch[0] avg_epoch_loss=1.086062\n",
      "[05/22/2025 15:48:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=1.0860618352890015\n",
      "[05/22/2025 15:48:42 INFO 140473021069120] Epoch[158] Batch[5] avg_epoch_loss=0.904882\n",
      "[05/22/2025 15:48:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=0.9048817654450735\n",
      "[05/22/2025 15:48:42 INFO 140473021069120] Epoch[158] Batch [5]#011Speed: 373.65 samples/sec#011loss=0.904882\n",
      "[05/22/2025 15:48:43 INFO 140473021069120] processed a total of 1269 examples\n",
      "#metrics {\"StartTime\": 1747928919.0653443, \"EndTime\": 1747928923.7184992, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4652.901172637939, \"count\": 1, \"min\": 4652.901172637939, \"max\": 4652.901172637939}}}\n",
      "[05/22/2025 15:48:43 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=272.7275593720787 records/second\n",
      "[05/22/2025 15:48:43 INFO 140473021069120] #progress_metric: host=algo-1, completed 39.75 % of epochs\n",
      "[05/22/2025 15:48:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=158, train loss <loss>=0.9473598718643188\n",
      "[05/22/2025 15:48:43 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:45 INFO 140473021069120] Epoch[159] Batch[0] avg_epoch_loss=0.916451\n",
      "[05/22/2025 15:48:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=0.9164509773254395\n",
      "[05/22/2025 15:48:46 INFO 140473021069120] Epoch[159] Batch[5] avg_epoch_loss=0.986610\n",
      "[05/22/2025 15:48:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=0.986610343058904\n",
      "[05/22/2025 15:48:46 INFO 140473021069120] Epoch[159] Batch [5]#011Speed: 371.52 samples/sec#011loss=0.986610\n",
      "[05/22/2025 15:48:48 INFO 140473021069120] Epoch[159] Batch[10] avg_epoch_loss=0.986321\n",
      "[05/22/2025 15:48:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=0.9859739065170288\n",
      "[05/22/2025 15:48:48 INFO 140473021069120] Epoch[159] Batch [10]#011Speed: 362.10 samples/sec#011loss=0.985974\n",
      "[05/22/2025 15:48:48 INFO 140473021069120] processed a total of 1286 examples\n",
      "#metrics {\"StartTime\": 1747928923.718563, \"EndTime\": 1747928928.75329, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5034.380197525024, \"count\": 1, \"min\": 5034.380197525024, \"max\": 5034.380197525024}}}\n",
      "[05/22/2025 15:48:48 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=255.43928970887296 records/second\n",
      "[05/22/2025 15:48:48 INFO 140473021069120] #progress_metric: host=algo-1, completed 40.0 % of epochs\n",
      "[05/22/2025 15:48:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=159, train loss <loss>=0.986321053721688\n",
      "[05/22/2025 15:48:48 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:50 INFO 140473021069120] Epoch[160] Batch[0] avg_epoch_loss=0.710171\n",
      "[05/22/2025 15:48:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=0.7101711630821228\n",
      "[05/22/2025 15:48:51 INFO 140473021069120] Epoch[160] Batch[5] avg_epoch_loss=0.888940\n",
      "[05/22/2025 15:48:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=0.8889397780100504\n",
      "[05/22/2025 15:48:51 INFO 140473021069120] Epoch[160] Batch [5]#011Speed: 372.95 samples/sec#011loss=0.888940\n",
      "[05/22/2025 15:48:53 INFO 140473021069120] Epoch[160] Batch[10] avg_epoch_loss=1.040640\n",
      "[05/22/2025 15:48:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=1.2226797938346863\n",
      "[05/22/2025 15:48:53 INFO 140473021069120] Epoch[160] Batch [10]#011Speed: 353.44 samples/sec#011loss=1.222680\n",
      "[05/22/2025 15:48:53 INFO 140473021069120] processed a total of 1313 examples\n",
      "#metrics {\"StartTime\": 1747928928.7533472, \"EndTime\": 1747928933.8101711, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5056.53977394104, \"count\": 1, \"min\": 5056.53977394104, \"max\": 5056.53977394104}}}\n",
      "[05/22/2025 15:48:53 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.6593389681675 records/second\n",
      "[05/22/2025 15:48:53 INFO 140473021069120] #progress_metric: host=algo-1, completed 40.25 % of epochs\n",
      "[05/22/2025 15:48:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=160, train loss <loss>=1.0406397852030667\n",
      "[05/22/2025 15:48:53 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:48:55 INFO 140473021069120] Epoch[161] Batch[0] avg_epoch_loss=0.717730\n",
      "[05/22/2025 15:48:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=0.717729926109314\n",
      "[05/22/2025 15:48:57 INFO 140473021069120] Epoch[161] Batch[5] avg_epoch_loss=0.947116\n",
      "[05/22/2025 15:48:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=0.9471159875392914\n",
      "[05/22/2025 15:48:57 INFO 140473021069120] Epoch[161] Batch [5]#011Speed: 368.76 samples/sec#011loss=0.947116\n",
      "[05/22/2025 15:48:58 INFO 140473021069120] Epoch[161] Batch[10] avg_epoch_loss=0.970030\n",
      "[05/22/2025 15:48:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=0.9975269436836243\n",
      "[05/22/2025 15:48:58 INFO 140473021069120] Epoch[161] Batch [10]#011Speed: 362.03 samples/sec#011loss=0.997527\n",
      "[05/22/2025 15:48:58 INFO 140473021069120] processed a total of 1283 examples\n",
      "#metrics {\"StartTime\": 1747928933.8102286, \"EndTime\": 1747928938.860897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5050.422430038452, \"count\": 1, \"min\": 5050.422430038452, \"max\": 5050.422430038452}}}\n",
      "[05/22/2025 15:48:58 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=254.03385052215026 records/second\n",
      "[05/22/2025 15:48:58 INFO 140473021069120] #progress_metric: host=algo-1, completed 40.5 % of epochs\n",
      "[05/22/2025 15:48:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=161, train loss <loss>=0.9700300585139882\n",
      "[05/22/2025 15:48:58 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:00 INFO 140473021069120] Epoch[162] Batch[0] avg_epoch_loss=0.786232\n",
      "[05/22/2025 15:49:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=0.7862323522567749\n",
      "[05/22/2025 15:49:02 INFO 140473021069120] Epoch[162] Batch[5] avg_epoch_loss=0.847475\n",
      "[05/22/2025 15:49:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=0.8474752604961395\n",
      "[05/22/2025 15:49:02 INFO 140473021069120] Epoch[162] Batch [5]#011Speed: 368.03 samples/sec#011loss=0.847475\n",
      "[05/22/2025 15:49:03 INFO 140473021069120] processed a total of 1242 examples\n",
      "#metrics {\"StartTime\": 1747928938.8609562, \"EndTime\": 1747928943.5573828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4696.131944656372, \"count\": 1, \"min\": 4696.131944656372, \"max\": 4696.131944656372}}}\n",
      "[05/22/2025 15:49:03 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=264.46778172460597 records/second\n",
      "[05/22/2025 15:49:03 INFO 140473021069120] #progress_metric: host=algo-1, completed 40.75 % of epochs\n",
      "[05/22/2025 15:49:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=162, train loss <loss>=0.8637295305728913\n",
      "[05/22/2025 15:49:03 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:05 INFO 140473021069120] Epoch[163] Batch[0] avg_epoch_loss=0.908551\n",
      "[05/22/2025 15:49:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=0.9085510969161987\n",
      "[05/22/2025 15:49:06 INFO 140473021069120] Epoch[163] Batch[5] avg_epoch_loss=0.920516\n",
      "[05/22/2025 15:49:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=0.9205155173937479\n",
      "[05/22/2025 15:49:06 INFO 140473021069120] Epoch[163] Batch [5]#011Speed: 372.26 samples/sec#011loss=0.920516\n",
      "[05/22/2025 15:49:08 INFO 140473021069120] processed a total of 1240 examples\n",
      "#metrics {\"StartTime\": 1747928943.5574467, \"EndTime\": 1747928948.2362611, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4678.513288497925, \"count\": 1, \"min\": 4678.513288497925, \"max\": 4678.513288497925}}}\n",
      "[05/22/2025 15:49:08 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=265.0365587737541 records/second\n",
      "[05/22/2025 15:49:08 INFO 140473021069120] #progress_metric: host=algo-1, completed 41.0 % of epochs\n",
      "[05/22/2025 15:49:08 INFO 140473021069120] #quality_metric: host=algo-1, epoch=163, train loss <loss>=0.8531127005815506\n",
      "[05/22/2025 15:49:08 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:09 INFO 140473021069120] Epoch[164] Batch[0] avg_epoch_loss=0.858526\n",
      "[05/22/2025 15:49:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=0.8585257530212402\n",
      "[05/22/2025 15:49:11 INFO 140473021069120] Epoch[164] Batch[5] avg_epoch_loss=0.862340\n",
      "[05/22/2025 15:49:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=0.8623402814070383\n",
      "[05/22/2025 15:49:11 INFO 140473021069120] Epoch[164] Batch [5]#011Speed: 372.74 samples/sec#011loss=0.862340\n",
      "[05/22/2025 15:49:13 INFO 140473021069120] Epoch[164] Batch[10] avg_epoch_loss=0.879834\n",
      "[05/22/2025 15:49:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=164, batch=10 train loss <loss>=0.9008264422416687\n",
      "[05/22/2025 15:49:13 INFO 140473021069120] Epoch[164] Batch [10]#011Speed: 364.57 samples/sec#011loss=0.900826\n",
      "[05/22/2025 15:49:13 INFO 140473021069120] processed a total of 1281 examples\n",
      "#metrics {\"StartTime\": 1747928948.2363203, \"EndTime\": 1747928953.2532303, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5016.540288925171, \"count\": 1, \"min\": 5016.540288925171, \"max\": 5016.540288925171}}}\n",
      "[05/22/2025 15:49:13 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=255.3505976822703 records/second\n",
      "[05/22/2025 15:49:13 INFO 140473021069120] #progress_metric: host=algo-1, completed 41.25 % of epochs\n",
      "[05/22/2025 15:49:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=164, train loss <loss>=0.8798339908773248\n",
      "[05/22/2025 15:49:13 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:14 INFO 140473021069120] Epoch[165] Batch[0] avg_epoch_loss=0.904678\n",
      "[05/22/2025 15:49:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=0.9046780467033386\n",
      "[05/22/2025 15:49:16 INFO 140473021069120] Epoch[165] Batch[5] avg_epoch_loss=1.007238\n",
      "[05/22/2025 15:49:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=1.0072379410266876\n",
      "[05/22/2025 15:49:16 INFO 140473021069120] Epoch[165] Batch [5]#011Speed: 369.50 samples/sec#011loss=1.007238\n",
      "[05/22/2025 15:49:18 INFO 140473021069120] Epoch[165] Batch[10] avg_epoch_loss=1.016813\n",
      "[05/22/2025 15:49:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=165, batch=10 train loss <loss>=1.0283023715019226\n",
      "[05/22/2025 15:49:18 INFO 140473021069120] Epoch[165] Batch [10]#011Speed: 360.42 samples/sec#011loss=1.028302\n",
      "[05/22/2025 15:49:18 INFO 140473021069120] processed a total of 1283 examples\n",
      "#metrics {\"StartTime\": 1747928953.253291, \"EndTime\": 1747928958.3114877, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5057.925224304199, \"count\": 1, \"min\": 5057.925224304199, \"max\": 5057.925224304199}}}\n",
      "[05/22/2025 15:49:18 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.6568264534676 records/second\n",
      "[05/22/2025 15:49:18 INFO 140473021069120] #progress_metric: host=algo-1, completed 41.5 % of epochs\n",
      "[05/22/2025 15:49:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=165, train loss <loss>=1.0168126821517944\n",
      "[05/22/2025 15:49:18 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:19 INFO 140473021069120] Epoch[166] Batch[0] avg_epoch_loss=1.161912\n",
      "[05/22/2025 15:49:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=1.161911964416504\n",
      "[05/22/2025 15:49:21 INFO 140473021069120] Epoch[166] Batch[5] avg_epoch_loss=1.004813\n",
      "[05/22/2025 15:49:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=1.0048129161198933\n",
      "[05/22/2025 15:49:21 INFO 140473021069120] Epoch[166] Batch [5]#011Speed: 369.65 samples/sec#011loss=1.004813\n",
      "[05/22/2025 15:49:23 INFO 140473021069120] Epoch[166] Batch[10] avg_epoch_loss=1.088637\n",
      "[05/22/2025 15:49:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=1.1892265200614929\n",
      "[05/22/2025 15:49:23 INFO 140473021069120] Epoch[166] Batch [10]#011Speed: 355.89 samples/sec#011loss=1.189227\n",
      "[05/22/2025 15:49:23 INFO 140473021069120] processed a total of 1326 examples\n",
      "#metrics {\"StartTime\": 1747928958.311548, \"EndTime\": 1747928963.3737433, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5061.930179595947, \"count\": 1, \"min\": 5061.930179595947, \"max\": 5061.930179595947}}}\n",
      "[05/22/2025 15:49:23 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.95085816015273 records/second\n",
      "[05/22/2025 15:49:23 INFO 140473021069120] #progress_metric: host=algo-1, completed 41.75 % of epochs\n",
      "[05/22/2025 15:49:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=166, train loss <loss>=1.088637281547893\n",
      "[05/22/2025 15:49:23 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:24 INFO 140473021069120] Epoch[167] Batch[0] avg_epoch_loss=0.976829\n",
      "[05/22/2025 15:49:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=0.9768293499946594\n",
      "[05/22/2025 15:49:26 INFO 140473021069120] Epoch[167] Batch[5] avg_epoch_loss=0.969041\n",
      "[05/22/2025 15:49:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=0.9690407415231069\n",
      "[05/22/2025 15:49:26 INFO 140473021069120] Epoch[167] Batch [5]#011Speed: 372.08 samples/sec#011loss=0.969041\n",
      "[05/22/2025 15:49:28 INFO 140473021069120] processed a total of 1229 examples\n",
      "#metrics {\"StartTime\": 1747928963.3738017, \"EndTime\": 1747928968.0449243, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4670.870065689087, \"count\": 1, \"min\": 4670.870065689087, \"max\": 4670.870065689087}}}\n",
      "[05/22/2025 15:49:28 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.11473064885644 records/second\n",
      "[05/22/2025 15:49:28 INFO 140473021069120] #progress_metric: host=algo-1, completed 42.0 % of epochs\n",
      "[05/22/2025 15:49:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=167, train loss <loss>=1.0112612783908843\n",
      "[05/22/2025 15:49:28 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:29 INFO 140473021069120] Epoch[168] Batch[0] avg_epoch_loss=0.947300\n",
      "[05/22/2025 15:49:29 INFO 140473021069120] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=0.9473002552986145\n",
      "[05/22/2025 15:49:31 INFO 140473021069120] Epoch[168] Batch[5] avg_epoch_loss=0.934594\n",
      "[05/22/2025 15:49:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=0.9345942338307699\n",
      "[05/22/2025 15:49:31 INFO 140473021069120] Epoch[168] Batch [5]#011Speed: 371.23 samples/sec#011loss=0.934594\n",
      "[05/22/2025 15:49:33 INFO 140473021069120] Epoch[168] Batch[10] avg_epoch_loss=0.710527\n",
      "[05/22/2025 15:49:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=168, batch=10 train loss <loss>=0.44164586067199707\n",
      "[05/22/2025 15:49:33 INFO 140473021069120] Epoch[168] Batch [10]#011Speed: 349.30 samples/sec#011loss=0.441646\n",
      "[05/22/2025 15:49:33 INFO 140473021069120] processed a total of 1335 examples\n",
      "#metrics {\"StartTime\": 1747928968.0449908, \"EndTime\": 1747928973.1069427, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5061.659574508667, \"count\": 1, \"min\": 5061.659574508667, \"max\": 5061.659574508667}}}\n",
      "[05/22/2025 15:49:33 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.74281735511363 records/second\n",
      "[05/22/2025 15:49:33 INFO 140473021069120] #progress_metric: host=algo-1, completed 42.25 % of epochs\n",
      "[05/22/2025 15:49:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=168, train loss <loss>=0.7105267914858732\n",
      "[05/22/2025 15:49:33 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:49:33 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_f3c55b86-27d0-4377-87f8-990566927dcb-0000.params\"\n",
      "#metrics {\"StartTime\": 1747928973.1070058, \"EndTime\": 1747928973.1423552, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 34.966230392456055, \"count\": 1, \"min\": 34.966230392456055, \"max\": 34.966230392456055}}}\n",
      "[05/22/2025 15:49:34 INFO 140473021069120] Epoch[169] Batch[0] avg_epoch_loss=0.965645\n",
      "[05/22/2025 15:49:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=0.9656451940536499\n",
      "[05/22/2025 15:49:36 INFO 140473021069120] Epoch[169] Batch[5] avg_epoch_loss=0.999557\n",
      "[05/22/2025 15:49:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=0.9995570580164591\n",
      "[05/22/2025 15:49:36 INFO 140473021069120] Epoch[169] Batch [5]#011Speed: 376.43 samples/sec#011loss=0.999557\n",
      "[05/22/2025 15:49:38 INFO 140473021069120] Epoch[169] Batch[10] avg_epoch_loss=1.158270\n",
      "[05/22/2025 15:49:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=1.3487252473831177\n",
      "[05/22/2025 15:49:38 INFO 140473021069120] Epoch[169] Batch [10]#011Speed: 363.31 samples/sec#011loss=1.348725\n",
      "[05/22/2025 15:49:38 INFO 140473021069120] processed a total of 1304 examples\n",
      "#metrics {\"StartTime\": 1747928973.1424203, \"EndTime\": 1747928978.1451209, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5002.646446228027, \"count\": 1, \"min\": 5002.646446228027, \"max\": 5002.646446228027}}}\n",
      "[05/22/2025 15:49:38 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.65753742723507 records/second\n",
      "[05/22/2025 15:49:38 INFO 140473021069120] #progress_metric: host=algo-1, completed 42.5 % of epochs\n",
      "[05/22/2025 15:49:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=169, train loss <loss>=1.1582698713649402\n",
      "[05/22/2025 15:49:38 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:39 INFO 140473021069120] Epoch[170] Batch[0] avg_epoch_loss=1.140913\n",
      "[05/22/2025 15:49:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=1.1409131288528442\n",
      "[05/22/2025 15:49:41 INFO 140473021069120] Epoch[170] Batch[5] avg_epoch_loss=0.954974\n",
      "[05/22/2025 15:49:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=0.9549737274646759\n",
      "[05/22/2025 15:49:41 INFO 140473021069120] Epoch[170] Batch [5]#011Speed: 374.56 samples/sec#011loss=0.954974\n",
      "[05/22/2025 15:49:43 INFO 140473021069120] Epoch[170] Batch[10] avg_epoch_loss=0.957814\n",
      "[05/22/2025 15:49:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=0.9612220406532288\n",
      "[05/22/2025 15:49:43 INFO 140473021069120] Epoch[170] Batch [10]#011Speed: 370.43 samples/sec#011loss=0.961222\n",
      "[05/22/2025 15:49:43 INFO 140473021069120] processed a total of 1281 examples\n",
      "#metrics {\"StartTime\": 1747928978.1451797, \"EndTime\": 1747928983.1439576, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4998.492240905762, \"count\": 1, \"min\": 4998.492240905762, \"max\": 4998.492240905762}}}\n",
      "[05/22/2025 15:49:43 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.2727947730285 records/second\n",
      "[05/22/2025 15:49:43 INFO 140473021069120] #progress_metric: host=algo-1, completed 42.75 % of epochs\n",
      "[05/22/2025 15:49:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=170, train loss <loss>=0.957813869823109\n",
      "[05/22/2025 15:49:43 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:44 INFO 140473021069120] Epoch[171] Batch[0] avg_epoch_loss=0.924536\n",
      "[05/22/2025 15:49:44 INFO 140473021069120] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=0.9245359301567078\n",
      "[05/22/2025 15:49:46 INFO 140473021069120] Epoch[171] Batch[5] avg_epoch_loss=0.939568\n",
      "[05/22/2025 15:49:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=0.9395683109760284\n",
      "[05/22/2025 15:49:46 INFO 140473021069120] Epoch[171] Batch [5]#011Speed: 372.25 samples/sec#011loss=0.939568\n",
      "[05/22/2025 15:49:47 INFO 140473021069120] processed a total of 1274 examples\n",
      "#metrics {\"StartTime\": 1747928983.1440172, \"EndTime\": 1747928987.8201988, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4675.915718078613, \"count\": 1, \"min\": 4675.915718078613, \"max\": 4675.915718078613}}}\n",
      "[05/22/2025 15:49:47 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=272.4545133647964 records/second\n",
      "[05/22/2025 15:49:47 INFO 140473021069120] #progress_metric: host=algo-1, completed 43.0 % of epochs\n",
      "[05/22/2025 15:49:47 INFO 140473021069120] #quality_metric: host=algo-1, epoch=171, train loss <loss>=0.9321856737136841\n",
      "[05/22/2025 15:49:47 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:49 INFO 140473021069120] Epoch[172] Batch[0] avg_epoch_loss=1.100033\n",
      "[05/22/2025 15:49:49 INFO 140473021069120] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=1.1000330448150635\n",
      "[05/22/2025 15:49:51 INFO 140473021069120] Epoch[172] Batch[5] avg_epoch_loss=0.961644\n",
      "[05/22/2025 15:49:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=0.9616435865561167\n",
      "[05/22/2025 15:49:51 INFO 140473021069120] Epoch[172] Batch [5]#011Speed: 369.04 samples/sec#011loss=0.961644\n",
      "[05/22/2025 15:49:52 INFO 140473021069120] processed a total of 1266 examples\n",
      "#metrics {\"StartTime\": 1747928987.8202639, \"EndTime\": 1747928992.5052083, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4684.560537338257, \"count\": 1, \"min\": 4684.560537338257, \"max\": 4684.560537338257}}}\n",
      "[05/22/2025 15:49:52 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=270.2438853884627 records/second\n",
      "[05/22/2025 15:49:52 INFO 140473021069120] #progress_metric: host=algo-1, completed 43.25 % of epochs\n",
      "[05/22/2025 15:49:52 INFO 140473021069120] #quality_metric: host=algo-1, epoch=172, train loss <loss>=0.9305379390716553\n",
      "[05/22/2025 15:49:52 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:54 INFO 140473021069120] Epoch[173] Batch[0] avg_epoch_loss=0.907796\n",
      "[05/22/2025 15:49:54 INFO 140473021069120] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=0.9077958464622498\n",
      "[05/22/2025 15:49:55 INFO 140473021069120] Epoch[173] Batch[5] avg_epoch_loss=0.905716\n",
      "[05/22/2025 15:49:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=0.905715803305308\n",
      "[05/22/2025 15:49:55 INFO 140473021069120] Epoch[173] Batch [5]#011Speed: 365.88 samples/sec#011loss=0.905716\n",
      "[05/22/2025 15:49:57 INFO 140473021069120] Epoch[173] Batch[10] avg_epoch_loss=0.924531\n",
      "[05/22/2025 15:49:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=173, batch=10 train loss <loss>=0.9471085429191589\n",
      "[05/22/2025 15:49:57 INFO 140473021069120] Epoch[173] Batch [10]#011Speed: 365.11 samples/sec#011loss=0.947109\n",
      "[05/22/2025 15:49:57 INFO 140473021069120] processed a total of 1309 examples\n",
      "#metrics {\"StartTime\": 1747928992.5052726, \"EndTime\": 1747928997.5584648, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5052.886009216309, \"count\": 1, \"min\": 5052.886009216309, \"max\": 5052.886009216309}}}\n",
      "[05/22/2025 15:49:57 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.05536100358836 records/second\n",
      "[05/22/2025 15:49:57 INFO 140473021069120] #progress_metric: host=algo-1, completed 43.5 % of epochs\n",
      "[05/22/2025 15:49:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=173, train loss <loss>=0.9245306849479675\n",
      "[05/22/2025 15:49:57 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:49:59 INFO 140473021069120] Epoch[174] Batch[0] avg_epoch_loss=0.852001\n",
      "[05/22/2025 15:49:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=0.8520005941390991\n",
      "[05/22/2025 15:50:00 INFO 140473021069120] Epoch[174] Batch[5] avg_epoch_loss=0.881516\n",
      "[05/22/2025 15:50:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=0.8815164367357889\n",
      "[05/22/2025 15:50:00 INFO 140473021069120] Epoch[174] Batch [5]#011Speed: 367.96 samples/sec#011loss=0.881516\n",
      "[05/22/2025 15:50:02 INFO 140473021069120] processed a total of 1210 examples\n",
      "#metrics {\"StartTime\": 1747928997.5585237, \"EndTime\": 1747929002.2833648, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4724.575996398926, \"count\": 1, \"min\": 4724.575996398926, \"max\": 4724.575996398926}}}\n",
      "[05/22/2025 15:50:02 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.1025720730495 records/second\n",
      "[05/22/2025 15:50:02 INFO 140473021069120] #progress_metric: host=algo-1, completed 43.75 % of epochs\n",
      "[05/22/2025 15:50:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=174, train loss <loss>=0.8434817492961884\n",
      "[05/22/2025 15:50:02 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:50:03 INFO 140473021069120] Epoch[175] Batch[0] avg_epoch_loss=1.055354\n",
      "[05/22/2025 15:50:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=1.0553537607192993\n",
      "[05/22/2025 15:50:05 INFO 140473021069120] Epoch[175] Batch[5] avg_epoch_loss=0.965760\n",
      "[05/22/2025 15:50:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=0.965760350227356\n",
      "[05/22/2025 15:50:05 INFO 140473021069120] Epoch[175] Batch [5]#011Speed: 371.88 samples/sec#011loss=0.965760\n",
      "[05/22/2025 15:50:06 INFO 140473021069120] processed a total of 1248 examples\n",
      "#metrics {\"StartTime\": 1747929002.283428, \"EndTime\": 1747929006.937, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4653.275012969971, \"count\": 1, \"min\": 4653.275012969971, \"max\": 4653.275012969971}}}\n",
      "[05/22/2025 15:50:06 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.1927484393995 records/second\n",
      "[05/22/2025 15:50:06 INFO 140473021069120] #progress_metric: host=algo-1, completed 44.0 % of epochs\n",
      "[05/22/2025 15:50:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=175, train loss <loss>=0.9884836375713348\n",
      "[05/22/2025 15:50:06 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:50:08 INFO 140473021069120] Epoch[176] Batch[0] avg_epoch_loss=0.828526\n",
      "[05/22/2025 15:50:08 INFO 140473021069120] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=0.8285260796546936\n",
      "[05/22/2025 15:50:10 INFO 140473021069120] Epoch[176] Batch[5] avg_epoch_loss=0.896930\n",
      "[05/22/2025 15:50:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=0.8969303866227468\n",
      "[05/22/2025 15:50:10 INFO 140473021069120] Epoch[176] Batch [5]#011Speed: 373.35 samples/sec#011loss=0.896930\n",
      "[05/22/2025 15:50:11 INFO 140473021069120] Epoch[176] Batch[10] avg_epoch_loss=0.953389\n",
      "[05/22/2025 15:50:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=176, batch=10 train loss <loss>=1.0211383938789367\n",
      "[05/22/2025 15:50:11 INFO 140473021069120] Epoch[176] Batch [10]#011Speed: 362.87 samples/sec#011loss=1.021138\n",
      "[05/22/2025 15:50:11 INFO 140473021069120] processed a total of 1294 examples\n",
      "#metrics {\"StartTime\": 1747929006.9370642, \"EndTime\": 1747929011.962763, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5025.346040725708, \"count\": 1, \"min\": 5025.346040725708, \"max\": 5025.346040725708}}}\n",
      "[05/22/2025 15:50:11 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=257.49034456760177 records/second\n",
      "[05/22/2025 15:50:11 INFO 140473021069120] #progress_metric: host=algo-1, completed 44.25 % of epochs\n",
      "[05/22/2025 15:50:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=176, train loss <loss>=0.9533885717391968\n",
      "[05/22/2025 15:50:11 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:50:13 INFO 140473021069120] Epoch[177] Batch[0] avg_epoch_loss=0.999962\n",
      "[05/22/2025 15:50:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=0.9999623894691467\n",
      "[05/22/2025 15:50:15 INFO 140473021069120] Epoch[177] Batch[5] avg_epoch_loss=0.881478\n",
      "[05/22/2025 15:50:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=0.8814780116081238\n",
      "[05/22/2025 15:50:15 INFO 140473021069120] Epoch[177] Batch [5]#011Speed: 369.31 samples/sec#011loss=0.881478\n",
      "[05/22/2025 15:50:17 INFO 140473021069120] Epoch[177] Batch[10] avg_epoch_loss=0.971310\n",
      "[05/22/2025 15:50:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=1.0791074275970458\n",
      "[05/22/2025 15:50:17 INFO 140473021069120] Epoch[177] Batch [10]#011Speed: 346.12 samples/sec#011loss=1.079107\n",
      "[05/22/2025 15:50:17 INFO 140473021069120] processed a total of 1328 examples\n",
      "#metrics {\"StartTime\": 1747929011.962821, \"EndTime\": 1747929017.0577087, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5094.634532928467, \"count\": 1, \"min\": 5094.634532928467, \"max\": 5094.634532928467}}}\n",
      "[05/22/2025 15:50:17 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.6617195542161 records/second\n",
      "[05/22/2025 15:50:17 INFO 140473021069120] #progress_metric: host=algo-1, completed 44.5 % of epochs\n",
      "[05/22/2025 15:50:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=177, train loss <loss>=0.9713095643303611\n",
      "[05/22/2025 15:50:17 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:50:18 INFO 140473021069120] Epoch[178] Batch[0] avg_epoch_loss=0.755302\n",
      "[05/22/2025 15:50:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=0.7553023099899292\n",
      "[05/22/2025 15:50:20 INFO 140473021069120] Epoch[178] Batch[5] avg_epoch_loss=0.752779\n",
      "[05/22/2025 15:50:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=0.7527785102526346\n",
      "[05/22/2025 15:50:20 INFO 140473021069120] Epoch[178] Batch [5]#011Speed: 370.34 samples/sec#011loss=0.752779\n",
      "[05/22/2025 15:50:22 INFO 140473021069120] Epoch[178] Batch[10] avg_epoch_loss=0.647010\n",
      "[05/22/2025 15:50:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=0.5200869083404541\n",
      "[05/22/2025 15:50:22 INFO 140473021069120] Epoch[178] Batch [10]#011Speed: 359.04 samples/sec#011loss=0.520087\n",
      "[05/22/2025 15:50:22 INFO 140473021069120] processed a total of 1329 examples\n",
      "#metrics {\"StartTime\": 1747929017.0577712, \"EndTime\": 1747929022.104783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5046.743869781494, \"count\": 1, \"min\": 5046.743869781494, \"max\": 5046.743869781494}}}\n",
      "[05/22/2025 15:50:22 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.3334214854482 records/second\n",
      "[05/22/2025 15:50:22 INFO 140473021069120] #progress_metric: host=algo-1, completed 44.75 % of epochs\n",
      "[05/22/2025 15:50:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=178, train loss <loss>=0.6470096002925526\n",
      "[05/22/2025 15:50:22 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:50:22 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_b306c149-357a-46e7-bb39-6872e38aa403-0000.params\"\n",
      "#metrics {\"StartTime\": 1747929022.1048436, \"EndTime\": 1747929022.1404662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 35.32528877258301, \"count\": 1, \"min\": 35.32528877258301, \"max\": 35.32528877258301}}}\n",
      "[05/22/2025 15:50:23 INFO 140473021069120] Epoch[179] Batch[0] avg_epoch_loss=0.882736\n",
      "[05/22/2025 15:50:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=0.8827363848686218\n",
      "[05/22/2025 15:50:25 INFO 140473021069120] Epoch[179] Batch[5] avg_epoch_loss=0.915200\n",
      "[05/22/2025 15:50:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=0.9151999652385712\n",
      "[05/22/2025 15:50:25 INFO 140473021069120] Epoch[179] Batch [5]#011Speed: 370.52 samples/sec#011loss=0.915200\n",
      "[05/22/2025 15:50:26 INFO 140473021069120] processed a total of 1280 examples\n",
      "#metrics {\"StartTime\": 1747929022.1405256, \"EndTime\": 1747929026.8335454, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4692.962169647217, \"count\": 1, \"min\": 4692.962169647217, \"max\": 4692.962169647217}}}\n",
      "[05/22/2025 15:50:26 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=272.74356331719866 records/second\n",
      "[05/22/2025 15:50:26 INFO 140473021069120] #progress_metric: host=algo-1, completed 45.0 % of epochs\n",
      "[05/22/2025 15:50:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=179, train loss <loss>=0.9661560297012329\n",
      "[05/22/2025 15:50:26 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:50:28 INFO 140473021069120] Epoch[180] Batch[0] avg_epoch_loss=0.926611\n",
      "[05/22/2025 15:50:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=0.9266108274459839\n",
      "[05/22/2025 15:50:30 INFO 140473021069120] Epoch[180] Batch[5] avg_epoch_loss=0.897529\n",
      "[05/22/2025 15:50:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=0.8975291947523752\n",
      "[05/22/2025 15:50:30 INFO 140473021069120] Epoch[180] Batch [5]#011Speed: 369.84 samples/sec#011loss=0.897529\n",
      "[05/22/2025 15:50:31 INFO 140473021069120] processed a total of 1179 examples\n",
      "#metrics {\"StartTime\": 1747929026.833606, \"EndTime\": 1747929031.4887877, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4654.258489608765, \"count\": 1, \"min\": 4654.258489608765, \"max\": 4654.258489608765}}}\n",
      "[05/22/2025 15:50:31 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.31132521514704 records/second\n",
      "[05/22/2025 15:50:31 INFO 140473021069120] #progress_metric: host=algo-1, completed 45.25 % of epochs\n",
      "[05/22/2025 15:50:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=180, train loss <loss>=0.5521681904792786\n",
      "[05/22/2025 15:50:31 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:50:31 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_62dbad2d-5196-4454-bb1e-a2dd61b3187f-0000.params\"\n",
      "#metrics {\"StartTime\": 1747929031.4888515, \"EndTime\": 1747929031.5241113, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 34.886837005615234, \"count\": 1, \"min\": 34.886837005615234, \"max\": 34.886837005615234}}}\n",
      "[05/22/2025 15:50:33 INFO 140473021069120] Epoch[181] Batch[0] avg_epoch_loss=1.294301\n",
      "[05/22/2025 15:50:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=1.29430091381073\n",
      "[05/22/2025 15:50:34 INFO 140473021069120] Epoch[181] Batch[5] avg_epoch_loss=1.152270\n",
      "[05/22/2025 15:50:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=1.1522698601086934\n",
      "[05/22/2025 15:50:34 INFO 140473021069120] Epoch[181] Batch [5]#011Speed: 370.82 samples/sec#011loss=1.152270\n",
      "[05/22/2025 15:50:36 INFO 140473021069120] Epoch[181] Batch[10] avg_epoch_loss=0.935182\n",
      "[05/22/2025 15:50:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=181, batch=10 train loss <loss>=0.6746755123138428\n",
      "[05/22/2025 15:50:36 INFO 140473021069120] Epoch[181] Batch [10]#011Speed: 358.18 samples/sec#011loss=0.674676\n",
      "[05/22/2025 15:50:36 INFO 140473021069120] processed a total of 1324 examples\n",
      "#metrics {\"StartTime\": 1747929031.5241706, \"EndTime\": 1747929036.545262, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5021.036863327026, \"count\": 1, \"min\": 5021.036863327026, \"max\": 5021.036863327026}}}\n",
      "[05/22/2025 15:50:36 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.68597293434357 records/second\n",
      "[05/22/2025 15:50:36 INFO 140473021069120] #progress_metric: host=algo-1, completed 45.5 % of epochs\n",
      "[05/22/2025 15:50:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=181, train loss <loss>=0.9351815202019431\n",
      "[05/22/2025 15:50:36 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:50:38 INFO 140473021069120] Epoch[182] Batch[0] avg_epoch_loss=1.105957\n",
      "[05/22/2025 15:50:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=1.1059566736221313\n",
      "[05/22/2025 15:50:39 INFO 140473021069120] Epoch[182] Batch[5] avg_epoch_loss=0.944408\n",
      "[05/22/2025 15:50:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=0.9444078803062439\n",
      "[05/22/2025 15:50:39 INFO 140473021069120] Epoch[182] Batch [5]#011Speed: 366.58 samples/sec#011loss=0.944408\n",
      "[05/22/2025 15:50:41 INFO 140473021069120] Epoch[182] Batch[10] avg_epoch_loss=1.011415\n",
      "[05/22/2025 15:50:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=182, batch=10 train loss <loss>=1.0918237447738648\n",
      "[05/22/2025 15:50:41 INFO 140473021069120] Epoch[182] Batch [10]#011Speed: 360.71 samples/sec#011loss=1.091824\n",
      "[05/22/2025 15:50:41 INFO 140473021069120] processed a total of 1305 examples\n",
      "#metrics {\"StartTime\": 1747929036.545321, \"EndTime\": 1747929041.6038373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5058.271169662476, \"count\": 1, \"min\": 5058.271169662476, \"max\": 5058.271169662476}}}\n",
      "[05/22/2025 15:50:41 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=257.9889203982261 records/second\n",
      "[05/22/2025 15:50:41 INFO 140473021069120] #progress_metric: host=algo-1, completed 45.75 % of epochs\n",
      "[05/22/2025 15:50:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=182, train loss <loss>=1.0114150914278897\n",
      "[05/22/2025 15:50:41 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:50:43 INFO 140473021069120] Epoch[183] Batch[0] avg_epoch_loss=0.910191\n",
      "[05/22/2025 15:50:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=0.9101914763450623\n",
      "[05/22/2025 15:50:44 INFO 140473021069120] Epoch[183] Batch[5] avg_epoch_loss=0.829034\n",
      "[05/22/2025 15:50:44 INFO 140473021069120] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=0.8290344476699829\n",
      "[05/22/2025 15:50:44 INFO 140473021069120] Epoch[183] Batch [5]#011Speed: 368.53 samples/sec#011loss=0.829034\n",
      "[05/22/2025 15:50:46 INFO 140473021069120] processed a total of 1258 examples\n",
      "#metrics {\"StartTime\": 1747929041.6038952, \"EndTime\": 1747929046.2667172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4662.546396255493, \"count\": 1, \"min\": 4662.546396255493, \"max\": 4662.546396255493}}}\n",
      "[05/22/2025 15:50:46 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.8044325382104 records/second\n",
      "[05/22/2025 15:50:46 INFO 140473021069120] #progress_metric: host=algo-1, completed 46.0 % of epochs\n",
      "[05/22/2025 15:50:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=183, train loss <loss>=0.8901211857795716\n",
      "[05/22/2025 15:50:46 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:50:47 INFO 140473021069120] Epoch[184] Batch[0] avg_epoch_loss=0.883879\n",
      "[05/22/2025 15:50:47 INFO 140473021069120] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=0.8838789463043213\n",
      "[05/22/2025 15:50:49 INFO 140473021069120] Epoch[184] Batch[5] avg_epoch_loss=0.815932\n",
      "[05/22/2025 15:50:49 INFO 140473021069120] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=0.8159321745236715\n",
      "[05/22/2025 15:50:49 INFO 140473021069120] Epoch[184] Batch [5]#011Speed: 372.80 samples/sec#011loss=0.815932\n",
      "[05/22/2025 15:50:50 INFO 140473021069120] processed a total of 1218 examples\n",
      "#metrics {\"StartTime\": 1747929046.266777, \"EndTime\": 1747929050.9185174, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4651.456117630005, \"count\": 1, \"min\": 4651.456117630005, \"max\": 4651.456117630005}}}\n",
      "[05/22/2025 15:50:50 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.84826517259603 records/second\n",
      "[05/22/2025 15:50:50 INFO 140473021069120] #progress_metric: host=algo-1, completed 46.25 % of epochs\n",
      "[05/22/2025 15:50:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=184, train loss <loss>=0.8619114518165588\n",
      "[05/22/2025 15:50:50 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:50:52 INFO 140473021069120] Epoch[185] Batch[0] avg_epoch_loss=0.651334\n",
      "[05/22/2025 15:50:52 INFO 140473021069120] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=0.6513336300849915\n",
      "[05/22/2025 15:50:54 INFO 140473021069120] Epoch[185] Batch[5] avg_epoch_loss=0.915016\n",
      "[05/22/2025 15:50:54 INFO 140473021069120] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=0.9150161246458689\n",
      "[05/22/2025 15:50:54 INFO 140473021069120] Epoch[185] Batch [5]#011Speed: 369.54 samples/sec#011loss=0.915016\n",
      "[05/22/2025 15:50:55 INFO 140473021069120] Epoch[185] Batch[10] avg_epoch_loss=1.005125\n",
      "[05/22/2025 15:50:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=185, batch=10 train loss <loss>=1.1132561922073365\n",
      "[05/22/2025 15:50:55 INFO 140473021069120] Epoch[185] Batch [10]#011Speed: 360.64 samples/sec#011loss=1.113256\n",
      "[05/22/2025 15:50:55 INFO 140473021069120] processed a total of 1292 examples\n",
      "#metrics {\"StartTime\": 1747929050.918581, \"EndTime\": 1747929055.9595983, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5040.724754333496, \"count\": 1, \"min\": 5040.724754333496, \"max\": 5040.724754333496}}}\n",
      "[05/22/2025 15:50:55 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.30783877029165 records/second\n",
      "[05/22/2025 15:50:55 INFO 140473021069120] #progress_metric: host=algo-1, completed 46.5 % of epochs\n",
      "[05/22/2025 15:50:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=185, train loss <loss>=1.0051252462647178\n",
      "[05/22/2025 15:50:55 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:50:57 INFO 140473021069120] Epoch[186] Batch[0] avg_epoch_loss=1.142486\n",
      "[05/22/2025 15:50:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=1.1424857378005981\n",
      "[05/22/2025 15:50:59 INFO 140473021069120] Epoch[186] Batch[5] avg_epoch_loss=0.968387\n",
      "[05/22/2025 15:50:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=0.9683868984381357\n",
      "[05/22/2025 15:50:59 INFO 140473021069120] Epoch[186] Batch [5]#011Speed: 370.08 samples/sec#011loss=0.968387\n",
      "[05/22/2025 15:51:01 INFO 140473021069120] Epoch[186] Batch[10] avg_epoch_loss=1.080841\n",
      "[05/22/2025 15:51:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=186, batch=10 train loss <loss>=1.215786623954773\n",
      "[05/22/2025 15:51:01 INFO 140473021069120] Epoch[186] Batch [10]#011Speed: 359.37 samples/sec#011loss=1.215787\n",
      "[05/22/2025 15:51:01 INFO 140473021069120] processed a total of 1296 examples\n",
      "#metrics {\"StartTime\": 1747929055.9596589, \"EndTime\": 1747929061.0118413, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5051.920175552368, \"count\": 1, \"min\": 5051.920175552368, \"max\": 5051.920175552368}}}\n",
      "[05/22/2025 15:51:01 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.5316525637176 records/second\n",
      "[05/22/2025 15:51:01 INFO 140473021069120] #progress_metric: host=algo-1, completed 46.75 % of epochs\n",
      "[05/22/2025 15:51:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=186, train loss <loss>=1.0808413191275164\n",
      "[05/22/2025 15:51:01 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:02 INFO 140473021069120] Epoch[187] Batch[0] avg_epoch_loss=0.888839\n",
      "[05/22/2025 15:51:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=0.8888385891914368\n",
      "[05/22/2025 15:51:04 INFO 140473021069120] Epoch[187] Batch[5] avg_epoch_loss=0.902703\n",
      "[05/22/2025 15:51:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=0.9027034739653269\n",
      "[05/22/2025 15:51:04 INFO 140473021069120] Epoch[187] Batch [5]#011Speed: 373.24 samples/sec#011loss=0.902703\n",
      "[05/22/2025 15:51:06 INFO 140473021069120] Epoch[187] Batch[10] avg_epoch_loss=1.000473\n",
      "[05/22/2025 15:51:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=187, batch=10 train loss <loss>=1.117795705795288\n",
      "[05/22/2025 15:51:06 INFO 140473021069120] Epoch[187] Batch [10]#011Speed: 350.84 samples/sec#011loss=1.117796\n",
      "[05/22/2025 15:51:06 INFO 140473021069120] processed a total of 1327 examples\n",
      "#metrics {\"StartTime\": 1747929061.0119011, \"EndTime\": 1747929066.071684, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5059.508562088013, \"count\": 1, \"min\": 5059.508562088013, \"max\": 5059.508562088013}}}\n",
      "[05/22/2025 15:51:06 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.2739016743598 records/second\n",
      "[05/22/2025 15:51:06 INFO 140473021069120] #progress_metric: host=algo-1, completed 47.0 % of epochs\n",
      "[05/22/2025 15:51:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=187, train loss <loss>=1.0004726702516729\n",
      "[05/22/2025 15:51:06 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:07 INFO 140473021069120] Epoch[188] Batch[0] avg_epoch_loss=0.880512\n",
      "[05/22/2025 15:51:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=0.8805123567581177\n",
      "[05/22/2025 15:51:09 INFO 140473021069120] Epoch[188] Batch[5] avg_epoch_loss=0.949465\n",
      "[05/22/2025 15:51:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=0.9494645396868387\n",
      "[05/22/2025 15:51:09 INFO 140473021069120] Epoch[188] Batch [5]#011Speed: 367.71 samples/sec#011loss=0.949465\n",
      "[05/22/2025 15:51:11 INFO 140473021069120] Epoch[188] Batch[10] avg_epoch_loss=0.982202\n",
      "[05/22/2025 15:51:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=1.0214878916740417\n",
      "[05/22/2025 15:51:11 INFO 140473021069120] Epoch[188] Batch [10]#011Speed: 361.77 samples/sec#011loss=1.021488\n",
      "[05/22/2025 15:51:11 INFO 140473021069120] processed a total of 1292 examples\n",
      "#metrics {\"StartTime\": 1747929066.0717425, \"EndTime\": 1747929071.1330068, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5061.001777648926, \"count\": 1, \"min\": 5061.001777648926, \"max\": 5061.001777648926}}}\n",
      "[05/22/2025 15:51:11 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=255.28101347731345 records/second\n",
      "[05/22/2025 15:51:11 INFO 140473021069120] #progress_metric: host=algo-1, completed 47.25 % of epochs\n",
      "[05/22/2025 15:51:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=188, train loss <loss>=0.9822024269537493\n",
      "[05/22/2025 15:51:11 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:12 INFO 140473021069120] Epoch[189] Batch[0] avg_epoch_loss=0.846505\n",
      "[05/22/2025 15:51:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=0.8465047478675842\n",
      "[05/22/2025 15:51:14 INFO 140473021069120] Epoch[189] Batch[5] avg_epoch_loss=0.922998\n",
      "[05/22/2025 15:51:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=0.9229983786741892\n",
      "[05/22/2025 15:51:14 INFO 140473021069120] Epoch[189] Batch [5]#011Speed: 371.02 samples/sec#011loss=0.922998\n",
      "[05/22/2025 15:51:15 INFO 140473021069120] processed a total of 1267 examples\n",
      "#metrics {\"StartTime\": 1747929071.1330667, \"EndTime\": 1747929075.8028352, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4669.500112533569, \"count\": 1, \"min\": 4669.500112533569, \"max\": 4669.500112533569}}}\n",
      "[05/22/2025 15:51:15 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=271.32985123375516 records/second\n",
      "[05/22/2025 15:51:15 INFO 140473021069120] #progress_metric: host=algo-1, completed 47.5 % of epochs\n",
      "[05/22/2025 15:51:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=189, train loss <loss>=0.9145124912261963\n",
      "[05/22/2025 15:51:15 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:17 INFO 140473021069120] Epoch[190] Batch[0] avg_epoch_loss=0.949800\n",
      "[05/22/2025 15:51:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=0.9498002529144287\n",
      "[05/22/2025 15:51:19 INFO 140473021069120] Epoch[190] Batch[5] avg_epoch_loss=0.913962\n",
      "[05/22/2025 15:51:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=0.9139618476231893\n",
      "[05/22/2025 15:51:19 INFO 140473021069120] Epoch[190] Batch [5]#011Speed: 373.41 samples/sec#011loss=0.913962\n",
      "[05/22/2025 15:51:20 INFO 140473021069120] Epoch[190] Batch[10] avg_epoch_loss=0.994502\n",
      "[05/22/2025 15:51:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=190, batch=10 train loss <loss>=1.0911508917808532\n",
      "[05/22/2025 15:51:20 INFO 140473021069120] Epoch[190] Batch [10]#011Speed: 357.00 samples/sec#011loss=1.091151\n",
      "[05/22/2025 15:51:20 INFO 140473021069120] processed a total of 1322 examples\n",
      "#metrics {\"StartTime\": 1747929075.8028984, \"EndTime\": 1747929080.8326805, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5029.488801956177, \"count\": 1, \"min\": 5029.488801956177, \"max\": 5029.488801956177}}}\n",
      "[05/22/2025 15:51:20 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.8450776091411 records/second\n",
      "[05/22/2025 15:51:20 INFO 140473021069120] #progress_metric: host=algo-1, completed 47.75 % of epochs\n",
      "[05/22/2025 15:51:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=190, train loss <loss>=0.9945023222403093\n",
      "[05/22/2025 15:51:20 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:22 INFO 140473021069120] Epoch[191] Batch[0] avg_epoch_loss=0.871661\n",
      "[05/22/2025 15:51:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=0.8716611862182617\n",
      "[05/22/2025 15:51:24 INFO 140473021069120] Epoch[191] Batch[5] avg_epoch_loss=0.809527\n",
      "[05/22/2025 15:51:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=0.809527059396108\n",
      "[05/22/2025 15:51:24 INFO 140473021069120] Epoch[191] Batch [5]#011Speed: 366.57 samples/sec#011loss=0.809527\n",
      "[05/22/2025 15:51:25 INFO 140473021069120] processed a total of 1242 examples\n",
      "#metrics {\"StartTime\": 1747929080.8327417, \"EndTime\": 1747929085.5334892, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4700.477361679077, \"count\": 1, \"min\": 4700.477361679077, \"max\": 4700.477361679077}}}\n",
      "[05/22/2025 15:51:25 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=264.22316184906714 records/second\n",
      "[05/22/2025 15:51:25 INFO 140473021069120] #progress_metric: host=algo-1, completed 48.0 % of epochs\n",
      "[05/22/2025 15:51:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=191, train loss <loss>=0.7839593529701233\n",
      "[05/22/2025 15:51:25 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:27 INFO 140473021069120] Epoch[192] Batch[0] avg_epoch_loss=0.935582\n",
      "[05/22/2025 15:51:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=0.9355820417404175\n",
      "[05/22/2025 15:51:28 INFO 140473021069120] Epoch[192] Batch[5] avg_epoch_loss=0.758224\n",
      "[05/22/2025 15:51:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=0.7582242886225382\n",
      "[05/22/2025 15:51:28 INFO 140473021069120] Epoch[192] Batch [5]#011Speed: 366.62 samples/sec#011loss=0.758224\n",
      "[05/22/2025 15:51:30 INFO 140473021069120] processed a total of 1218 examples\n",
      "#metrics {\"StartTime\": 1747929085.5335543, \"EndTime\": 1747929090.2401516, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4706.294298171997, \"count\": 1, \"min\": 4706.294298171997, \"max\": 4706.294298171997}}}\n",
      "[05/22/2025 15:51:30 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=258.79690399879553 records/second\n",
      "[05/22/2025 15:51:30 INFO 140473021069120] #progress_metric: host=algo-1, completed 48.25 % of epochs\n",
      "[05/22/2025 15:51:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=192, train loss <loss>=0.8223906636238099\n",
      "[05/22/2025 15:51:30 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:31 INFO 140473021069120] Epoch[193] Batch[0] avg_epoch_loss=0.659130\n",
      "[05/22/2025 15:51:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=0.6591299772262573\n",
      "[05/22/2025 15:51:33 INFO 140473021069120] Epoch[193] Batch[5] avg_epoch_loss=0.817391\n",
      "[05/22/2025 15:51:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=0.8173909385999044\n",
      "[05/22/2025 15:51:33 INFO 140473021069120] Epoch[193] Batch [5]#011Speed: 367.36 samples/sec#011loss=0.817391\n",
      "[05/22/2025 15:51:34 INFO 140473021069120] processed a total of 1240 examples\n",
      "#metrics {\"StartTime\": 1747929090.2402189, \"EndTime\": 1747929094.9626102, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4722.08046913147, \"count\": 1, \"min\": 4722.08046913147, \"max\": 4722.08046913147}}}\n",
      "[05/22/2025 15:51:34 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.5908279410933 records/second\n",
      "[05/22/2025 15:51:34 INFO 140473021069120] #progress_metric: host=algo-1, completed 48.5 % of epochs\n",
      "[05/22/2025 15:51:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=193, train loss <loss>=0.8341477870941162\n",
      "[05/22/2025 15:51:34 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:36 INFO 140473021069120] Epoch[194] Batch[0] avg_epoch_loss=0.739134\n",
      "[05/22/2025 15:51:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=0.7391335368156433\n",
      "[05/22/2025 15:51:38 INFO 140473021069120] Epoch[194] Batch[5] avg_epoch_loss=0.895893\n",
      "[05/22/2025 15:51:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=0.8958932856718699\n",
      "[05/22/2025 15:51:38 INFO 140473021069120] Epoch[194] Batch [5]#011Speed: 365.44 samples/sec#011loss=0.895893\n",
      "[05/22/2025 15:51:39 INFO 140473021069120] processed a total of 1198 examples\n",
      "#metrics {\"StartTime\": 1747929094.9626746, \"EndTime\": 1747929099.6899898, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4727.019786834717, \"count\": 1, \"min\": 4727.019786834717, \"max\": 4727.019786834717}}}\n",
      "[05/22/2025 15:51:39 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.43156283957873 records/second\n",
      "[05/22/2025 15:51:39 INFO 140473021069120] #progress_metric: host=algo-1, completed 48.75 % of epochs\n",
      "[05/22/2025 15:51:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=194, train loss <loss>=0.7767303436994553\n",
      "[05/22/2025 15:51:39 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:41 INFO 140473021069120] Epoch[195] Batch[0] avg_epoch_loss=1.114736\n",
      "[05/22/2025 15:51:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=1.1147361993789673\n",
      "[05/22/2025 15:51:43 INFO 140473021069120] Epoch[195] Batch[5] avg_epoch_loss=0.857281\n",
      "[05/22/2025 15:51:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=0.8572810788949331\n",
      "[05/22/2025 15:51:43 INFO 140473021069120] Epoch[195] Batch [5]#011Speed: 361.31 samples/sec#011loss=0.857281\n",
      "[05/22/2025 15:51:44 INFO 140473021069120] processed a total of 1223 examples\n",
      "#metrics {\"StartTime\": 1747929099.6900547, \"EndTime\": 1747929104.4273496, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4737.001895904541, \"count\": 1, \"min\": 4737.001895904541, \"max\": 4737.001895904541}}}\n",
      "[05/22/2025 15:51:44 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=258.17498185348245 records/second\n",
      "[05/22/2025 15:51:44 INFO 140473021069120] #progress_metric: host=algo-1, completed 49.0 % of epochs\n",
      "[05/22/2025 15:51:44 INFO 140473021069120] #quality_metric: host=algo-1, epoch=195, train loss <loss>=0.9101906955242157\n",
      "[05/22/2025 15:51:44 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:45 INFO 140473021069120] Epoch[196] Batch[0] avg_epoch_loss=0.940045\n",
      "[05/22/2025 15:51:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=0.9400449991226196\n",
      "[05/22/2025 15:51:47 INFO 140473021069120] Epoch[196] Batch[5] avg_epoch_loss=0.959305\n",
      "[05/22/2025 15:51:47 INFO 140473021069120] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=0.959305206934611\n",
      "[05/22/2025 15:51:47 INFO 140473021069120] Epoch[196] Batch [5]#011Speed: 369.26 samples/sec#011loss=0.959305\n",
      "[05/22/2025 15:51:49 INFO 140473021069120] processed a total of 1270 examples\n",
      "#metrics {\"StartTime\": 1747929104.4274144, \"EndTime\": 1747929109.1402736, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4712.572336196899, \"count\": 1, \"min\": 4712.572336196899, \"max\": 4712.572336196899}}}\n",
      "[05/22/2025 15:51:49 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.48654008152135 records/second\n",
      "[05/22/2025 15:51:49 INFO 140473021069120] #progress_metric: host=algo-1, completed 49.25 % of epochs\n",
      "[05/22/2025 15:51:49 INFO 140473021069120] #quality_metric: host=algo-1, epoch=196, train loss <loss>=0.9048604488372802\n",
      "[05/22/2025 15:51:49 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:50 INFO 140473021069120] Epoch[197] Batch[0] avg_epoch_loss=0.802832\n",
      "[05/22/2025 15:51:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=0.8028320074081421\n",
      "[05/22/2025 15:51:52 INFO 140473021069120] Epoch[197] Batch[5] avg_epoch_loss=0.873690\n",
      "[05/22/2025 15:51:52 INFO 140473021069120] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=0.8736903667449951\n",
      "[05/22/2025 15:51:52 INFO 140473021069120] Epoch[197] Batch [5]#011Speed: 367.68 samples/sec#011loss=0.873690\n",
      "[05/22/2025 15:51:53 INFO 140473021069120] processed a total of 1259 examples\n",
      "#metrics {\"StartTime\": 1747929109.1403375, \"EndTime\": 1747929113.8359296, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4695.255517959595, \"count\": 1, \"min\": 4695.255517959595, \"max\": 4695.255517959595}}}\n",
      "[05/22/2025 15:51:53 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.13761580718653 records/second\n",
      "[05/22/2025 15:51:53 INFO 140473021069120] #progress_metric: host=algo-1, completed 49.5 % of epochs\n",
      "[05/22/2025 15:51:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=197, train loss <loss>=0.9037581741809845\n",
      "[05/22/2025 15:51:53 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:51:55 INFO 140473021069120] Epoch[198] Batch[0] avg_epoch_loss=0.893408\n",
      "[05/22/2025 15:51:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=0.8934081196784973\n",
      "[05/22/2025 15:51:57 INFO 140473021069120] Epoch[198] Batch[5] avg_epoch_loss=0.939762\n",
      "[05/22/2025 15:51:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=0.9397616883118948\n",
      "[05/22/2025 15:51:57 INFO 140473021069120] Epoch[198] Batch [5]#011Speed: 369.80 samples/sec#011loss=0.939762\n",
      "[05/22/2025 15:51:58 INFO 140473021069120] Epoch[198] Batch[10] avg_epoch_loss=0.533083\n",
      "[05/22/2025 15:51:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=198, batch=10 train loss <loss>=0.04506815671920776\n",
      "[05/22/2025 15:51:58 INFO 140473021069120] Epoch[198] Batch [10]#011Speed: 351.43 samples/sec#011loss=0.045068\n",
      "[05/22/2025 15:51:58 INFO 140473021069120] processed a total of 1314 examples\n",
      "#metrics {\"StartTime\": 1747929113.8359933, \"EndTime\": 1747929118.9120607, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5075.697660446167, \"count\": 1, \"min\": 5075.697660446167, \"max\": 5075.697660446167}}}\n",
      "[05/22/2025 15:51:58 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=258.87514716780447 records/second\n",
      "[05/22/2025 15:51:58 INFO 140473021069120] #progress_metric: host=algo-1, completed 49.75 % of epochs\n",
      "[05/22/2025 15:51:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=198, train loss <loss>=0.5330828103152189\n",
      "[05/22/2025 15:51:58 INFO 140473021069120] best epoch loss so far\n",
      "[05/22/2025 15:51:58 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/state_499f08b0-476e-4081-9eb5-e20cd1310028-0000.params\"\n",
      "#metrics {\"StartTime\": 1747929118.9121394, \"EndTime\": 1747929118.9475152, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 35.08162498474121, \"count\": 1, \"min\": 35.08162498474121, \"max\": 35.08162498474121}}}\n",
      "[05/22/2025 15:52:00 INFO 140473021069120] Epoch[199] Batch[0] avg_epoch_loss=0.912212\n",
      "[05/22/2025 15:52:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=0.9122117161750793\n",
      "[05/22/2025 15:52:02 INFO 140473021069120] Epoch[199] Batch[5] avg_epoch_loss=0.913581\n",
      "[05/22/2025 15:52:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=0.9135809342066447\n",
      "[05/22/2025 15:52:02 INFO 140473021069120] Epoch[199] Batch [5]#011Speed: 367.31 samples/sec#011loss=0.913581\n",
      "[05/22/2025 15:52:03 INFO 140473021069120] processed a total of 1260 examples\n",
      "#metrics {\"StartTime\": 1747929118.947574, \"EndTime\": 1747929123.62538, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4677.749872207642, \"count\": 1, \"min\": 4677.749872207642, \"max\": 4677.749872207642}}}\n",
      "[05/22/2025 15:52:03 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.35500491353616 records/second\n",
      "[05/22/2025 15:52:03 INFO 140473021069120] #progress_metric: host=algo-1, completed 50.0 % of epochs\n",
      "[05/22/2025 15:52:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=199, train loss <loss>=0.9245854914188385\n",
      "[05/22/2025 15:52:03 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:05 INFO 140473021069120] Epoch[200] Batch[0] avg_epoch_loss=0.538627\n",
      "[05/22/2025 15:52:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=0.5386268496513367\n",
      "[05/22/2025 15:52:06 INFO 140473021069120] Epoch[200] Batch[5] avg_epoch_loss=0.792758\n",
      "[05/22/2025 15:52:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=0.7927579681078593\n",
      "[05/22/2025 15:52:06 INFO 140473021069120] Epoch[200] Batch [5]#011Speed: 374.76 samples/sec#011loss=0.792758\n",
      "[05/22/2025 15:52:08 INFO 140473021069120] Epoch[200] Batch[10] avg_epoch_loss=0.998072\n",
      "[05/22/2025 15:52:08 INFO 140473021069120] #quality_metric: host=algo-1, epoch=200, batch=10 train loss <loss>=1.2444493532180787\n",
      "[05/22/2025 15:52:08 INFO 140473021069120] Epoch[200] Batch [10]#011Speed: 358.85 samples/sec#011loss=1.244449\n",
      "[05/22/2025 15:52:08 INFO 140473021069120] processed a total of 1288 examples\n",
      "#metrics {\"StartTime\": 1747929123.6254425, \"EndTime\": 1747929128.6683183, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5042.538166046143, \"count\": 1, \"min\": 5042.538166046143, \"max\": 5042.538166046143}}}\n",
      "[05/22/2025 15:52:08 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=255.42147484852407 records/second\n",
      "[05/22/2025 15:52:08 INFO 140473021069120] #progress_metric: host=algo-1, completed 50.25 % of epochs\n",
      "[05/22/2025 15:52:08 INFO 140473021069120] #quality_metric: host=algo-1, epoch=200, train loss <loss>=0.9980722340670499\n",
      "[05/22/2025 15:52:08 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:10 INFO 140473021069120] Epoch[201] Batch[0] avg_epoch_loss=1.022016\n",
      "[05/22/2025 15:52:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=1.0220160484313965\n",
      "[05/22/2025 15:52:11 INFO 140473021069120] Epoch[201] Batch[5] avg_epoch_loss=0.913556\n",
      "[05/22/2025 15:52:11 INFO 140473021069120] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=0.913555512825648\n",
      "[05/22/2025 15:52:11 INFO 140473021069120] Epoch[201] Batch [5]#011Speed: 372.90 samples/sec#011loss=0.913556\n",
      "[05/22/2025 15:52:13 INFO 140473021069120] processed a total of 1227 examples\n",
      "#metrics {\"StartTime\": 1747929128.6683784, \"EndTime\": 1747929133.329013, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4660.377502441406, \"count\": 1, \"min\": 4660.377502441406, \"max\": 4660.377502441406}}}\n",
      "[05/22/2025 15:52:13 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=263.27858425344306 records/second\n",
      "[05/22/2025 15:52:13 INFO 140473021069120] #progress_metric: host=algo-1, completed 50.5 % of epochs\n",
      "[05/22/2025 15:52:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=201, train loss <loss>=0.8867435783147812\n",
      "[05/22/2025 15:52:13 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:14 INFO 140473021069120] Epoch[202] Batch[0] avg_epoch_loss=0.747049\n",
      "[05/22/2025 15:52:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=0.7470490336418152\n",
      "[05/22/2025 15:52:16 INFO 140473021069120] Epoch[202] Batch[5] avg_epoch_loss=0.745275\n",
      "[05/22/2025 15:52:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=0.7452750404675802\n",
      "[05/22/2025 15:52:16 INFO 140473021069120] Epoch[202] Batch [5]#011Speed: 366.70 samples/sec#011loss=0.745275\n",
      "[05/22/2025 15:52:18 INFO 140473021069120] processed a total of 1223 examples\n",
      "#metrics {\"StartTime\": 1747929133.3290694, \"EndTime\": 1747929138.0429099, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4713.486909866333, \"count\": 1, \"min\": 4713.486909866333, \"max\": 4713.486909866333}}}\n",
      "[05/22/2025 15:52:18 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.4627741706955 records/second\n",
      "[05/22/2025 15:52:18 INFO 140473021069120] #progress_metric: host=algo-1, completed 50.75 % of epochs\n",
      "[05/22/2025 15:52:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=202, train loss <loss>=0.779075312614441\n",
      "[05/22/2025 15:52:18 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:19 INFO 140473021069120] Epoch[203] Batch[0] avg_epoch_loss=0.837434\n",
      "[05/22/2025 15:52:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=0.8374342918395996\n",
      "[05/22/2025 15:52:21 INFO 140473021069120] Epoch[203] Batch[5] avg_epoch_loss=0.812770\n",
      "[05/22/2025 15:52:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=0.812770406405131\n",
      "[05/22/2025 15:52:21 INFO 140473021069120] Epoch[203] Batch [5]#011Speed: 364.07 samples/sec#011loss=0.812770\n",
      "[05/22/2025 15:52:23 INFO 140473021069120] Epoch[203] Batch[10] avg_epoch_loss=0.867344\n",
      "[05/22/2025 15:52:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=203, batch=10 train loss <loss>=0.9328318238258362\n",
      "[05/22/2025 15:52:23 INFO 140473021069120] Epoch[203] Batch [10]#011Speed: 354.88 samples/sec#011loss=0.932832\n",
      "[05/22/2025 15:52:23 INFO 140473021069120] processed a total of 1314 examples\n",
      "#metrics {\"StartTime\": 1747929138.0429769, \"EndTime\": 1747929143.123577, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5080.322980880737, \"count\": 1, \"min\": 5080.322980880737, \"max\": 5080.322980880737}}}\n",
      "[05/22/2025 15:52:23 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=258.64045764022933 records/second\n",
      "[05/22/2025 15:52:23 INFO 140473021069120] #progress_metric: host=algo-1, completed 51.0 % of epochs\n",
      "[05/22/2025 15:52:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=203, train loss <loss>=0.867343777959997\n",
      "[05/22/2025 15:52:23 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:24 INFO 140473021069120] Epoch[204] Batch[0] avg_epoch_loss=0.931520\n",
      "[05/22/2025 15:52:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=0.9315202236175537\n",
      "[05/22/2025 15:52:26 INFO 140473021069120] Epoch[204] Batch[5] avg_epoch_loss=0.828600\n",
      "[05/22/2025 15:52:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=0.8285999298095703\n",
      "[05/22/2025 15:52:26 INFO 140473021069120] Epoch[204] Batch [5]#011Speed: 361.27 samples/sec#011loss=0.828600\n",
      "[05/22/2025 15:52:28 INFO 140473021069120] Epoch[204] Batch[10] avg_epoch_loss=1.037998\n",
      "[05/22/2025 15:52:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=204, batch=10 train loss <loss>=1.2892766833305358\n",
      "[05/22/2025 15:52:28 INFO 140473021069120] Epoch[204] Batch [10]#011Speed: 355.41 samples/sec#011loss=1.289277\n",
      "[05/22/2025 15:52:28 INFO 140473021069120] processed a total of 1293 examples\n",
      "#metrics {\"StartTime\": 1747929143.123637, \"EndTime\": 1747929148.2534657, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5129.564046859741, \"count\": 1, \"min\": 5129.564046859741, \"max\": 5129.564046859741}}}\n",
      "[05/22/2025 15:52:28 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=252.06362380853625 records/second\n",
      "[05/22/2025 15:52:28 INFO 140473021069120] #progress_metric: host=algo-1, completed 51.25 % of epochs\n",
      "[05/22/2025 15:52:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=204, train loss <loss>=1.037998454137282\n",
      "[05/22/2025 15:52:28 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:29 INFO 140473021069120] Epoch[205] Batch[0] avg_epoch_loss=1.099403\n",
      "[05/22/2025 15:52:29 INFO 140473021069120] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=1.0994027853012085\n",
      "[05/22/2025 15:52:31 INFO 140473021069120] Epoch[205] Batch[5] avg_epoch_loss=1.029573\n",
      "[05/22/2025 15:52:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=1.0295726259549458\n",
      "[05/22/2025 15:52:31 INFO 140473021069120] Epoch[205] Batch [5]#011Speed: 364.15 samples/sec#011loss=1.029573\n",
      "[05/22/2025 15:52:33 INFO 140473021069120] processed a total of 1245 examples\n",
      "#metrics {\"StartTime\": 1747929148.2535284, \"EndTime\": 1747929153.0013814, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4747.565984725952, \"count\": 1, \"min\": 4747.565984725952, \"max\": 4747.565984725952}}}\n",
      "[05/22/2025 15:52:33 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=262.23350429697365 records/second\n",
      "[05/22/2025 15:52:33 INFO 140473021069120] #progress_metric: host=algo-1, completed 51.5 % of epochs\n",
      "[05/22/2025 15:52:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=205, train loss <loss>=1.0075975418090821\n",
      "[05/22/2025 15:52:33 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:34 INFO 140473021069120] Epoch[206] Batch[0] avg_epoch_loss=0.977883\n",
      "[05/22/2025 15:52:34 INFO 140473021069120] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=0.9778829216957092\n",
      "[05/22/2025 15:52:36 INFO 140473021069120] Epoch[206] Batch[5] avg_epoch_loss=0.959580\n",
      "[05/22/2025 15:52:36 INFO 140473021069120] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=0.9595799644788107\n",
      "[05/22/2025 15:52:36 INFO 140473021069120] Epoch[206] Batch [5]#011Speed: 367.70 samples/sec#011loss=0.959580\n",
      "[05/22/2025 15:52:38 INFO 140473021069120] Epoch[206] Batch[10] avg_epoch_loss=0.942619\n",
      "[05/22/2025 15:52:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=206, batch=10 train loss <loss>=0.9222649097442627\n",
      "[05/22/2025 15:52:38 INFO 140473021069120] Epoch[206] Batch [10]#011Speed: 352.48 samples/sec#011loss=0.922265\n",
      "[05/22/2025 15:52:38 INFO 140473021069120] processed a total of 1288 examples\n",
      "#metrics {\"StartTime\": 1747929153.0014577, \"EndTime\": 1747929158.1053584, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5103.424787521362, \"count\": 1, \"min\": 5103.424787521362, \"max\": 5103.424787521362}}}\n",
      "[05/22/2025 15:52:38 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=252.37498888730624 records/second\n",
      "[05/22/2025 15:52:38 INFO 140473021069120] #progress_metric: host=algo-1, completed 51.75 % of epochs\n",
      "[05/22/2025 15:52:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=206, train loss <loss>=0.942618575963107\n",
      "[05/22/2025 15:52:38 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:39 INFO 140473021069120] Epoch[207] Batch[0] avg_epoch_loss=1.024591\n",
      "[05/22/2025 15:52:39 INFO 140473021069120] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=1.0245912075042725\n",
      "[05/22/2025 15:52:41 INFO 140473021069120] Epoch[207] Batch[5] avg_epoch_loss=0.878645\n",
      "[05/22/2025 15:52:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=0.8786452313264211\n",
      "[05/22/2025 15:52:41 INFO 140473021069120] Epoch[207] Batch [5]#011Speed: 365.09 samples/sec#011loss=0.878645\n",
      "[05/22/2025 15:52:43 INFO 140473021069120] Epoch[207] Batch[10] avg_epoch_loss=0.802038\n",
      "[05/22/2025 15:52:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=207, batch=10 train loss <loss>=0.7101086348295211\n",
      "[05/22/2025 15:52:43 INFO 140473021069120] Epoch[207] Batch [10]#011Speed: 352.56 samples/sec#011loss=0.710109\n",
      "[05/22/2025 15:52:43 INFO 140473021069120] processed a total of 1322 examples\n",
      "#metrics {\"StartTime\": 1747929158.1054184, \"EndTime\": 1747929163.1950603, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5089.322805404663, \"count\": 1, \"min\": 5089.322805404663, \"max\": 5089.322805404663}}}\n",
      "[05/22/2025 15:52:43 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.7551174598511 records/second\n",
      "[05/22/2025 15:52:43 INFO 140473021069120] #progress_metric: host=algo-1, completed 52.0 % of epochs\n",
      "[05/22/2025 15:52:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=207, train loss <loss>=0.8020376874641939\n",
      "[05/22/2025 15:52:43 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:44 INFO 140473021069120] Epoch[208] Batch[0] avg_epoch_loss=0.891614\n",
      "[05/22/2025 15:52:44 INFO 140473021069120] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=0.8916140794754028\n",
      "[05/22/2025 15:52:46 INFO 140473021069120] Epoch[208] Batch[5] avg_epoch_loss=0.863055\n",
      "[05/22/2025 15:52:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=0.8630547026793162\n",
      "[05/22/2025 15:52:46 INFO 140473021069120] Epoch[208] Batch [5]#011Speed: 363.58 samples/sec#011loss=0.863055\n",
      "[05/22/2025 15:52:47 INFO 140473021069120] processed a total of 1258 examples\n",
      "#metrics {\"StartTime\": 1747929163.1951184, \"EndTime\": 1747929167.9155498, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4720.180034637451, \"count\": 1, \"min\": 4720.180034637451, \"max\": 4720.180034637451}}}\n",
      "[05/22/2025 15:52:47 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=266.5103849677082 records/second\n",
      "[05/22/2025 15:52:47 INFO 140473021069120] #progress_metric: host=algo-1, completed 52.25 % of epochs\n",
      "[05/22/2025 15:52:47 INFO 140473021069120] #quality_metric: host=algo-1, epoch=208, train loss <loss>=0.9095950484275818\n",
      "[05/22/2025 15:52:47 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:49 INFO 140473021069120] Epoch[209] Batch[0] avg_epoch_loss=0.923271\n",
      "[05/22/2025 15:52:49 INFO 140473021069120] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=0.9232713580131531\n",
      "[05/22/2025 15:52:51 INFO 140473021069120] Epoch[209] Batch[5] avg_epoch_loss=0.846138\n",
      "[05/22/2025 15:52:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=0.84613769253095\n",
      "[05/22/2025 15:52:51 INFO 140473021069120] Epoch[209] Batch [5]#011Speed: 367.80 samples/sec#011loss=0.846138\n",
      "[05/22/2025 15:52:53 INFO 140473021069120] Epoch[209] Batch[10] avg_epoch_loss=0.742051\n",
      "[05/22/2025 15:52:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=209, batch=10 train loss <loss>=0.6171476542949677\n",
      "[05/22/2025 15:52:53 INFO 140473021069120] Epoch[209] Batch [10]#011Speed: 347.07 samples/sec#011loss=0.617148\n",
      "[05/22/2025 15:52:53 INFO 140473021069120] processed a total of 1300 examples\n",
      "#metrics {\"StartTime\": 1747929167.9156075, \"EndTime\": 1747929173.052184, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5136.229515075684, \"count\": 1, \"min\": 5136.229515075684, \"max\": 5136.229515075684}}}\n",
      "[05/22/2025 15:52:53 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.09950148275888 records/second\n",
      "[05/22/2025 15:52:53 INFO 140473021069120] #progress_metric: host=algo-1, completed 52.5 % of epochs\n",
      "[05/22/2025 15:52:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=209, train loss <loss>=0.7420513115145944\n",
      "[05/22/2025 15:52:53 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:54 INFO 140473021069120] Epoch[210] Batch[0] avg_epoch_loss=1.179523\n",
      "[05/22/2025 15:52:54 INFO 140473021069120] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=1.179523229598999\n",
      "[05/22/2025 15:52:56 INFO 140473021069120] Epoch[210] Batch[5] avg_epoch_loss=1.019052\n",
      "[05/22/2025 15:52:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=1.019051859776179\n",
      "[05/22/2025 15:52:56 INFO 140473021069120] Epoch[210] Batch [5]#011Speed: 368.09 samples/sec#011loss=1.019052\n",
      "[05/22/2025 15:52:58 INFO 140473021069120] Epoch[210] Batch[10] avg_epoch_loss=1.057523\n",
      "[05/22/2025 15:52:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=210, batch=10 train loss <loss>=1.1036873936653138\n",
      "[05/22/2025 15:52:58 INFO 140473021069120] Epoch[210] Batch [10]#011Speed: 352.65 samples/sec#011loss=1.103687\n",
      "[05/22/2025 15:52:58 INFO 140473021069120] processed a total of 1309 examples\n",
      "#metrics {\"StartTime\": 1747929173.0522447, \"EndTime\": 1747929178.1511602, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5098.648309707642, \"count\": 1, \"min\": 5098.648309707642, \"max\": 5098.648309707642}}}\n",
      "[05/22/2025 15:52:58 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.7294887858793 records/second\n",
      "[05/22/2025 15:52:58 INFO 140473021069120] #progress_metric: host=algo-1, completed 52.75 % of epochs\n",
      "[05/22/2025 15:52:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=210, train loss <loss>=1.057522556998513\n",
      "[05/22/2025 15:52:58 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:52:59 INFO 140473021069120] Epoch[211] Batch[0] avg_epoch_loss=0.859631\n",
      "[05/22/2025 15:52:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=0.8596311807632446\n",
      "[05/22/2025 15:53:01 INFO 140473021069120] Epoch[211] Batch[5] avg_epoch_loss=0.837243\n",
      "[05/22/2025 15:53:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=0.8372430801391602\n",
      "[05/22/2025 15:53:01 INFO 140473021069120] Epoch[211] Batch [5]#011Speed: 363.34 samples/sec#011loss=0.837243\n",
      "[05/22/2025 15:53:02 INFO 140473021069120] processed a total of 1280 examples\n",
      "#metrics {\"StartTime\": 1747929178.1512203, \"EndTime\": 1747929182.9104323, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4758.879899978638, \"count\": 1, \"min\": 4758.879899978638, \"max\": 4758.879899978638}}}\n",
      "[05/22/2025 15:53:02 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.9657623453282 records/second\n",
      "[05/22/2025 15:53:02 INFO 140473021069120] #progress_metric: host=algo-1, completed 53.0 % of epochs\n",
      "[05/22/2025 15:53:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=211, train loss <loss>=0.8464046657085419\n",
      "[05/22/2025 15:53:02 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:04 INFO 140473021069120] Epoch[212] Batch[0] avg_epoch_loss=1.042909\n",
      "[05/22/2025 15:53:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=1.0429089069366455\n",
      "[05/22/2025 15:53:06 INFO 140473021069120] Epoch[212] Batch[5] avg_epoch_loss=0.879809\n",
      "[05/22/2025 15:53:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=0.8798089722792307\n",
      "[05/22/2025 15:53:06 INFO 140473021069120] Epoch[212] Batch [5]#011Speed: 367.04 samples/sec#011loss=0.879809\n",
      "[05/22/2025 15:53:07 INFO 140473021069120] processed a total of 1265 examples\n",
      "#metrics {\"StartTime\": 1747929182.910492, \"EndTime\": 1747929187.6187217, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4707.880735397339, \"count\": 1, \"min\": 4707.880735397339, \"max\": 4707.880735397339}}}\n",
      "[05/22/2025 15:53:07 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.69307509710245 records/second\n",
      "[05/22/2025 15:53:07 INFO 140473021069120] #progress_metric: host=algo-1, completed 53.25 % of epochs\n",
      "[05/22/2025 15:53:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=212, train loss <loss>=0.846161562204361\n",
      "[05/22/2025 15:53:07 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:09 INFO 140473021069120] Epoch[213] Batch[0] avg_epoch_loss=1.068997\n",
      "[05/22/2025 15:53:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=1.068996787071228\n",
      "[05/22/2025 15:53:10 INFO 140473021069120] Epoch[213] Batch[5] avg_epoch_loss=0.937164\n",
      "[05/22/2025 15:53:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=0.9371643662452698\n",
      "[05/22/2025 15:53:10 INFO 140473021069120] Epoch[213] Batch [5]#011Speed: 365.72 samples/sec#011loss=0.937164\n",
      "[05/22/2025 15:53:12 INFO 140473021069120] Epoch[213] Batch[10] avg_epoch_loss=0.951057\n",
      "[05/22/2025 15:53:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=213, batch=10 train loss <loss>=0.9677281737327575\n",
      "[05/22/2025 15:53:12 INFO 140473021069120] Epoch[213] Batch [10]#011Speed: 348.71 samples/sec#011loss=0.967728\n",
      "[05/22/2025 15:53:12 INFO 140473021069120] processed a total of 1324 examples\n",
      "#metrics {\"StartTime\": 1747929187.6187847, \"EndTime\": 1747929192.7228968, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5103.804111480713, \"count\": 1, \"min\": 5103.804111480713, \"max\": 5103.804111480713}}}\n",
      "[05/22/2025 15:53:12 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.4094976733644 records/second\n",
      "[05/22/2025 15:53:12 INFO 140473021069120] #progress_metric: host=algo-1, completed 53.5 % of epochs\n",
      "[05/22/2025 15:53:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=213, train loss <loss>=0.9510570060123097\n",
      "[05/22/2025 15:53:12 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:14 INFO 140473021069120] Epoch[214] Batch[0] avg_epoch_loss=0.930748\n",
      "[05/22/2025 15:53:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=0.9307484030723572\n",
      "[05/22/2025 15:53:16 INFO 140473021069120] Epoch[214] Batch[5] avg_epoch_loss=0.893748\n",
      "[05/22/2025 15:53:16 INFO 140473021069120] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=0.8937483628590902\n",
      "[05/22/2025 15:53:16 INFO 140473021069120] Epoch[214] Batch [5]#011Speed: 361.70 samples/sec#011loss=0.893748\n",
      "[05/22/2025 15:53:17 INFO 140473021069120] Epoch[214] Batch[10] avg_epoch_loss=0.950799\n",
      "[05/22/2025 15:53:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=214, batch=10 train loss <loss>=1.0192593932151794\n",
      "[05/22/2025 15:53:17 INFO 140473021069120] Epoch[214] Batch [10]#011Speed: 355.98 samples/sec#011loss=1.019259\n",
      "[05/22/2025 15:53:17 INFO 140473021069120] processed a total of 1284 examples\n",
      "#metrics {\"StartTime\": 1747929192.722961, \"EndTime\": 1747929197.8488264, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5125.54144859314, \"count\": 1, \"min\": 5125.54144859314, \"max\": 5125.54144859314}}}\n",
      "[05/22/2025 15:53:17 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=250.5050388829182 records/second\n",
      "[05/22/2025 15:53:17 INFO 140473021069120] #progress_metric: host=algo-1, completed 53.75 % of epochs\n",
      "[05/22/2025 15:53:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=214, train loss <loss>=0.9507988312027671\n",
      "[05/22/2025 15:53:17 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:19 INFO 140473021069120] Epoch[215] Batch[0] avg_epoch_loss=0.937152\n",
      "[05/22/2025 15:53:19 INFO 140473021069120] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=0.9371516108512878\n",
      "[05/22/2025 15:53:21 INFO 140473021069120] Epoch[215] Batch[5] avg_epoch_loss=0.762413\n",
      "[05/22/2025 15:53:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=0.7624127467473348\n",
      "[05/22/2025 15:53:21 INFO 140473021069120] Epoch[215] Batch [5]#011Speed: 365.65 samples/sec#011loss=0.762413\n",
      "[05/22/2025 15:53:22 INFO 140473021069120] processed a total of 1196 examples\n",
      "#metrics {\"StartTime\": 1747929197.848901, \"EndTime\": 1747929202.546759, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4697.575330734253, \"count\": 1, \"min\": 4697.575330734253, \"max\": 4697.575330734253}}}\n",
      "[05/22/2025 15:53:22 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=254.59294307257454 records/second\n",
      "[05/22/2025 15:53:22 INFO 140473021069120] #progress_metric: host=algo-1, completed 54.0 % of epochs\n",
      "[05/22/2025 15:53:22 INFO 140473021069120] #quality_metric: host=algo-1, epoch=215, train loss <loss>=0.8135919272899628\n",
      "[05/22/2025 15:53:22 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:24 INFO 140473021069120] Epoch[216] Batch[0] avg_epoch_loss=0.618645\n",
      "[05/22/2025 15:53:24 INFO 140473021069120] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=0.6186447143554688\n",
      "[05/22/2025 15:53:25 INFO 140473021069120] Epoch[216] Batch[5] avg_epoch_loss=0.787283\n",
      "[05/22/2025 15:53:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=0.7872830331325531\n",
      "[05/22/2025 15:53:25 INFO 140473021069120] Epoch[216] Batch [5]#011Speed: 365.61 samples/sec#011loss=0.787283\n",
      "[05/22/2025 15:53:27 INFO 140473021069120] Epoch[216] Batch[10] avg_epoch_loss=1.011290\n",
      "[05/22/2025 15:53:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=216, batch=10 train loss <loss>=1.2800982594490051\n",
      "[05/22/2025 15:53:27 INFO 140473021069120] Epoch[216] Batch [10]#011Speed: 357.01 samples/sec#011loss=1.280098\n",
      "[05/22/2025 15:53:27 INFO 140473021069120] processed a total of 1288 examples\n",
      "#metrics {\"StartTime\": 1747929202.5468318, \"EndTime\": 1747929207.647036, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5099.838733673096, \"count\": 1, \"min\": 5099.838733673096, \"max\": 5099.838733673096}}}\n",
      "[05/22/2025 15:53:27 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=252.55258989816485 records/second\n",
      "[05/22/2025 15:53:27 INFO 140473021069120] #progress_metric: host=algo-1, completed 54.25 % of epochs\n",
      "[05/22/2025 15:53:27 INFO 140473021069120] #quality_metric: host=algo-1, epoch=216, train loss <loss>=1.0112899541854858\n",
      "[05/22/2025 15:53:27 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:29 INFO 140473021069120] Epoch[217] Batch[0] avg_epoch_loss=0.923277\n",
      "[05/22/2025 15:53:29 INFO 140473021069120] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=0.9232773184776306\n",
      "[05/22/2025 15:53:30 INFO 140473021069120] Epoch[217] Batch[5] avg_epoch_loss=0.887626\n",
      "[05/22/2025 15:53:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=0.8876264194647471\n",
      "[05/22/2025 15:53:30 INFO 140473021069120] Epoch[217] Batch [5]#011Speed: 367.87 samples/sec#011loss=0.887626\n",
      "[05/22/2025 15:53:32 INFO 140473021069120] processed a total of 1275 examples\n",
      "#metrics {\"StartTime\": 1747929207.6470962, \"EndTime\": 1747929212.3557854, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4708.440780639648, \"count\": 1, \"min\": 4708.440780639648, \"max\": 4708.440780639648}}}\n",
      "[05/22/2025 15:53:32 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=270.7846039753009 records/second\n",
      "[05/22/2025 15:53:32 INFO 140473021069120] #progress_metric: host=algo-1, completed 54.5 % of epochs\n",
      "[05/22/2025 15:53:32 INFO 140473021069120] #quality_metric: host=algo-1, epoch=217, train loss <loss>=0.8862441718578339\n",
      "[05/22/2025 15:53:32 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:33 INFO 140473021069120] Epoch[218] Batch[0] avg_epoch_loss=0.985094\n",
      "[05/22/2025 15:53:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=218, batch=0 train loss <loss>=0.985093891620636\n",
      "[05/22/2025 15:53:35 INFO 140473021069120] Epoch[218] Batch[5] avg_epoch_loss=0.906737\n",
      "[05/22/2025 15:53:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=218, batch=5 train loss <loss>=0.9067373474438986\n",
      "[05/22/2025 15:53:35 INFO 140473021069120] Epoch[218] Batch [5]#011Speed: 363.14 samples/sec#011loss=0.906737\n",
      "[05/22/2025 15:53:37 INFO 140473021069120] processed a total of 1214 examples\n",
      "#metrics {\"StartTime\": 1747929212.3558543, \"EndTime\": 1747929217.0812604, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4725.037336349487, \"count\": 1, \"min\": 4725.037336349487, \"max\": 4725.037336349487}}}\n",
      "[05/22/2025 15:53:37 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.9240529584297 records/second\n",
      "[05/22/2025 15:53:37 INFO 140473021069120] #progress_metric: host=algo-1, completed 54.75 % of epochs\n",
      "[05/22/2025 15:53:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=218, train loss <loss>=0.886192399263382\n",
      "[05/22/2025 15:53:37 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:38 INFO 140473021069120] Epoch[219] Batch[0] avg_epoch_loss=0.843711\n",
      "[05/22/2025 15:53:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=219, batch=0 train loss <loss>=0.8437106609344482\n",
      "[05/22/2025 15:53:40 INFO 140473021069120] Epoch[219] Batch[5] avg_epoch_loss=0.769252\n",
      "[05/22/2025 15:53:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=219, batch=5 train loss <loss>=0.7692518631617228\n",
      "[05/22/2025 15:53:40 INFO 140473021069120] Epoch[219] Batch [5]#011Speed: 362.42 samples/sec#011loss=0.769252\n",
      "[05/22/2025 15:53:41 INFO 140473021069120] processed a total of 1257 examples\n",
      "#metrics {\"StartTime\": 1747929217.0813239, \"EndTime\": 1747929221.7974417, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4715.822219848633, \"count\": 1, \"min\": 4715.822219848633, \"max\": 4715.822219848633}}}\n",
      "[05/22/2025 15:53:41 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=266.5442867529984 records/second\n",
      "[05/22/2025 15:53:41 INFO 140473021069120] #progress_metric: host=algo-1, completed 55.0 % of epochs\n",
      "[05/22/2025 15:53:41 INFO 140473021069120] #quality_metric: host=algo-1, epoch=219, train loss <loss>=0.7672994196414947\n",
      "[05/22/2025 15:53:41 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:43 INFO 140473021069120] Epoch[220] Batch[0] avg_epoch_loss=0.813940\n",
      "[05/22/2025 15:53:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=220, batch=0 train loss <loss>=0.8139403462409973\n",
      "[05/22/2025 15:53:45 INFO 140473021069120] Epoch[220] Batch[5] avg_epoch_loss=0.766102\n",
      "[05/22/2025 15:53:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=220, batch=5 train loss <loss>=0.7661019364992777\n",
      "[05/22/2025 15:53:45 INFO 140473021069120] Epoch[220] Batch [5]#011Speed: 380.10 samples/sec#011loss=0.766102\n",
      "[05/22/2025 15:53:46 INFO 140473021069120] processed a total of 1256 examples\n",
      "#metrics {\"StartTime\": 1747929221.7975042, \"EndTime\": 1747929226.3773572, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4579.546928405762, \"count\": 1, \"min\": 4579.546928405762, \"max\": 4579.546928405762}}}\n",
      "[05/22/2025 15:53:46 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=274.2572419967482 records/second\n",
      "[05/22/2025 15:53:46 INFO 140473021069120] #progress_metric: host=algo-1, completed 55.25 % of epochs\n",
      "[05/22/2025 15:53:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=220, train loss <loss>=0.8359389245510102\n",
      "[05/22/2025 15:53:46 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:47 INFO 140473021069120] Epoch[221] Batch[0] avg_epoch_loss=0.539321\n",
      "[05/22/2025 15:53:47 INFO 140473021069120] #quality_metric: host=algo-1, epoch=221, batch=0 train loss <loss>=0.5393208265304565\n",
      "[05/22/2025 15:53:49 INFO 140473021069120] Epoch[221] Batch[5] avg_epoch_loss=0.809440\n",
      "[05/22/2025 15:53:49 INFO 140473021069120] #quality_metric: host=algo-1, epoch=221, batch=5 train loss <loss>=0.8094402650992075\n",
      "[05/22/2025 15:53:49 INFO 140473021069120] Epoch[221] Batch [5]#011Speed: 374.63 samples/sec#011loss=0.809440\n",
      "[05/22/2025 15:53:51 INFO 140473021069120] Epoch[221] Batch[10] avg_epoch_loss=0.827432\n",
      "[05/22/2025 15:53:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=221, batch=10 train loss <loss>=0.8490209937095642\n",
      "[05/22/2025 15:53:51 INFO 140473021069120] Epoch[221] Batch [10]#011Speed: 362.92 samples/sec#011loss=0.849021\n",
      "[05/22/2025 15:53:51 INFO 140473021069120] processed a total of 1308 examples\n",
      "#metrics {\"StartTime\": 1747929226.3774223, \"EndTime\": 1747929231.388749, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5011.014223098755, \"count\": 1, \"min\": 5011.014223098755, \"max\": 5011.014223098755}}}\n",
      "[05/22/2025 15:53:51 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=261.020494355336 records/second\n",
      "[05/22/2025 15:53:51 INFO 140473021069120] #progress_metric: host=algo-1, completed 55.5 % of epochs\n",
      "[05/22/2025 15:53:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=221, train loss <loss>=0.8274315053766425\n",
      "[05/22/2025 15:53:51 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:52 INFO 140473021069120] Epoch[222] Batch[0] avg_epoch_loss=0.782189\n",
      "[05/22/2025 15:53:52 INFO 140473021069120] #quality_metric: host=algo-1, epoch=222, batch=0 train loss <loss>=0.7821885347366333\n",
      "[05/22/2025 15:53:54 INFO 140473021069120] Epoch[222] Batch[5] avg_epoch_loss=0.967794\n",
      "[05/22/2025 15:53:54 INFO 140473021069120] #quality_metric: host=algo-1, epoch=222, batch=5 train loss <loss>=0.9677935341993967\n",
      "[05/22/2025 15:53:54 INFO 140473021069120] Epoch[222] Batch [5]#011Speed: 374.19 samples/sec#011loss=0.967794\n",
      "[05/22/2025 15:53:56 INFO 140473021069120] Epoch[222] Batch[10] avg_epoch_loss=0.775150\n",
      "[05/22/2025 15:53:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=222, batch=10 train loss <loss>=0.5439785957336426\n",
      "[05/22/2025 15:53:56 INFO 140473021069120] Epoch[222] Batch [10]#011Speed: 368.55 samples/sec#011loss=0.543979\n",
      "[05/22/2025 15:53:56 INFO 140473021069120] processed a total of 1299 examples\n",
      "#metrics {\"StartTime\": 1747929231.3888075, \"EndTime\": 1747929236.3704202, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4980.833053588867, \"count\": 1, \"min\": 4980.833053588867, \"max\": 4980.833053588867}}}\n",
      "[05/22/2025 15:53:56 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.7951030802001 records/second\n",
      "[05/22/2025 15:53:56 INFO 140473021069120] #progress_metric: host=algo-1, completed 55.75 % of epochs\n",
      "[05/22/2025 15:53:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=222, train loss <loss>=0.7751503803513267\n",
      "[05/22/2025 15:53:56 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:53:57 INFO 140473021069120] Epoch[223] Batch[0] avg_epoch_loss=0.979068\n",
      "[05/22/2025 15:53:57 INFO 140473021069120] #quality_metric: host=algo-1, epoch=223, batch=0 train loss <loss>=0.979068398475647\n",
      "[05/22/2025 15:53:59 INFO 140473021069120] Epoch[223] Batch[5] avg_epoch_loss=1.011636\n",
      "[05/22/2025 15:53:59 INFO 140473021069120] #quality_metric: host=algo-1, epoch=223, batch=5 train loss <loss>=1.0116355220476787\n",
      "[05/22/2025 15:53:59 INFO 140473021069120] Epoch[223] Batch [5]#011Speed: 378.53 samples/sec#011loss=1.011636\n",
      "[05/22/2025 15:54:01 INFO 140473021069120] Epoch[223] Batch[10] avg_epoch_loss=1.255643\n",
      "[05/22/2025 15:54:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=223, batch=10 train loss <loss>=1.5484509825706483\n",
      "[05/22/2025 15:54:01 INFO 140473021069120] Epoch[223] Batch [10]#011Speed: 365.88 samples/sec#011loss=1.548451\n",
      "[05/22/2025 15:54:01 INFO 140473021069120] processed a total of 1293 examples\n",
      "#metrics {\"StartTime\": 1747929236.370479, \"EndTime\": 1747929241.3384047, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4967.6055908203125, \"count\": 1, \"min\": 4967.6055908203125, \"max\": 4967.6055908203125}}}\n",
      "[05/22/2025 15:54:01 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.2815676228564 records/second\n",
      "[05/22/2025 15:54:01 INFO 140473021069120] #progress_metric: host=algo-1, completed 56.0 % of epochs\n",
      "[05/22/2025 15:54:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=223, train loss <loss>=1.2556425495581194\n",
      "[05/22/2025 15:54:01 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:02 INFO 140473021069120] Epoch[224] Batch[0] avg_epoch_loss=0.889995\n",
      "[05/22/2025 15:54:02 INFO 140473021069120] #quality_metric: host=algo-1, epoch=224, batch=0 train loss <loss>=0.8899953365325928\n",
      "[05/22/2025 15:54:04 INFO 140473021069120] Epoch[224] Batch[5] avg_epoch_loss=0.880853\n",
      "[05/22/2025 15:54:04 INFO 140473021069120] #quality_metric: host=algo-1, epoch=224, batch=5 train loss <loss>=0.8808531363805135\n",
      "[05/22/2025 15:54:04 INFO 140473021069120] Epoch[224] Batch [5]#011Speed: 369.50 samples/sec#011loss=0.880853\n",
      "[05/22/2025 15:54:06 INFO 140473021069120] processed a total of 1262 examples\n",
      "#metrics {\"StartTime\": 1747929241.3384666, \"EndTime\": 1747929246.0271177, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4688.352823257446, \"count\": 1, \"min\": 4688.352823257446, \"max\": 4688.352823257446}}}\n",
      "[05/22/2025 15:54:06 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=269.17242364794447 records/second\n",
      "[05/22/2025 15:54:06 INFO 140473021069120] #progress_metric: host=algo-1, completed 56.25 % of epochs\n",
      "[05/22/2025 15:54:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=224, train loss <loss>=0.8506381750106812\n",
      "[05/22/2025 15:54:06 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:07 INFO 140473021069120] Epoch[225] Batch[0] avg_epoch_loss=0.740658\n",
      "[05/22/2025 15:54:07 INFO 140473021069120] #quality_metric: host=algo-1, epoch=225, batch=0 train loss <loss>=0.7406576871871948\n",
      "[05/22/2025 15:54:09 INFO 140473021069120] Epoch[225] Batch[5] avg_epoch_loss=0.841972\n",
      "[05/22/2025 15:54:09 INFO 140473021069120] #quality_metric: host=algo-1, epoch=225, batch=5 train loss <loss>=0.8419715762138367\n",
      "[05/22/2025 15:54:09 INFO 140473021069120] Epoch[225] Batch [5]#011Speed: 367.60 samples/sec#011loss=0.841972\n",
      "[05/22/2025 15:54:10 INFO 140473021069120] processed a total of 1267 examples\n",
      "#metrics {\"StartTime\": 1747929246.02718, \"EndTime\": 1747929250.7637014, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4736.224889755249, \"count\": 1, \"min\": 4736.224889755249, \"max\": 4736.224889755249}}}\n",
      "[05/22/2025 15:54:10 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=267.50715423576486 records/second\n",
      "[05/22/2025 15:54:10 INFO 140473021069120] #progress_metric: host=algo-1, completed 56.5 % of epochs\n",
      "[05/22/2025 15:54:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=225, train loss <loss>=0.8622779190540314\n",
      "[05/22/2025 15:54:10 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:12 INFO 140473021069120] Epoch[226] Batch[0] avg_epoch_loss=0.915662\n",
      "[05/22/2025 15:54:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=226, batch=0 train loss <loss>=0.9156616926193237\n",
      "[05/22/2025 15:54:14 INFO 140473021069120] Epoch[226] Batch[5] avg_epoch_loss=0.853493\n",
      "[05/22/2025 15:54:14 INFO 140473021069120] #quality_metric: host=algo-1, epoch=226, batch=5 train loss <loss>=0.8534931341807047\n",
      "[05/22/2025 15:54:14 INFO 140473021069120] Epoch[226] Batch [5]#011Speed: 364.46 samples/sec#011loss=0.853493\n",
      "[05/22/2025 15:54:15 INFO 140473021069120] processed a total of 1229 examples\n",
      "#metrics {\"StartTime\": 1747929250.7637687, \"EndTime\": 1747929255.4775836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4713.481903076172, \"count\": 1, \"min\": 4713.481903076172, \"max\": 4713.481903076172}}}\n",
      "[05/22/2025 15:54:15 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.73612569396187 records/second\n",
      "[05/22/2025 15:54:15 INFO 140473021069120] #progress_metric: host=algo-1, completed 56.75 % of epochs\n",
      "[05/22/2025 15:54:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=226, train loss <loss>=0.8307494997978211\n",
      "[05/22/2025 15:54:15 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:17 INFO 140473021069120] Epoch[227] Batch[0] avg_epoch_loss=0.955796\n",
      "[05/22/2025 15:54:17 INFO 140473021069120] #quality_metric: host=algo-1, epoch=227, batch=0 train loss <loss>=0.9557961821556091\n",
      "[05/22/2025 15:54:18 INFO 140473021069120] Epoch[227] Batch[5] avg_epoch_loss=0.771319\n",
      "[05/22/2025 15:54:18 INFO 140473021069120] #quality_metric: host=algo-1, epoch=227, batch=5 train loss <loss>=0.7713187436262766\n",
      "[05/22/2025 15:54:18 INFO 140473021069120] Epoch[227] Batch [5]#011Speed: 366.35 samples/sec#011loss=0.771319\n",
      "[05/22/2025 15:54:20 INFO 140473021069120] processed a total of 1268 examples\n",
      "#metrics {\"StartTime\": 1747929255.4776487, \"EndTime\": 1747929260.2013688, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4723.368167877197, \"count\": 1, \"min\": 4723.368167877197, \"max\": 4723.368167877197}}}\n",
      "[05/22/2025 15:54:20 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.4471220175171 records/second\n",
      "[05/22/2025 15:54:20 INFO 140473021069120] #progress_metric: host=algo-1, completed 57.0 % of epochs\n",
      "[05/22/2025 15:54:20 INFO 140473021069120] #quality_metric: host=algo-1, epoch=227, train loss <loss>=0.8329696834087372\n",
      "[05/22/2025 15:54:20 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:21 INFO 140473021069120] Epoch[228] Batch[0] avg_epoch_loss=0.634608\n",
      "[05/22/2025 15:54:21 INFO 140473021069120] #quality_metric: host=algo-1, epoch=228, batch=0 train loss <loss>=0.6346080899238586\n",
      "[05/22/2025 15:54:23 INFO 140473021069120] Epoch[228] Batch[5] avg_epoch_loss=0.711615\n",
      "[05/22/2025 15:54:23 INFO 140473021069120] #quality_metric: host=algo-1, epoch=228, batch=5 train loss <loss>=0.7116147776444753\n",
      "[05/22/2025 15:54:23 INFO 140473021069120] Epoch[228] Batch [5]#011Speed: 368.36 samples/sec#011loss=0.711615\n",
      "[05/22/2025 15:54:25 INFO 140473021069120] Epoch[228] Batch[10] avg_epoch_loss=0.841282\n",
      "[05/22/2025 15:54:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=228, batch=10 train loss <loss>=0.9968827486038208\n",
      "[05/22/2025 15:54:25 INFO 140473021069120] Epoch[228] Batch [10]#011Speed: 352.45 samples/sec#011loss=0.996883\n",
      "[05/22/2025 15:54:25 INFO 140473021069120] processed a total of 1307 examples\n",
      "#metrics {\"StartTime\": 1747929260.201433, \"EndTime\": 1747929265.298604, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5096.866607666016, \"count\": 1, \"min\": 5096.866607666016, \"max\": 5096.866607666016}}}\n",
      "[05/22/2025 15:54:25 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=256.42763309518864 records/second\n",
      "[05/22/2025 15:54:25 INFO 140473021069120] #progress_metric: host=algo-1, completed 57.25 % of epochs\n",
      "[05/22/2025 15:54:25 INFO 140473021069120] #quality_metric: host=algo-1, epoch=228, train loss <loss>=0.8412820371714506\n",
      "[05/22/2025 15:54:25 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:26 INFO 140473021069120] Epoch[229] Batch[0] avg_epoch_loss=0.950153\n",
      "[05/22/2025 15:54:26 INFO 140473021069120] #quality_metric: host=algo-1, epoch=229, batch=0 train loss <loss>=0.9501528143882751\n",
      "[05/22/2025 15:54:28 INFO 140473021069120] Epoch[229] Batch[5] avg_epoch_loss=0.914236\n",
      "[05/22/2025 15:54:28 INFO 140473021069120] #quality_metric: host=algo-1, epoch=229, batch=5 train loss <loss>=0.9142357409000397\n",
      "[05/22/2025 15:54:28 INFO 140473021069120] Epoch[229] Batch [5]#011Speed: 365.41 samples/sec#011loss=0.914236\n",
      "[05/22/2025 15:54:30 INFO 140473021069120] Epoch[229] Batch[10] avg_epoch_loss=0.994810\n",
      "[05/22/2025 15:54:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=229, batch=10 train loss <loss>=1.0914996266365051\n",
      "[05/22/2025 15:54:30 INFO 140473021069120] Epoch[229] Batch [10]#011Speed: 343.95 samples/sec#011loss=1.091500\n",
      "[05/22/2025 15:54:30 INFO 140473021069120] processed a total of 1331 examples\n",
      "#metrics {\"StartTime\": 1747929265.2986634, \"EndTime\": 1747929270.4311037, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5132.177352905273, \"count\": 1, \"min\": 5132.177352905273, \"max\": 5132.177352905273}}}\n",
      "[05/22/2025 15:54:30 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.33970675283484 records/second\n",
      "[05/22/2025 15:54:30 INFO 140473021069120] #progress_metric: host=algo-1, completed 57.5 % of epochs\n",
      "[05/22/2025 15:54:30 INFO 140473021069120] #quality_metric: host=algo-1, epoch=229, train loss <loss>=0.9948102344166149\n",
      "[05/22/2025 15:54:30 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:31 INFO 140473021069120] Epoch[230] Batch[0] avg_epoch_loss=0.584333\n",
      "[05/22/2025 15:54:31 INFO 140473021069120] #quality_metric: host=algo-1, epoch=230, batch=0 train loss <loss>=0.5843325257301331\n",
      "[05/22/2025 15:54:33 INFO 140473021069120] Epoch[230] Batch[5] avg_epoch_loss=0.845661\n",
      "[05/22/2025 15:54:33 INFO 140473021069120] #quality_metric: host=algo-1, epoch=230, batch=5 train loss <loss>=0.845661054054896\n",
      "[05/22/2025 15:54:33 INFO 140473021069120] Epoch[230] Batch [5]#011Speed: 363.48 samples/sec#011loss=0.845661\n",
      "[05/22/2025 15:54:35 INFO 140473021069120] Epoch[230] Batch[10] avg_epoch_loss=0.920446\n",
      "[05/22/2025 15:54:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=230, batch=10 train loss <loss>=1.010186994075775\n",
      "[05/22/2025 15:54:35 INFO 140473021069120] Epoch[230] Batch [10]#011Speed: 360.95 samples/sec#011loss=1.010187\n",
      "[05/22/2025 15:54:35 INFO 140473021069120] processed a total of 1282 examples\n",
      "#metrics {\"StartTime\": 1747929270.4311626, \"EndTime\": 1747929275.518556, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5087.127923965454, \"count\": 1, \"min\": 5087.127923965454, \"max\": 5087.127923965454}}}\n",
      "[05/22/2025 15:54:35 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=252.004197337868 records/second\n",
      "[05/22/2025 15:54:35 INFO 140473021069120] #progress_metric: host=algo-1, completed 57.75 % of epochs\n",
      "[05/22/2025 15:54:35 INFO 140473021069120] #quality_metric: host=algo-1, epoch=230, train loss <loss>=0.9204455722462047\n",
      "[05/22/2025 15:54:35 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:37 INFO 140473021069120] Epoch[231] Batch[0] avg_epoch_loss=0.775545\n",
      "[05/22/2025 15:54:37 INFO 140473021069120] #quality_metric: host=algo-1, epoch=231, batch=0 train loss <loss>=0.775545060634613\n",
      "[05/22/2025 15:54:38 INFO 140473021069120] Epoch[231] Batch[5] avg_epoch_loss=0.802873\n",
      "[05/22/2025 15:54:38 INFO 140473021069120] #quality_metric: host=algo-1, epoch=231, batch=5 train loss <loss>=0.8028731246789297\n",
      "[05/22/2025 15:54:38 INFO 140473021069120] Epoch[231] Batch [5]#011Speed: 366.28 samples/sec#011loss=0.802873\n",
      "[05/22/2025 15:54:40 INFO 140473021069120] Epoch[231] Batch[10] avg_epoch_loss=1.061132\n",
      "[05/22/2025 15:54:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=231, batch=10 train loss <loss>=1.371043622493744\n",
      "[05/22/2025 15:54:40 INFO 140473021069120] Epoch[231] Batch [10]#011Speed: 354.07 samples/sec#011loss=1.371044\n",
      "[05/22/2025 15:54:40 INFO 140473021069120] processed a total of 1324 examples\n",
      "#metrics {\"StartTime\": 1747929275.518617, \"EndTime\": 1747929280.606961, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5088.086366653442, \"count\": 1, \"min\": 5088.086366653442, \"max\": 5088.086366653442}}}\n",
      "[05/22/2025 15:54:40 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=260.21101442147614 records/second\n",
      "[05/22/2025 15:54:40 INFO 140473021069120] #progress_metric: host=algo-1, completed 58.0 % of epochs\n",
      "[05/22/2025 15:54:40 INFO 140473021069120] #quality_metric: host=algo-1, epoch=231, train loss <loss>=1.0611324418674817\n",
      "[05/22/2025 15:54:40 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:42 INFO 140473021069120] Epoch[232] Batch[0] avg_epoch_loss=0.904483\n",
      "[05/22/2025 15:54:42 INFO 140473021069120] #quality_metric: host=algo-1, epoch=232, batch=0 train loss <loss>=0.904482901096344\n",
      "[05/22/2025 15:54:43 INFO 140473021069120] Epoch[232] Batch[5] avg_epoch_loss=0.838012\n",
      "[05/22/2025 15:54:43 INFO 140473021069120] #quality_metric: host=algo-1, epoch=232, batch=5 train loss <loss>=0.8380117813746134\n",
      "[05/22/2025 15:54:43 INFO 140473021069120] Epoch[232] Batch [5]#011Speed: 365.79 samples/sec#011loss=0.838012\n",
      "[05/22/2025 15:54:45 INFO 140473021069120] processed a total of 1273 examples\n",
      "#metrics {\"StartTime\": 1747929280.607022, \"EndTime\": 1747929285.342019, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4734.724998474121, \"count\": 1, \"min\": 4734.724998474121, \"max\": 4734.724998474121}}}\n",
      "[05/22/2025 15:54:45 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=268.85919667967755 records/second\n",
      "[05/22/2025 15:54:45 INFO 140473021069120] #progress_metric: host=algo-1, completed 58.25 % of epochs\n",
      "[05/22/2025 15:54:45 INFO 140473021069120] #quality_metric: host=algo-1, epoch=232, train loss <loss>=0.8656842529773712\n",
      "[05/22/2025 15:54:45 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:46 INFO 140473021069120] Epoch[233] Batch[0] avg_epoch_loss=0.843329\n",
      "[05/22/2025 15:54:46 INFO 140473021069120] #quality_metric: host=algo-1, epoch=233, batch=0 train loss <loss>=0.8433285355567932\n",
      "[05/22/2025 15:54:48 INFO 140473021069120] Epoch[233] Batch[5] avg_epoch_loss=0.776484\n",
      "[05/22/2025 15:54:48 INFO 140473021069120] #quality_metric: host=algo-1, epoch=233, batch=5 train loss <loss>=0.7764842212200165\n",
      "[05/22/2025 15:54:48 INFO 140473021069120] Epoch[233] Batch [5]#011Speed: 365.53 samples/sec#011loss=0.776484\n",
      "[05/22/2025 15:54:50 INFO 140473021069120] processed a total of 1260 examples\n",
      "#metrics {\"StartTime\": 1747929285.342084, \"EndTime\": 1747929290.0781317, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4735.708475112915, \"count\": 1, \"min\": 4735.708475112915, \"max\": 4735.708475112915}}}\n",
      "[05/22/2025 15:54:50 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=266.05837057981574 records/second\n",
      "[05/22/2025 15:54:50 INFO 140473021069120] #progress_metric: host=algo-1, completed 58.5 % of epochs\n",
      "[05/22/2025 15:54:50 INFO 140473021069120] #quality_metric: host=algo-1, epoch=233, train loss <loss>=0.8250872611999511\n",
      "[05/22/2025 15:54:50 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:51 INFO 140473021069120] Epoch[234] Batch[0] avg_epoch_loss=0.899985\n",
      "[05/22/2025 15:54:51 INFO 140473021069120] #quality_metric: host=algo-1, epoch=234, batch=0 train loss <loss>=0.8999851942062378\n",
      "[05/22/2025 15:54:53 INFO 140473021069120] Epoch[234] Batch[5] avg_epoch_loss=0.777036\n",
      "[05/22/2025 15:54:53 INFO 140473021069120] #quality_metric: host=algo-1, epoch=234, batch=5 train loss <loss>=0.7770359814167023\n",
      "[05/22/2025 15:54:53 INFO 140473021069120] Epoch[234] Batch [5]#011Speed: 366.92 samples/sec#011loss=0.777036\n",
      "[05/22/2025 15:54:55 INFO 140473021069120] Epoch[234] Batch[10] avg_epoch_loss=0.814890\n",
      "[05/22/2025 15:54:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=234, batch=10 train loss <loss>=0.8603152871131897\n",
      "[05/22/2025 15:54:55 INFO 140473021069120] Epoch[234] Batch [10]#011Speed: 354.07 samples/sec#011loss=0.860315\n",
      "[05/22/2025 15:54:55 INFO 140473021069120] processed a total of 1303 examples\n",
      "#metrics {\"StartTime\": 1747929290.078197, \"EndTime\": 1747929295.1837792, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5105.222225189209, \"count\": 1, \"min\": 5105.222225189209, \"max\": 5105.222225189209}}}\n",
      "[05/22/2025 15:54:55 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=255.22424966983044 records/second\n",
      "[05/22/2025 15:54:55 INFO 140473021069120] #progress_metric: host=algo-1, completed 58.75 % of epochs\n",
      "[05/22/2025 15:54:55 INFO 140473021069120] #quality_metric: host=algo-1, epoch=234, train loss <loss>=0.8148902112787421\n",
      "[05/22/2025 15:54:55 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:54:56 INFO 140473021069120] Epoch[235] Batch[0] avg_epoch_loss=1.128191\n",
      "[05/22/2025 15:54:56 INFO 140473021069120] #quality_metric: host=algo-1, epoch=235, batch=0 train loss <loss>=1.1281908750534058\n",
      "[05/22/2025 15:54:58 INFO 140473021069120] Epoch[235] Batch[5] avg_epoch_loss=1.043743\n",
      "[05/22/2025 15:54:58 INFO 140473021069120] #quality_metric: host=algo-1, epoch=235, batch=5 train loss <loss>=1.043743113676707\n",
      "[05/22/2025 15:54:58 INFO 140473021069120] Epoch[235] Batch [5]#011Speed: 368.53 samples/sec#011loss=1.043743\n",
      "[05/22/2025 15:55:00 INFO 140473021069120] Epoch[235] Batch[10] avg_epoch_loss=0.987440\n",
      "[05/22/2025 15:55:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=235, batch=10 train loss <loss>=0.9198771238327026\n",
      "[05/22/2025 15:55:00 INFO 140473021069120] Epoch[235] Batch [10]#011Speed: 356.16 samples/sec#011loss=0.919877\n",
      "[05/22/2025 15:55:00 INFO 140473021069120] processed a total of 1312 examples\n",
      "#metrics {\"StartTime\": 1747929295.1838417, \"EndTime\": 1747929300.244999, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5060.394525527954, \"count\": 1, \"min\": 5060.394525527954, \"max\": 5060.394525527954}}}\n",
      "[05/22/2025 15:55:00 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.26366857832386 records/second\n",
      "[05/22/2025 15:55:00 INFO 140473021069120] #progress_metric: host=algo-1, completed 59.0 % of epochs\n",
      "[05/22/2025 15:55:00 INFO 140473021069120] #quality_metric: host=algo-1, epoch=235, train loss <loss>=0.9874403910203413\n",
      "[05/22/2025 15:55:00 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:55:01 INFO 140473021069120] Epoch[236] Batch[0] avg_epoch_loss=0.706881\n",
      "[05/22/2025 15:55:01 INFO 140473021069120] #quality_metric: host=algo-1, epoch=236, batch=0 train loss <loss>=0.7068809866905212\n",
      "[05/22/2025 15:55:03 INFO 140473021069120] Epoch[236] Batch[5] avg_epoch_loss=0.887951\n",
      "[05/22/2025 15:55:03 INFO 140473021069120] #quality_metric: host=algo-1, epoch=236, batch=5 train loss <loss>=0.8879512151082357\n",
      "[05/22/2025 15:55:03 INFO 140473021069120] Epoch[236] Batch [5]#011Speed: 362.24 samples/sec#011loss=0.887951\n",
      "[05/22/2025 15:55:05 INFO 140473021069120] Epoch[236] Batch[10] avg_epoch_loss=0.914009\n",
      "[05/22/2025 15:55:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=236, batch=10 train loss <loss>=0.9452791333198547\n",
      "[05/22/2025 15:55:05 INFO 140473021069120] Epoch[236] Batch [10]#011Speed: 351.23 samples/sec#011loss=0.945279\n",
      "[05/22/2025 15:55:05 INFO 140473021069120] processed a total of 1305 examples\n",
      "#metrics {\"StartTime\": 1747929300.2450597, \"EndTime\": 1747929305.4017162, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5156.3849449157715, \"count\": 1, \"min\": 5156.3849449157715, \"max\": 5156.3849449157715}}}\n",
      "[05/22/2025 15:55:05 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=253.07934744192076 records/second\n",
      "[05/22/2025 15:55:05 INFO 140473021069120] #progress_metric: host=algo-1, completed 59.25 % of epochs\n",
      "[05/22/2025 15:55:05 INFO 140473021069120] #quality_metric: host=algo-1, epoch=236, train loss <loss>=0.9140093597498807\n",
      "[05/22/2025 15:55:05 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:55:06 INFO 140473021069120] Epoch[237] Batch[0] avg_epoch_loss=0.709697\n",
      "[05/22/2025 15:55:06 INFO 140473021069120] #quality_metric: host=algo-1, epoch=237, batch=0 train loss <loss>=0.7096967101097107\n",
      "[05/22/2025 15:55:08 INFO 140473021069120] Epoch[237] Batch[5] avg_epoch_loss=0.873965\n",
      "[05/22/2025 15:55:08 INFO 140473021069120] #quality_metric: host=algo-1, epoch=237, batch=5 train loss <loss>=0.8739650050799052\n",
      "[05/22/2025 15:55:08 INFO 140473021069120] Epoch[237] Batch [5]#011Speed: 358.26 samples/sec#011loss=0.873965\n",
      "[05/22/2025 15:55:10 INFO 140473021069120] Epoch[237] Batch[10] avg_epoch_loss=1.037007\n",
      "[05/22/2025 15:55:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=237, batch=10 train loss <loss>=1.232657265663147\n",
      "[05/22/2025 15:55:10 INFO 140473021069120] Epoch[237] Batch [10]#011Speed: 354.41 samples/sec#011loss=1.232657\n",
      "[05/22/2025 15:55:10 INFO 140473021069120] processed a total of 1285 examples\n",
      "#metrics {\"StartTime\": 1747929305.4017832, \"EndTime\": 1747929310.5508862, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5148.761510848999, \"count\": 1, \"min\": 5148.761510848999, \"max\": 5148.761510848999}}}\n",
      "[05/22/2025 15:55:10 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=249.5700514492058 records/second\n",
      "[05/22/2025 15:55:10 INFO 140473021069120] #progress_metric: host=algo-1, completed 59.5 % of epochs\n",
      "[05/22/2025 15:55:10 INFO 140473021069120] #quality_metric: host=algo-1, epoch=237, train loss <loss>=1.0370069417086514\n",
      "[05/22/2025 15:55:10 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:55:12 INFO 140473021069120] Epoch[238] Batch[0] avg_epoch_loss=0.732692\n",
      "[05/22/2025 15:55:12 INFO 140473021069120] #quality_metric: host=algo-1, epoch=238, batch=0 train loss <loss>=0.7326916456222534\n",
      "[05/22/2025 15:55:13 INFO 140473021069120] Epoch[238] Batch[5] avg_epoch_loss=0.836038\n",
      "[05/22/2025 15:55:13 INFO 140473021069120] #quality_metric: host=algo-1, epoch=238, batch=5 train loss <loss>=0.836037834485372\n",
      "[05/22/2025 15:55:13 INFO 140473021069120] Epoch[238] Batch [5]#011Speed: 360.30 samples/sec#011loss=0.836038\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] processed a total of 1250 examples\n",
      "#metrics {\"StartTime\": 1747929310.5509493, \"EndTime\": 1747929315.3605034, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4809.278964996338, \"count\": 1, \"min\": 4809.278964996338, \"max\": 4809.278964996338}}}\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] #throughput_metric: host=algo-1, train throughput=259.9089390693669 records/second\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] #progress_metric: host=algo-1, completed 59.75 % of epochs\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] #quality_metric: host=algo-1, epoch=238, train loss <loss>=0.7683838918805123\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] loss did not improve\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] Loading parameters from best epoch (198)\n",
      "#metrics {\"StartTime\": 1747929315.3605707, \"EndTime\": 1747929315.3784842, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 17.54307746887207, \"count\": 1, \"min\": 17.54307746887207, \"max\": 17.54307746887207}}}\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] stopping training now\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] #progress_metric: host=algo-1, completed 100 % of epochs\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] Final loss: 0.5330828103152189 (occurred at epoch 198)\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] #quality_metric: host=algo-1, train final_loss <loss>=0.5330828103152189\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] Worker algo-1 finished training.\n",
      "[05/22/2025 15:55:15 WARNING 140473021069120] wait_for_all_workers will not sync workers since the kv store is not running distributed\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] All workers finished. Serializing model for prediction.\n",
      "#metrics {\"StartTime\": 1747929315.3785572, \"EndTime\": 1747929315.7991526, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 420.1233386993408, \"count\": 1, \"min\": 420.1233386993408, \"max\": 420.1233386993408}}}\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] Number of GPUs being used: 0\n",
      "#metrics {\"StartTime\": 1747929315.7992227, \"EndTime\": 1747929315.9255962, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 546.5974807739258, \"count\": 1, \"min\": 546.5974807739258, \"max\": 546.5974807739258}}}\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] Serializing to /opt/ml/model/model_algo-1\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\n",
      "#metrics {\"StartTime\": 1747929315.9256485, \"EndTime\": 1747929315.9469776, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 21.29340171813965, \"count\": 1, \"min\": 21.29340171813965, \"max\": 21.29340171813965}}}\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] Successfully serialized the model for prediction.\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] #memory_usage::<batchbuffer> = 82.7978515625 mb\n",
      "[05/22/2025 15:55:15 INFO 140473021069120] Evaluating model accuracy on testset using 100 samples\n",
      "#metrics {\"StartTime\": 1747929315.9470289, \"EndTime\": 1747929315.9517128, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.030040740966796875, \"count\": 1, \"min\": 0.030040740966796875, \"max\": 0.030040740966796875}}}\n",
      "#metrics {\"StartTime\": 1747929315.951757, \"EndTime\": 1747929316.8085296, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 856.8484783172607, \"count\": 1, \"min\": 856.8484783172607, \"max\": 856.8484783172607}}}\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, RMSE): 2.479164964299548\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, mean_absolute_QuantileLoss): 816.2482016904044\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, mean_wQuantileLoss): 0.8843425803796363\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, wQuantileLoss[0.1]): 0.26970040876002316\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, wQuantileLoss[0.2]): 0.45267361660901906\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, wQuantileLoss[0.3]): 0.634899575355024\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, wQuantileLoss[0.4]): 0.828168235859391\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, wQuantileLoss[0.5]): 0.9974627855019802\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, wQuantileLoss[0.6]): 1.1508145947684105\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, wQuantileLoss[0.7]): 1.2342498047960369\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, wQuantileLoss[0.8]): 1.2456691565534794\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #test_score (algo-1, wQuantileLoss[0.9]): 1.1454450452133629\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #quality_metric: host=algo-1, test RMSE <loss>=2.479164964299548\n",
      "[05/22/2025 15:55:16 INFO 140473021069120] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.8843425803796363\n",
      "#metrics {\"StartTime\": 1747929316.808608, \"EndTime\": 1747929316.856673, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 3.758668899536133, \"count\": 1, \"min\": 3.758668899536133, \"max\": 3.758668899536133}, \"totaltime\": {\"sum\": 1166061.7921352386, \"count\": 1, \"min\": 1166061.7921352386, \"max\": 1166061.7921352386}}}\n",
      "\n",
      "2025-05-22 15:55:33 Uploading - Uploading generated training model\n",
      "2025-05-22 15:55:33 Completed - Training job completed\n",
      "Training seconds: 1309\n",
      "Billable seconds: 1309\n",
      "CPU times: total: 35.4 s\n",
      "Wall time: 22min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train/\".format(s3_data_path), \"test\": \"{}/test/\".format(s3_data_path)}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "55c08a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-05-22 15:55:33 Starting - Preparing the instances for training\n",
      "2025-05-22 15:55:33 Downloading - Downloading the training image\n",
      "2025-05-22 15:55:33 Training - Training image download completed. Training in progress.\n",
      "2025-05-22 15:55:33 Uploading - Uploading generated training model\n",
      "2025-05-22 15:55:33 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator.attach('lilipink-forecasting-2025-05-22-15-32-57-063',sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "763335fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8b3eb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "df08e0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/22/25 12:13:25] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: lilipink-forecasting-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-05-22-17-13-24-972 <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/22/25 12:13:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: lilipink-forecasting-\u001b[1;36m2025\u001b[0m-05-22-17-13-24-972 \u001b]8;id=61649;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=148414;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#6019\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6019</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         lilipink-forecasting-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-05-22-17-13-24-972                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=894669;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=809090;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#6019\u001b\\\u001b[2m6019\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         lilipink-forecasting-\u001b[1;36m2025\u001b[0m-05-22-17-13-24-972                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/22/25 12:13:26] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name                                            <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#4841\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4841</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         lilipink-forecasting-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-05-22-17-13-24-972                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/22/25 12:13:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name                                            \u001b]8;id=49765;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=849915;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#4841\u001b\\\u001b[2m4841\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         lilipink-forecasting-\u001b[1;36m2025\u001b[0m-05-22-17-13-24-972                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.large\", predictor_cls=DeepARPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5ff7b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_a_series(timeseries):\n",
    "    series_list = []\n",
    "    \n",
    "    for ts in timeseries:\n",
    "        # Verificar que la columna 'cantidad' existe\n",
    "        if 'cantidad' in ts.columns:\n",
    "            # Extraer la columna 'cantidad' como una serie\n",
    "            serie = ts['cantidad']\n",
    "\n",
    "            # Asegurarse de que el índice esté ordenado\n",
    "            serie = serie.sort_index()\n",
    "\n",
    "            # Intentar inferir la frecuencia del índice\n",
    "            try:\n",
    "                freq = pd.infer_freq(serie.index)\n",
    "                if freq is not None:\n",
    "                    serie.index.freq = freq\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo inferir frecuencia para una serie: {e}\")\n",
    "\n",
    "            series_list.append(serie)\n",
    "        else:\n",
    "            print(f\"Advertencia: Un dataframe no contiene la columna 'cantidad'\")\n",
    "    \n",
    "    return series_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7129b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_list = convertir_a_series(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f818233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_lista_features_dinamicas(lista_dataframes):\n",
    "    \"\"\"\n",
    "    Crea una lista de listas con los vectores temporales para cada dataframe.\n",
    "    \n",
    "    Args:\n",
    "        lista_dataframes: Lista de dataframes con columnas temporales\n",
    "    \n",
    "    Returns:\n",
    "        Lista de listas donde cada elemento es [V1, day, weekday, week, month, quarter] para un dataframe\n",
    "    \"\"\"\n",
    "    lista_features = []\n",
    "    \n",
    "    # Columnas temporales esperadas\n",
    "    columnas_esperadas = ['V1', 'day', 'weekday', 'week', 'month', 'quarter']\n",
    "    \n",
    "    for i, df in enumerate(lista_dataframes):\n",
    "        # Verificar que el dataframe tenga registros\n",
    "        if len(df) == 0:\n",
    "            print(f\"Advertencia: Dataframe {i} está vacío. Se añadirá una lista vacía.\")\n",
    "            lista_features.append([[] for _ in columnas_esperadas])\n",
    "            continue\n",
    "        \n",
    "        # Verificar que existan las columnas necesarias\n",
    "        columnas_faltantes = []\n",
    "        for col in columnas_esperadas:\n",
    "            if col not in df.columns:\n",
    "                columnas_faltantes.append(col)\n",
    "        \n",
    "        if columnas_faltantes:\n",
    "            print(f\"Advertencia: Dataframe {i} no tiene las columnas: {columnas_faltantes}\")\n",
    "            \n",
    "            # Si faltan columnas, intentar generarlas a partir del índice si es posible\n",
    "            df_temp = df.copy()\n",
    "            \n",
    "            # Generar columnas de fechas si el índice es de tipo datetime\n",
    "            if isinstance(df_temp.index, pd.DatetimeIndex):\n",
    "                if 'day' not in df_temp.columns:\n",
    "                    df_temp['day'] = df_temp.index.day\n",
    "                if 'weekday' not in df_temp.columns:\n",
    "                    df_temp['weekday'] = df_temp.index.dayofweek\n",
    "                if 'week' not in df_temp.columns:\n",
    "                    df_temp['week'] = df_temp.index.isocalendar().week\n",
    "                if 'month' not in df_temp.columns:\n",
    "                    df_temp['month'] = df_temp.index.month\n",
    "                if 'quarter' not in df_temp.columns:\n",
    "                    df_temp['quarter'] = df_temp.index.quarter\n",
    "                if 'V1' not in df_temp.columns:\n",
    "                    # Crear V1 basado en el período 2023-09-10 a 2023-11-02\n",
    "                    fecha_inicio_v1 = pd.Timestamp(\"2023-09-10\")\n",
    "                    fecha_fin_v1 = pd.Timestamp(\"2023-11-02\")\n",
    "                    df_temp['V1'] = 0\n",
    "                    mask_v1 = (df_temp.index >= fecha_inicio_v1) & (df_temp.index <= fecha_fin_v1)\n",
    "                    df_temp.loc[mask_v1, 'V1'] = 1\n",
    "                \n",
    "                df = df_temp\n",
    "                print(f\"  Columnas generadas automáticamente para Dataframe {i}\")\n",
    "                \n",
    "            else:\n",
    "                # Intentar convertir el índice a datetime si no lo es\n",
    "                try:\n",
    "                    df_temp.index = pd.to_datetime(df_temp.index)\n",
    "                    \n",
    "                    # Generar columnas después de convertir índice\n",
    "                    if 'day' not in df_temp.columns:\n",
    "                        df_temp['day'] = df_temp.index.day\n",
    "                    if 'weekday' not in df_temp.columns:\n",
    "                        df_temp['weekday'] = df_temp.index.dayofweek\n",
    "                    if 'week' not in df_temp.columns:\n",
    "                        df_temp['week'] = df_temp.index.isocalendar().week\n",
    "                    if 'month' not in df_temp.columns:\n",
    "                        df_temp['month'] = df_temp.index.month\n",
    "                    if 'quarter' not in df_temp.columns:\n",
    "                        df_temp['quarter'] = df_temp.index.quarter\n",
    "                    if 'V1' not in df_temp.columns:\n",
    "                        fecha_inicio_v1 = pd.Timestamp(\"2023-09-10\")\n",
    "                        fecha_fin_v1 = pd.Timestamp(\"2023-11-02\")\n",
    "                        df_temp['V1'] = 0\n",
    "                        mask_v1 = (df_temp.index >= fecha_inicio_v1) & (df_temp.index <= fecha_fin_v1)\n",
    "                        df_temp.loc[mask_v1, 'V1'] = 1\n",
    "                    \n",
    "                    df = df_temp\n",
    "                    print(f\"  Índice convertido a datetime y columnas generadas para Dataframe {i}\")\n",
    "                    \n",
    "                except:\n",
    "                    print(f\"Error: No se pueden generar las columnas {columnas_faltantes} para el Dataframe {i}. Se añadirá una lista vacía.\")\n",
    "                    lista_features.append([[] for _ in columnas_esperadas])\n",
    "                    continue\n",
    "        \n",
    "        # Crear los vectores de características dinámicas\n",
    "        V1_vector = df['V1'].tolist()\n",
    "        day_vector = df['day'].tolist()\n",
    "        weekday_vector = df['weekday'].tolist()\n",
    "        week_vector = df['week'].tolist()\n",
    "        month_vector = df['month'].tolist()\n",
    "        quarter_vector = df['quarter'].tolist()\n",
    "        \n",
    "        # Añadir los vectores a la lista en el orden correcto\n",
    "        features_dataframe = [V1_vector, day_vector, weekday_vector, week_vector, month_vector, quarter_vector]\n",
    "        lista_features.append(features_dataframe)\n",
    "        \n",
    "        # Información de diagnóstico para los primeros DataFrames\n",
    "        if i < 5:\n",
    "            print(f\"Dataframe {i} procesado:\")\n",
    "            print(f\"  Longitud de vectores: {len(V1_vector)}\")\n",
    "            print(f\"  V1 - unos: {V1_vector.count(1)}, ceros: {V1_vector.count(0)}\")\n",
    "            print(f\"  day rango: {min(day_vector)} - {max(day_vector)}\")\n",
    "            print(f\"  weekday rango: {min(weekday_vector)} - {max(weekday_vector)}\")\n",
    "            print(f\"  week rango: {min(week_vector)} - {max(week_vector)}\")\n",
    "            print(f\"  month rango: {min(month_vector)} - {max(month_vector)}\")\n",
    "            print(f\"  quarter rango: {min(quarter_vector)} - {max(quarter_vector)}\")\n",
    "    \n",
    "    print(f\"\\nTotal de features dinámicas creadas: {len(lista_features)}\")\n",
    "    return lista_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e4fd79f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe 0 procesado:\n",
      "  Longitud de vectores: 1370\n",
      "  V1 - unos: 54, ceros: 1316\n",
      "  day rango: 1 - 31\n",
      "  weekday rango: 0 - 6\n",
      "  week rango: 1 - 52\n",
      "  month rango: 1 - 12\n",
      "  quarter rango: 1 - 4\n",
      "Dataframe 1 procesado:\n",
      "  Longitud de vectores: 1282\n",
      "  V1 - unos: 54, ceros: 1228\n",
      "  day rango: 1 - 31\n",
      "  weekday rango: 0 - 6\n",
      "  week rango: 1 - 52\n",
      "  month rango: 1 - 12\n",
      "  quarter rango: 1 - 4\n",
      "Dataframe 2 procesado:\n",
      "  Longitud de vectores: 1374\n",
      "  V1 - unos: 54, ceros: 1320\n",
      "  day rango: 1 - 31\n",
      "  weekday rango: 0 - 6\n",
      "  week rango: 1 - 52\n",
      "  month rango: 1 - 12\n",
      "  quarter rango: 1 - 4\n",
      "Dataframe 3 procesado:\n",
      "  Longitud de vectores: 1370\n",
      "  V1 - unos: 54, ceros: 1316\n",
      "  day rango: 1 - 31\n",
      "  weekday rango: 0 - 6\n",
      "  week rango: 1 - 52\n",
      "  month rango: 1 - 12\n",
      "  quarter rango: 1 - 4\n",
      "Dataframe 4 procesado:\n",
      "  Longitud de vectores: 1372\n",
      "  V1 - unos: 54, ceros: 1318\n",
      "  day rango: 1 - 31\n",
      "  weekday rango: 0 - 6\n",
      "  week rango: 1 - 52\n",
      "  month rango: 1 - 12\n",
      "  quarter rango: 1 - 4\n",
      "\n",
      "Total de features dinámicas creadas: 27\n"
     ]
    }
   ],
   "source": [
    "dynamic_list = crear_lista_features_dinamicas(timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b6fa4068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-05-11</th>\n",
       "      <td>0.628661</td>\n",
       "      <td>8.023063</td>\n",
       "      <td>12.768902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-12</th>\n",
       "      <td>0.410730</td>\n",
       "      <td>4.412768</td>\n",
       "      <td>8.170145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-13</th>\n",
       "      <td>-0.486587</td>\n",
       "      <td>3.543970</td>\n",
       "      <td>8.228227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-14</th>\n",
       "      <td>-0.737101</td>\n",
       "      <td>4.278812</td>\n",
       "      <td>9.811383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-15</th>\n",
       "      <td>-0.043989</td>\n",
       "      <td>4.375904</td>\n",
       "      <td>10.301924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-16</th>\n",
       "      <td>0.811715</td>\n",
       "      <td>5.791229</td>\n",
       "      <td>11.961033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0.1       0.5        0.9\n",
       "2025-05-11  0.628661  8.023063  12.768902\n",
       "2025-05-12  0.410730  4.412768   8.170145\n",
       "2025-05-13 -0.486587  3.543970   8.228227\n",
       "2025-05-14 -0.737101  4.278812   9.811383\n",
       "2025-05-15 -0.043989  4.375904  10.301924\n",
       "2025-05-16  0.811715  5.791229  11.961033"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horizon_pred=30\n",
    "i=10\n",
    "predictor.predict(\n",
    "    ts = timeseries_list[i][:-horizon_pred], \n",
    "    cat=vectores_cat[i],\n",
    "    dynamic_feat=dynamic_list[i],\n",
    "    quantiles=[0.1, 0.5, 0.9],\n",
    ").tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "id": "bd5efb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha\n",
       "2025-05-11     NaN\n",
       "2025-05-12     1.0\n",
       "2025-05-13     NaN\n",
       "2025-05-14     3.0\n",
       "2025-05-15     6.0\n",
       "2025-05-16    12.0\n",
       "Freq: D, Name: cantidad, dtype: float64"
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries_list[i].tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "005d6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_predicciones_por_material(materiales, timeseries_list, vectores_cat, dynamic_list, predictor, horizon_pred=6):\n",
    "    \"\"\"\n",
    "    Genera predicciones para cada material y las devuelve en un diccionario.\n",
    "    \n",
    "    Args:\n",
    "        materiales: Lista con los nombres de materiales\n",
    "        timeseries_list: Lista de series temporales\n",
    "        vectores_cat: Lista de vectores categóricos\n",
    "        dynamic_list: Lista de features dinámicas\n",
    "        predictor: Modelo predictor entrenado\n",
    "        horizon_pred: Horizonte de predicción (default: 6)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con materiales como keys y dataframes de predicciones como values\n",
    "    \"\"\"\n",
    "    predicciones_dict = {}\n",
    "    \n",
    "    for i in range(len(materiales)):\n",
    "        material = materiales[i]\n",
    "        \n",
    "        # Generar predicción para el índice i\n",
    "        prediccion = predictor.predict(\n",
    "            ts = timeseries_list[i][:-horizon_pred], \n",
    "            cat=vectores_cat[i],\n",
    "            dynamic_feat=dynamic_list[i],\n",
    "            quantiles=[0.1, 0.5, 0.9],\n",
    "        )\n",
    "        \n",
    "        # Guardar en el diccionario\n",
    "        predicciones_dict[material] = prediccion\n",
    "        \n",
    "    return predicciones_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e59e8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_por_material = generar_predicciones_por_material(\n",
    "    materiales=materiales,\n",
    "    timeseries_list=timeseries_list,\n",
    "    vectores_cat=vectores_cat,\n",
    "    dynamic_list=dynamic_list,\n",
    "    predictor=predictor,\n",
    "    horizon_pred=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "1e9995bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado: diario_modificado_test_27.xlsx\n",
      "Orden de columnas: ['Material', 'Fecha', '0.1', '0.5', '0.9']\n",
      "Formato de fecha: YYYY-MM-DD\n"
     ]
    }
   ],
   "source": [
    "def exportar_predicciones_consolidado(predicciones_dict, nombre_archivo=\"diario_modificado_test_27.xlsx\"):\n",
    "    \"\"\"\n",
    "    Exporta todas las predicciones con Material y Fecha como primeras dos columnas.\n",
    "    \"\"\"\n",
    "    \n",
    "    dfs_list = []\n",
    "    \n",
    "    for material, prediccion in predicciones_dict.items():\n",
    "        try:\n",
    "            # Convertir a DataFrame\n",
    "            if hasattr(prediccion, 'to_dataframe'):\n",
    "                df_pred = prediccion.to_dataframe()\n",
    "            elif hasattr(prediccion, 'to_pandas'):\n",
    "                df_pred = prediccion.to_pandas()\n",
    "            else:\n",
    "                df_pred = prediccion\n",
    "            \n",
    "            # Resetear índice y renombrar a 'Fecha'\n",
    "            df_pred = df_pred.reset_index()\n",
    "            date_col = df_pred.columns[0]\n",
    "            df_pred = df_pred.rename(columns={date_col: 'Fecha'})\n",
    "            \n",
    "            # Formatear fechas\n",
    "            df_pred['Fecha'] = pd.to_datetime(df_pred['Fecha']).dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Agregar columna de material\n",
    "            df_pred['Material'] = material\n",
    "            \n",
    "            dfs_list.append(df_pred)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {material}: {e}\")\n",
    "    \n",
    "    # Concatenar todos los DataFrames\n",
    "    if dfs_list:\n",
    "        df_final = pd.concat(dfs_list, ignore_index=True)\n",
    "        \n",
    "        # Reorganizar columnas: Material, Fecha, luego el resto en orden\n",
    "        other_cols = [col for col in df_final.columns if col not in ['Material', 'Fecha']]\n",
    "        cols = ['Material', 'Fecha'] + other_cols\n",
    "        df_final = df_final[cols]\n",
    "        \n",
    "        # Exportar\n",
    "        df_final.to_excel(nombre_archivo, index=False)\n",
    "        print(f\"Archivo guardado: {nombre_archivo}\")\n",
    "        print(f\"Orden de columnas: {list(df_final.columns)}\")\n",
    "        print(\"Formato de fecha: YYYY-MM-DD\")\n",
    "    else:\n",
    "        print(\"Error: No se pudieron procesar las predicciones\")\n",
    "\n",
    "# Ejecutar\n",
    "exportar_predicciones_consolidado(predicciones_por_material, \"diario_modificado_test_27.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4affbf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/22/25 12:29:35] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting model with name: lilipink-forecasting-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-05-22-17-13-24-972 <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#5356\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5356</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/22/25 12:29:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting model with name: lilipink-forecasting-\u001b[1;36m2025\u001b[0m-05-22-17-13-24-972 \u001b]8;id=348816;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=988575;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#5356\u001b\\\u001b[2m5356\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint configuration with name:                             <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#4995\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4995</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         lilipink-forecasting-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-05-22-17-13-24-972                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint configuration with name:                             \u001b]8;id=802946;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=205363;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#4995\u001b\\\u001b[2m4995\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         lilipink-forecasting-\u001b[1;36m2025\u001b[0m-05-22-17-13-24-972                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint with name:                                           <a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#4985\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4985</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         lilipink-forecasting-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-05-22-17-13-24-972                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint with name:                                           \u001b]8;id=47534;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=722365;file://c:\\Users\\Usuario\\Desktop\\Proyecto_lilipink\\venv\\Lib\\site-packages\\sagemaker\\session.py#4985\u001b\\\u001b[2m4985\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         lilipink-forecasting-\u001b[1;36m2025\u001b[0m-05-22-17-13-24-972                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
